<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Petastorm Training Data Create" />
<meta property="og:description" content="Create MNIST Petastorm Dataset In this notebook we will go through how you can create a Petastorm dataset with the MNIST images of digits, and also how you can save it as a documented and reusable training dataset in the Hopsworks Feature Store. The petastorm dataset can later on be used to train models using either Tensorflow, PyTorch or SparkML
from hops import hdfs, featurestore import numpy as np import pydoop import gzip from tempfile import TemporaryFile # IMPORTANT: must import tensorflow before petastorm." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://examples.hopsworks.ai/featurestore/petastorm/petastormmnist_createdataset/" />



<meta property="article:published_time" content="2021-02-24T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2021-02-24T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Petastorm Training Data Create"/>
<meta name="twitter:description" content="Create MNIST Petastorm Dataset In this notebook we will go through how you can create a Petastorm dataset with the MNIST images of digits, and also how you can save it as a documented and reusable training dataset in the Hopsworks Feature Store. The petastorm dataset can later on be used to train models using either Tensorflow, PyTorch or SparkML
from hops import hdfs, featurestore import numpy as np import pydoop import gzip from tempfile import TemporaryFile # IMPORTANT: must import tensorflow before petastorm."/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Petastorm Training Data Create",
  "url": "https://examples.hopsworks.ai/featurestore/petastorm/petastormmnist_createdataset/",
  "wordCount": "1383",
  "datePublished": "2021-02-24T00:00:00&#43;00:00",
  "dateModified": "2021-02-24T00:00:00&#43;00:00",
  "author": {
  "@type": "Person",
  "name": ""
  }
  }
</script> 

    <title>Petastorm Training Data Create</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://examples.hopsworks.ai/css/custom.css" rel="stylesheet">
    <link href="https://examples.hopsworks.ai/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Hopsworks Examples" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://examples.hopsworks.ai">Hopsworks Examples</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://hopsworks.ai" title="Hopsworks.ai">hopsworks.ai</a></li>
                    <li><a href="https://docs.hopsworks.ai" title="Docs">docs.hopsworks.ai</a></li>
                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
    <header>
      <div class="alert alert-warning flashcard_ad" role="alert">
	Want to learn machine learning? Try my <a href="https://machinelearningflashcards.com" class="alert-link">machine learning flashcards</a>, <a href='https://amzn.to/2HwnWty' class="alert-link">book</a>, or <a href='https://www.youtube.com/channel/UCnd4Fi-ODvuPbxR2fO2j7kA' class="alert-link">study with me.</a>.
      </div>
      <h1 class="technical_note_title">Petastorm Training Data Create</h1>
      <div class="technical_note_date">
	<time datetime=" 2021-02-24T00:00:00Z "> 24 Feb 2021</time>
      </div>
    </header>
    <div class="content">

      

<h1 id="create-mnist-petastorm-dataset">Create MNIST Petastorm Dataset</h1>

<p>In this notebook we will go through how you can create a Petastorm dataset with the MNIST images of digits, and also how you can save it as a documented and reusable training dataset in the Hopsworks Feature Store. The petastorm dataset can later on be used to train models using either Tensorflow, PyTorch or SparkML</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">from hops import hdfs, featurestore
import numpy as np
import pydoop
import gzip
from tempfile import TemporaryFile

# IMPORTANT: must import  tensorflow before petastorm.tf_utils due to a bug in petastorm
import tensorflow as tf
from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField
from pyspark.sql.types import StructType, StructField, IntegerType
from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec</code></pre></div>
<pre><code>Starting Spark application
</code></pre>

<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1551196216588_0003</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://hopsworks0.logicalclocks.com:8088/proxy/application_1551196216588_0003/">Link</a></td><td><a target="_blank" href="http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1551196216588_0003_01_000001/demo_featurestore_admin000__meb10000">Link</a></td><td>âœ”</td></tr></table>

<pre><code>SparkSession available as 'spark'.
</code></pre>

<h3 id="step-1-create-a-hopsworks-dataset-called-mnist-and-upload-the-mnist-images-to-hopsworks">Step 1: Create a Hopsworks Dataset called &ldquo;mnist&rdquo; and Upload the MNIST images to Hopsworks</h3>

<p>The MNIST dataset can be downloaded here: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>, it consists of four files:</p>

<ul>
<li>train-images-idx3-ubyte.gz:  training set images (9912422 bytes)</li>
<li>train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)</li>
<li>t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)</li>
<li>t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)</li>
</ul>

<p>The data is stored in the idx format. The IDX file format is a simple format for vectors and multidimensional matrices of various numerical types.</p>

<p>The basic format according to <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> is:</p>

<pre><code>magic number
size in dimension 1
size in dimension 2
size in dimension 3
....
size in dimension N
data
</code></pre>

<p>The magic number is four bytes long. The first 2 bytes are always 0.</p>

<p>The third byte codes the type of the data:</p>

<pre><code>0x08: unsigned byte
0x09: signed byte
0x0B: short (2 bytes)
0x0C: int (4 bytes)
0x0D: float (4 bytes)
0x0E: double (8 bytes)
</code></pre>

<p>The fouth byte codes the number of dimensions of the vector/matrix: 1 for vectors, 2 for matrices&hellip;.</p>

<p>The sizes in each dimension are 4-byte integers (big endian, like in most non-Intel processors).</p>

<p>The data is stored like in a C array, i.e. the index in the last dimension changes the fastest.</p>

<p>You could also have uploaded the .png images directly and read them using Spark: <a href="https://databricks.com/blog/2018/12/10/introducing-built-in-image-data-source-in-apache-spark-2-4.html">https://databricks.com/blog/2018/12/10/introducing-built-in-image-data-source-in-apache-spark-2-4.html</a></p>

<p>But to save us the effort of uploading 70000 images and unzipping them, we will use Yann LeCunns IDX files.</p>

<p>To upload the files, (1) download them from here: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>; (2) Go to the dataset browser in Hopsworks in your project and click the button &lsquo;Create new dataset&rsquo; and name the new dataset &ldquo;mnist&rdquo;; (3) Click on the dataset and upload the files.</p>

<p><img src="./../images/petastorm3.png" alt="Petastorm 3" title="Petastorm 3" /></p>

<p><img src="./../images/petastorm4.png" alt="Petastorm 4" title="Petastorm 4" /></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">files = hdfs.ls(hdfs.project_path() + &#34;mnist&#34;)
for i in range(len(files)):
    print(files[i])</code></pre></div>
<pre><code>hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/README.md
hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/t10k-images-idx3-ubyte.gz
hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/t10k-labels-idx1-ubyte.gz
hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/train-images-idx3-ubyte.gz
hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/train-labels-idx1-ubyte.gz
</code></pre>

<h3 id="step-2-parse-the-idx-files-into-numpy-arrays">Step 2: Parse the IDX files into Numpy Arrays</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def read_mnist(train_num_examples, test_num_examples):
    &#34;&#34;&#34;
    Parses MNIST dataset in the IDX format into numpy multi-dimensional arrays
    
    Args:
        :train_num_examples: number of training examples to parse
        :test_num_examples: number of test examples to parse
    
    Returns: train_images, train_labels, test_images, test_labels
    &#34;&#34;&#34;
    image_size = 28
    train_num_examples = train_num_examples
    test_num_examples = test_num_examples
    train_images_path = hdfs.project_path() + &#34;mnist/train-images-idx3-ubyte.gz&#34;
    train_labels_path = hdfs.project_path() + &#34;mnist/train-labels-idx1-ubyte.gz&#34;
    test_images_path = hdfs.project_path() + &#34;mnist/t10k-images-idx3-ubyte.gz&#34;
    test_labels_path = hdfs.project_path() + &#34;mnist/t10k-labels-idx1-ubyte.gz&#34;
    image_padding = 16
    label_padding = 8
    def parse_zip(path, padding, bytes):
        f_zip = pydoop.hdfs.open(path, &#34;rb&#34;)
        f_unzip = gzip.GzipFile(mode=&#39;rb&#39;, fileobj=f_zip)
        f_unzip.read(padding) # read metadata string
        return f_unzip.read(bytes)
    train_images_data = np.frombuffer(parse_zip(train_images_path, image_padding, image_size * image_size * train_num_examples), dtype=np.uint8)
    train_images = train_images_data.reshape(train_num_examples, image_size, image_size, 1)
    train_labels_data = np.frombuffer(parse_zip(train_labels_path, label_padding, train_num_examples), dtype=np.uint8).astype(np.int64)
    train_labels = train_labels_data.reshape(train_num_examples, 1)
    test_images_data = np.frombuffer(parse_zip(test_images_path, image_padding, image_size * image_size * test_num_examples), dtype=np.uint8)
    test_images = test_images_data.reshape(test_num_examples, image_size, image_size, 1)
    test_labels_data = np.frombuffer(parse_zip(test_labels_path, label_padding, test_num_examples), dtype=np.uint8).astype(np.int64)
    test_labels = test_labels_data.reshape(test_num_examples, 1)
    return train_images, train_labels, test_images, test_labels</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">train_images, train_labels, test_images, test_labels = read_mnist(60000, 10000)</code></pre></div>
<h3 id="step-3-quick-data-validation">Step 3: Quick Data Validation</h3>

<p>To validate that we parsed the data correctly we can try to visualize a bunch of images and compare the imaegs to their labels. Plotting in a distributed computing setting is a little bit tricky, it is explained in more details here: <a href="https://hopsworks.readthedocs.io/en/0.9/user_guide/hopsworks/jupyter.html#plotting-with-pyspark-kernel">https://hopsworks.readthedocs.io/en/0.9/user_guide/hopsworks/jupyter.html#plotting-with-pyspark-kernel</a>.</p>

<p>TLDR; We can save a few images to HDFS and read them in the %%local environment to plot them</p>

<h5 id="save-a-handful-of-images-labels-to-hdfs">Save a handful of images/labels to HDFS</h5>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">from hops import numpy_helper as hops_numpy
hops_numpy.save(&#34;mnist/train_img_val.npy&#34;, train_images[0:2])
hops_numpy.save(&#34;mnist/train_lbl_val.npy&#34;, train_labels[0:2])
hops_numpy.save(&#34;mnist/test_img_val.npy&#34;, test_images[0:2])
hops_numpy.save(&#34;mnist/test_lbl_val.npy&#34;, test_labels[0:2])</code></pre></div>
<h5 id="read-sample-images-from-hdfs-in-local-and-plot">Read sample images from HDFS in %%local and plot</h5>

<h6 id="utility-functions-for-plotting">Utility functions for plotting</h6>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
import matplotlib.pyplot as plt
from hops import hdfs
import numpy as np
from tempfile import TemporaryFile
from hops import numpy_helper as hops_numpy

def plot_train_example(example_num):
    &#34;&#34;&#34;
    Utility function that plots a training image together with its label 
    
    Args:
         :example_num: the id of the example to plot
    
    Returns: None
    &#34;&#34;&#34;
    image = np.asarray(train_images_val[example_num]).squeeze()
    plt.imshow(image)
    print(&#34;label:&#34; + str(train_lbls_val[example_num][0]))
    
def plot_test_example(example_num):
    &#34;&#34;&#34;
    Utility function that plots a test image together with its label

    Args:
         :example_num: the id of the example to plot
    
    Returns: None
    &#34;&#34;&#34;
    image = np.asarray(test_images_val[example_num]).squeeze()
    plt.imshow(image)
    print(&#34;label:&#34; + str(test_lbls_val[example_num][0]))</code></pre></div>
<h6 id="read-images-in-local">Read images in %%local</h6>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
train_images_val = hops_numpy.load(&#34;mnist/train_img_val.npy&#34;)
train_lbls_val = hops_numpy.load(&#34;mnist/train_lbl_val.npy&#34;)
test_images_val = hops_numpy.load(&#34;mnist/test_img_val.npy&#34;)
test_lbls_val = hops_numpy.load(&#34;mnist/test_lbl_val.npy&#34;)</code></pre></div>
<h6 id="plot-using-matplotlib">Plot using matplotlib</h6>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
plot_train_example(0)</code></pre></div>
<pre><code>label:5
</code></pre>

<p><img src="PetastormMNIST_CreateDataset_files/PetastormMNIST_CreateDataset_17_1.png" alt="png" /></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
plot_train_example(1)</code></pre></div>
<pre><code>label:0
</code></pre>

<p><img src="PetastormMNIST_CreateDataset_files/PetastormMNIST_CreateDataset_18_1.png" alt="png" /></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
plot_test_example(0)</code></pre></div>
<pre><code>label:7
</code></pre>

<p><img src="PetastormMNIST_CreateDataset_files/PetastormMNIST_CreateDataset_19_1.png" alt="png" /></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
plot_test_example(1)</code></pre></div>
<pre><code>label:2
</code></pre>

<p><img src="PetastormMNIST_CreateDataset_files/PetastormMNIST_CreateDataset_20_1.png" alt="png" /></p>

<h3 id="step-4-save-the-numpy-dataset-as-a-petastorm-dataset">Step 4: Save  the Numpy Dataset as a Petastorm Dataset</h3>

<p>We have now parsed the MNIST database of handwritten images into numpy arrays <code>train_images, train_labels, test_images, test_labels</code>. We can store this to HDFS as a Petastorm dataset by (1) specifying the petastorm schema; (2) convert the numpy data to a spark dataframe; (3) Save the data to petastorm training dataset in the featurestore using  <code>featurestore.create_training_dataset()</code> and set <code>data_format='petastorm'</code>.</p>

<h4 id="specify-petastorm-schema">Specify Petastorm Schema</h4>

<p>Petastorm datasets are designed to be easy to read into deep learning frameworks such as Tensorflow and Pytorch. To enable this we need to specify the schema when writing the data to HDFS.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">MnistSchema = Unischema(&#39;MnistSchema&#39;, [
    UnischemaField(&#39;image&#39;, np.uint8, (28, 28,1), NdarrayCodec(), False),
    UnischemaField(&#39;digit&#39;, np.int_, (), ScalarCodec(IntegerType()), False)
])</code></pre></div>
<h4 id="create-a-spark-dataframe-with-the-data-in-the-numpy-arrays">Create a Spark Dataframe with the data in the numpy arrays</h4>

<p>Petastorm relies on Spark to write to HDFS so we have to convert the numpy arrays into a spark dataframe that conforms to the Petastorm schema.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">TRAIN_OUTPUT_PATH = hdfs.project_path() + &#34;mnist/train_petastorm&#34;
TEST_OUTPUT_PATH = hdfs.project_path() + &#34;mnist/test_petastorm&#34; 
mnist_data = {
            &#39;train&#39;: {&#34;images&#34;: train_images, &#34;labels&#34;: train_labels, &#34;output_path&#34;: TRAIN_OUTPUT_PATH},
            &#39;test&#39;: {&#34;images&#34;: test_images, &#34;labels&#34;: test_labels, &#34;output_path&#34;: TEST_OUTPUT_PATH}
        }</code></pre></div>
<p>First create an RDD of dict-rows with the numpy arrays and then convert the dict-rows into Spark Dataframe Rows using the petastorm utility function <code>dict_to_spark_row</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def create_petastorm_dict(i, test=False):
    &#34;&#34;&#34;
    Returns a train or test example from mnist_data variable as a dict with the Petastorm schema
    
    Args:
        :i: the index of the example
        :test: a boolean flag whether to get a test example, otherwise gets a training example
    &#34;&#34;&#34;
    if test:
        return {
            MnistSchema.digit.name: mnist_data[&#34;test&#34;][&#34;labels&#34;][i],
            MnistSchema.image.name: mnist_data[&#34;test&#34;][&#34;images&#34;][i]
        }
    return {
            MnistSchema.digit.name: mnist_data[&#34;train&#34;][&#34;labels&#34;][i],
            MnistSchema.image.name: mnist_data[&#34;train&#34;][&#34;images&#34;][i]
        } </code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">dataset_train_rdd = sc.parallelize(list(range(len(mnist_data[&#34;train&#34;][&#34;images&#34;]))))\
        .map(lambda i: create_petastorm_dict(i))\
        .map(lambda x: dict_to_spark_row(MnistSchema, x))</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">dataset_test_rdd = sc.parallelize(list(range(len(mnist_data[&#34;test&#34;][&#34;images&#34;]))))\
        .map(lambda i: create_petastorm_dict(i, test=True))\
        .map(lambda x: dict_to_spark_row(MnistSchema, x))</code></pre></div>
<p>Then convert the RDDs to Spark Dataframes using <code>MnistSchema.as_spark_schema()</code></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">df_train = spark.createDataFrame(dataset_train_rdd, MnistSchema.as_spark_schema())</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">df_test = spark.createDataFrame(dataset_test_rdd, MnistSchema.as_spark_schema())</code></pre></div>
<p><strong>Note</strong> Using dicts/rdds and then spark.createDataFrame() is just one approach to create a spark dataframe that conforms to the Petastorm schema. If you don&rsquo;t have the data in numpy arrays but rather saved on disk you might use another approach, the important thing is that you define a petastorm schema and that your dataframe&rsquo;s spark schema is equal to <code>petastorm_schema.as_spark_schema()</code>.</p>

<h4 id="save-the-spark-dataframes-as-training-dataset-in-the-petastorm-format-in-the-hopsworks-feature-store">Save the Spark Dataframes as Training Dataset in the Petastorm Format in the Hopsworks Feature Store</h4>

<p>All parameters available for the petastorm method <code>materialize_dataset</code> listed here: <a href="https://petastorm.readthedocs.io/en/latest/api.html">https://petastorm.readthedocs.io/en/latest/api.html</a> can be provided as arguments in a dict to <code>featurestore.create_training_dataset(petastorm_args=dict)</code>. The only required argument is &ldquo;schema&rdquo;.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">petastorm_args = {
    &#34;schema&#34;: MnistSchema
}</code></pre></div>
<p>Save Dataset with Training Data:</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">featurestore.create_training_dataset(df_train, &#34;MNIST_train_petastorm&#34;, data_format=&#34;petastorm&#34;, 
                                     petastorm_args=petastorm_args,
                                     description=&#34;MNIST Digit Database Training Dataset of 60000 images stored in the Petastorm Format&#34;,
                                     #this type of statistics do not make sense on image data
                                     feature_correlation=False, feature_histograms=False, cluster_analysis=False)</code></pre></div>
<pre><code>computing descriptive statistics for : MNIST_train_petastorm
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">petastorm_args = {
    &#34;schema&#34;: MnistSchema
}</code></pre></div>
<p>Save Dataset with Test Data:</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">featurestore.create_training_dataset(df_test, &#34;MNIST_test_petastorm&#34;, data_format=&#34;petastorm&#34;, 
                                     petastorm_args=petastorm_args,
                                     description=&#34;MNIST Digit Database Test Dataset of 10000 images stored in the Petastorm Format&#34;,
                                     #this type of statistics do not make sense on image data
                                     feature_correlation=False, feature_histograms=False, cluster_analysis=False)</code></pre></div>
<pre><code>computing descriptive statistics for : MNIST_test_petastorm
</code></pre>

<h3 id="step-5-use-the-training-dataset">Step 5: Use the Training Dataset</h3>

<p>Once the datasets are saved in the feature store you can find them in the Feature Registry inside your project and easily reuse it later. By saving the dataset as a petastorm dataset you can use the same data in several different deep learning frameworks, such as PyTorch and Tensorflow. We have two example notebooks that demonstrate this here:</p>

<ul>
<li><p><a href="PetastormMNIST_Tensorflow.ipynb">Tensorflow Example</a></p></li>

<li><p><a href="PetastormMNIST_PyTorch.ipynb">PyTorch Example</a></p></li>
</ul>

<p><img src="./../images/petastorm5.png" alt="Petastorm 5" title="Petastorm 5" /></p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark"></code></pre></div>
    </div>
    <aside>
      <div class="bug_reporting">
	<h4>Find an error or bug?</h4>
	<p>Everything on this site is available on GitHub. Head to <a href='https://github.com/chrisalbon/notes/issues/new'>and submit a suggested change</a>. Include the tutorial's URL in the issue.</p>
      </div>
    </aside>

  </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 59 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
