<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="TensorFlow/Petastorm MNIST Example using the Feature Store" />
<meta property="og:description" content="Image Classification with MNIST Using a Petastorm Dataset In this notebook we will read a training dataset saved in the Petastorm format in the project&rsquo;s feature store and use that to train a Deep CNN defined in Keras/Tensorflow to classify images of digits in the MNIST dataset.
This notebook assumes that you have already created the training datasets in the feature store, which you can do by running this notebook:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://examples.hopsworks.ai/featurestore/petastorm/petastormmnist_tensorflow/" />



<meta property="article:published_time" content="2021-02-24T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2021-02-24T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="TensorFlow/Petastorm MNIST Example using the Feature Store"/>
<meta name="twitter:description" content="Image Classification with MNIST Using a Petastorm Dataset In this notebook we will read a training dataset saved in the Petastorm format in the project&rsquo;s feature store and use that to train a Deep CNN defined in Keras/Tensorflow to classify images of digits in the MNIST dataset.
This notebook assumes that you have already created the training datasets in the feature store, which you can do by running this notebook:"/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "TensorFlow/Petastorm MNIST Example using the Feature Store",
  "url": "https://examples.hopsworks.ai/featurestore/petastorm/petastormmnist_tensorflow/",
  "wordCount": "877",
  "datePublished": "2021-02-24T00:00:00&#43;00:00",
  "dateModified": "2021-02-24T00:00:00&#43;00:00",
  "author": {
  "@type": "Person",
  "name": ""
  }
  }
</script> 

    <title>TensorFlow/Petastorm MNIST Example using the Feature Store</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://examples.hopsworks.ai/css/custom.css" rel="stylesheet">
    <link href="https://examples.hopsworks.ai/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Hopsworks Examples" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://examples.hopsworks.ai">Hopsworks Examples</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://hopsworks.ai" title="Hopsworks.ai">hopsworks.ai</a></li>
                    <li><a href="https://docs.hopsworks.ai" title="Docs">docs.hopsworks.ai</a></li>
                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
    <header>
      <div class="alert alert-warning flashcard_ad" role="alert">
	Want to learn machine learning? Try my <a href="https://machinelearningflashcards.com" class="alert-link">machine learning flashcards</a>, <a href='https://amzn.to/2HwnWty' class="alert-link">book</a>, or <a href='https://www.youtube.com/channel/UCnd4Fi-ODvuPbxR2fO2j7kA' class="alert-link">study with me.</a>.
      </div>
      <h1 class="technical_note_title">TensorFlow/Petastorm MNIST Example using the Feature Store</h1>
      <div class="technical_note_date">
	<time datetime=" 2021-02-24T00:00:00Z "> 24 Feb 2021</time>
      </div>
    </header>
    <div class="content">

      

<h1 id="image-classification-with-mnist-using-a-petastorm-dataset">Image Classification with MNIST Using a Petastorm Dataset</h1>

<p>In this notebook we will read a training dataset saved in the Petastorm format in the project&rsquo;s feature store and use that to train a Deep CNN defined in Keras/Tensorflow to classify images of digits in the MNIST dataset.</p>

<p>This notebook assumes that you have already created the training datasets in the feature store, which you can do by running this notebook:</p>

<p><a href="PetastormMNIST_CreateDataset.ipynb">Create Petastorm MNIST Dataset Notebook</a></p>

<p><img src="./../images/petastorm6.png" alt="Petastorm 6" title="Petastorm 6" /></p>

<h2 id="imports">Imports</h2>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">from hops import hdfs, featurestore, tensorboard, experiment

# IMPORTANT: must import tensorflow before petastorm.tf_utils due to a bug in petastorm
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras.callbacks import TensorBoard
import json
import numpy as np
import pydoop
from petastorm import make_reader
from petastorm.tf_utils import tf_tensors, make_petastorm_dataset</code></pre></div>
<pre><code>Starting Spark application
</code></pre>

<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1559565096638_0006</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://hopsworks0.logicalclocks.com:8088/proxy/application_1559565096638_0006/">Link</a></td><td><a target="_blank" href="http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1559565096638_0006_01_000001/demo_featurestore_admin000__meb10000">Link</a></td><td>âœ”</td></tr></table>

<pre><code>SparkSession available as 'spark'.
</code></pre>

<h2 id="constants">Constants</h2>

<p>In this tutorial we will just use static hyperparameters, you can potentially achieve better accuracy by optimizing the hyperparameters using hyperparameter search (<a href="https://hopsworks.readthedocs.io/en/latest/hopsml/hopsML.html">docs</a>)</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">TRAIN_DATASET_NAME = &#34;MNIST_train_petastorm&#34;
TEST_DATASET_NAME = &#34;MNIST_test_petastorm&#34;
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 3*BATCH_SIZE
NUM_EPOCHS = 5
STEPS_PER_EPOCH = 80</code></pre></div>
<h2 id="step-1-define-the-model">Step 1: Define The Model</h2>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def create_model():
    &#34;&#34;&#34;
    Defines a three-layer CNN with batch normalization, dropout and max pooling, relu activation, 
    and softmax output
    &#34;&#34;&#34;
    model = keras.Sequential()
    model.add(keras.layers.Conv2D(filters=32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(28,28,1))) 
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.MaxPooling2D(pool_size=2))
    model.add(keras.layers.Dropout(0.3))

    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.MaxPooling2D(pool_size=2))
    model.add(keras.layers.Dropout(0.3))

    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(128, activation=&#39;relu&#39;))
    model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(10, activation=&#39;softmax&#39;))
    return model</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">create_model().summary()</code></pre></div>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 28, 28, 32)        320       
_________________________________________________________________
batch_normalization (BatchNo (None, 28, 28, 32)        128       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         
_________________________________________________________________
dropout (Dropout)            (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     
_________________________________________________________________
batch_normalization_1 (Batch (None, 14, 14, 64)        256       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 7, 7, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               401536    
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 422,026
Trainable params: 421,834
Non-trainable params: 192
_________________________________________________________________
</code></pre>

<h2 id="step-2-define-tensorflow-dataset">Step 2: Define Tensorflow Dataset</h2>

<p>Petastorm datasets can be read directly with tensorflow by using <code>make_reader</code> and <code>make_petastorm_dataset</code> from the Petastorm library</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def create_tf_dataset(dataset_url, shuffle_buffer_size, batch_size, num_epochs):
    &#34;&#34;&#34;
    Defines the Tensorflow Dataset Abstraction from the Petastorm Dataset.
    One-hot encodes the labels.
    &#34;&#34;&#34;
    with make_reader(dataset_url, num_epochs=None, hdfs_driver=&#39;libhdfs&#39;,
                    workers_count=1, shuffle_row_groups=False) as train_reader:
        train_dataset = make_petastorm_dataset(train_reader)
        def preprocess(sample):
            return sample.image, tf.one_hot(sample.digit, 10)
        return train_dataset.map(preprocess).shuffle(shuffle_buffer_size).batch(batch_size).repeat(num_epochs)</code></pre></div>
<h2 id="step-3-put-it-all-together-in-a-training-function">Step 3: Put it All Together in a Training Function</h2>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def train_fn():
    # get dataset path from the featurestore
    train_dataset_path = featurestore.get_training_dataset_path(TRAIN_DATASET_NAME)
    # get dataset
    dataset = create_tf_dataset(train_dataset_path, SHUFFLE_BUFFER_SIZE, BATCH_SIZE, NUM_EPOCHS)
    # define model
    model = create_model()
    # define optimizer
    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=[&#39;accuracy&#39;])
    # setup tensorboard
    tb_callback = TensorBoard(log_dir=tensorboard.logdir(), histogram_freq=0,write_graph=True, write_images=True)
    # setup model checkpointing
    model_ckpt_callback = keras.callbacks.ModelCheckpoint(tensorboard.logdir() + &#39;/checkpoint-{epoch}.h5&#39;,monitor=&#39;acc&#39;, verbose=0, save_best_only=True)
    callbacks = [tb_callback, model_ckpt_callback]
    # train model
    history = model.fit(dataset, epochs=NUM_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=callbacks)
    # save training history to HDFS
    results_path = hdfs.project_path() + &#34;mnist/mnist_train_results.txt&#34;
    hdfs.dump(json.dumps(history.history), results_path)
    # save trained model
    model.save(&#34;mnist_tf_ps.h5&#34;) #Keras can&#39;t save to HDFS in the current version so save to local fs first
    hdfs.copy_to_hdfs(&#34;mnist_tf_ps.h5&#34;, hdfs.project_path() + &#34;mnist&#34;, overwrite=True) # copy from local fs to hdfs
    # return latest accuracy
    return history.history[&#34;acc&#34;][-1]</code></pre></div>
<h2 id="step-4-training-experiments">Step 4: Training Experiments</h2>

<p>We can use the experiments service to run our train_fn function and handle things like Tensorboard, logging, versioning etc. While the experiment is running you can monitor it using Tensorboard and the logs, it is explained in the README <a href="https://github.com/logicalclocks/hops-util-py">here</a>.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">experiment.launch(train_fn, name=&#34;mnist_tf_ps&#34;, 
                  description=&#34;Petastorm MNIST Tensorflow Example&#34;, local_logdir=True)</code></pre></div>
<pre><code>Finished Experiment 

'hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/Experiments/application_1559565096638_0006/launcher/run.2'
</code></pre>

<h2 id="step-5-plot-training-results">Step 5: Plot Training Results</h2>

<p>Inside the <code>train_fn</code> function we saved the training history to HDFS, which means we can later read it in %%local mode for plotting.</p>

<h3 id="load-training-results-from-hdfs">Load Training Results From HDFS</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
import json
from hops import hdfs
import matplotlib.pyplot as plt
from pylab import rcParams
results_path = hdfs.project_path() + &#34;mnist/mnist_train_results.txt&#34;
results = json.loads(hdfs.load(results_path))</code></pre></div>
<h3 id="plot-loss-epoch-during-training">Plot Loss/Epoch During Training</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
y = results[&#34;loss&#34;] #loss
x = list(range(1, len(y)+1))#epoch
plt.title(&#34;Loss per Epoch - MNIST Training&#34;)
plt.xlabel(&#34;Epoch&#34;)
plt.ylabel(&#34;Loss&#34;)
plt.plot(x,y)</code></pre></div>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f1980fa4550&gt;]
</code></pre>

<p><img src="PetastormMNIST_Tensorflow_files/PetastormMNIST_Tensorflow_19_1.png" alt="png" /></p>

<h3 id="plot-accuracy-epoch-during-training">Plot Accuracy/Epoch During Training</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
%matplotlib inline
y = results[&#34;acc&#34;] #acc
x = list(range(1, len(y)+1))#epoch
plt.title(&#34;Accuracy per Epoch - MNIST Training&#34;)
plt.xlabel(&#34;Epoch&#34;)
plt.ylabel(&#34;Accuracy&#34;)
plt.plot(x,y)</code></pre></div>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x7f1980f89358&gt;]
</code></pre>

<p><img src="PetastormMNIST_Tensorflow_files/PetastormMNIST_Tensorflow_21_1.png" alt="png" /></p>

<h2 id="step-6-evaluation-using-trained-model-and-test-dataset">Step 6: Evaluation Using Trained Model and Test Dataset</h2>

<p>Inside the <code>train_fn</code> function we saved the trained model to HDFS in the hdf5 format. We can load the weights of this model and use for serving predictions or for evaluation, in this example we will evaluate the model against the test set.</p>

<h3 id="load-model-weights">Load Model Weights</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">model_path_hdfs = hdfs.project_path() + &#34;mnist/&#34; + &#34;mnist_tf_ps.h5&#34;</code></pre></div>
<p>In future releases of Tensorflow, Keras will be able to read directly from HDFS, but currently it is not supported. To get around this we can download the hdf5 model in the local file system and load it from there using <code>model.load_weights()</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">local_path = hdfs.copy_to_local(model_path_hdfs, overwrite=True)</code></pre></div>
<pre><code>Started copying hdfs://10.0.2.15:8020/Projects/demo_featurestore_admin000/mnist/mnist_tf_ps.h5 to local disk on path /srv/hops/hopsdata/tmp/nm-local-dir/usercache/FatxKv_Lnvnybr5ulVBa6ZBxZaETIhWRCL9ga70hOV8/appcache/application_1559565096638_0006/container_e01_1559565096638_0006_01_000001/

Finished copying
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">local_path</code></pre></div>
<pre><code>'/srv/hops/hopsdata/tmp/nm-local-dir/usercache/FatxKv_Lnvnybr5ulVBa6ZBxZaETIhWRCL9ga70hOV8/appcache/application_1559565096638_0006/container_e01_1559565096638_0006_01_000001//mnist_tf_ps.h5'
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">loaded_model = create_model()
loaded_model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=[&#39;accuracy&#39;])
loaded_model.load_weights(local_path)</code></pre></div>
<h3 id="evaluate-loaded-model">Evaluate Loaded Model</h3>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark"># We have 10 000 Test examples, we can evaluate in batches of 100 to speed up the process
BATCH_SIZE = 100
NUM_EPOCHS = 100 
# get dataset path from the featurestore
test_dataset_path = featurestore.get_training_dataset_path(TEST_DATASET_NAME)
test_dataset = create_tf_dataset(test_dataset_path, SHUFFLE_BUFFER_SIZE, BATCH_SIZE, NUM_EPOCHS)</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">score = loaded_model.evaluate(test_dataset, verbose=1, steps=1)
print(&#39;Test loss:&#39;, score[0])
print(&#39;Test accuracy:&#39;, score[1])</code></pre></div>
<pre><code>1/1 [==============================] - 1s 768ms/step
Test loss: 0.0319652184844017
Test accuracy: 0.9900000095367432
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark"></code></pre></div>
    </div>
    <aside>
      <div class="bug_reporting">
	<h4>Find an error or bug?</h4>
	<p>Everything on this site is available on GitHub. Head to <a href='https://github.com/chrisalbon/notes/issues/new'>and submit a suggested change</a>. Include the tutorial's URL in the issue.</p>
      </div>
    </aside>

  </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 59 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
