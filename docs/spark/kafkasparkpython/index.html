<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Kafka PySpark Example" />
<meta property="og:description" content="Producing and Consuming Messages to/from Kafka and plotting, using python producer and spark consumer To run this notebook you must already have created a Kafka topic
Imports We use utility functions from the hops library to make Kafka configuration simple
Dependencies:
 hops-py-util confluent-kafka  from hops import kafka from hops import tls from hops import hdfs from confluent_kafka import Producer, Consumer import numpy as np from pyspark.sql.types import StructType, StructField, FloatType, TimestampType Starting Spark application   IDYARN Application IDKindStateSpark UIDriver logCurrent session?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://examples.hopsworks.ai/spark/kafkasparkpython/" />



<meta property="article:published_time" content="2021-02-24T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2021-02-24T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kafka PySpark Example"/>
<meta name="twitter:description" content="Producing and Consuming Messages to/from Kafka and plotting, using python producer and spark consumer To run this notebook you must already have created a Kafka topic
Imports We use utility functions from the hops library to make Kafka configuration simple
Dependencies:
 hops-py-util confluent-kafka  from hops import kafka from hops import tls from hops import hdfs from confluent_kafka import Producer, Consumer import numpy as np from pyspark.sql.types import StructType, StructField, FloatType, TimestampType Starting Spark application   IDYARN Application IDKindStateSpark UIDriver logCurrent session?"/>
<meta name="generator" content="Hugo 0.40.3" />

    
<script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Kafka PySpark Example",
  "url": "https://examples.hopsworks.ai/spark/kafkasparkpython/",
  "wordCount": "2072",
  "datePublished": "2021-02-24T00:00:00&#43;00:00",
  "dateModified": "2021-02-24T00:00:00&#43;00:00",
  "author": {
  "@type": "Person",
  "name": ""
  }
  }
</script> 

    <title>Kafka PySpark Example</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://examples.hopsworks.ai/css/custom.css" rel="stylesheet">
    <link href="https://examples.hopsworks.ai/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    <link href="" rel="alternate" type="application/rss+xml" title="Hopsworks Examples" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="https://examples.hopsworks.ai">Hopsworks Examples</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="nav navbar-nav">
                    <li><a href="https://hopsworks.ai" title="Hopsworks.ai">hopsworks.ai</a></li>
                    <li><a href="https://docs.hopsworks.ai" title="Docs">docs.hopsworks.ai</a></li>
                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
    <header>
      <div class="alert alert-warning flashcard_ad" role="alert">
	Want to learn machine learning? Try my <a href="https://machinelearningflashcards.com" class="alert-link">machine learning flashcards</a>, <a href='https://amzn.to/2HwnWty' class="alert-link">book</a>, or <a href='https://www.youtube.com/channel/UCnd4Fi-ODvuPbxR2fO2j7kA' class="alert-link">study with me.</a>.
      </div>
      <h1 class="technical_note_title">Kafka PySpark Example</h1>
      <div class="technical_note_date">
	<time datetime=" 2021-02-24T00:00:00Z "> 24 Feb 2021</time>
      </div>
    </header>
    <div class="content">

      

<h1 id="producing-and-consuming-messages-to-from-kafka-and-plotting-using-python-producer-and-spark-consumer">Producing and Consuming Messages to/from Kafka and plotting, using python producer and spark consumer</h1>

<p>To run this notebook you must already have created a Kafka topic</p>

<h2 id="imports">Imports</h2>

<p>We use utility functions from the hops library to make Kafka configuration simple</p>

<p>Dependencies:</p>

<ul>
<li>hops-py-util</li>
<li>confluent-kafka</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">from hops import kafka
from hops import tls
from hops import hdfs
from confluent_kafka import Producer, Consumer
import numpy as np
from pyspark.sql.types import StructType, StructField, FloatType, TimestampType</code></pre></div>
<pre><code>Starting Spark application
</code></pre>

<table>
<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>34</td><td>application_1538483294796_0037</td><td>pyspark</td><td>idle</td><td><a target="_blank" href="http://hopsworks0:8088/proxy/application_1538483294796_0037/">Link</a></td><td><a target="_blank" href="http://hopsworks0:8042/node/containerlogs/container_e01_1538483294796_0037_01_000001/KafkaPython__meb10000">Link</a></td><td>âœ”</td></tr></table>

<pre><code>SparkSession available as 'spark'.
</code></pre>

<h2 id="constants">Constants</h2>

<p>Update the <code>TOPIC_NAME</code> field to reflect the name of your Kafka topic that you want to read/write from/to
Update the <code>OUTPUT_PATH</code> field to where the output data should be written</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">TOPIC_NAME = &#34;test&#34;
OUTPUT_PATH = &#34;/Projects/&#34; + hdfs.project_name() + &#34;/Resources/data-csv&#34;
CHECKPOINT_PATH = &#34;/Projects/&#34; + hdfs.project_name() + &#34;/Resources/checkpoint-csv&#34;</code></pre></div>
<h2 id="produce-some-messages-to-the-topic">Produce some Messages to the Topic</h2>

<p>Specify the configuration, using hops-py-util</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">config = {
    &#34;bootstrap.servers&#34;: kafka.get_broker_endpoints(),
    &#34;security.protocol&#34;: kafka.get_security_protocol(),
    &#34;ssl.ca.location&#34;: tls.get_ca_chain_location(),
    &#34;ssl.certificate.location&#34;: tls.get_client_certificate_location(),
    &#34;ssl.key.location&#34;: tls.get_client_key_location(),
    &#34;group.id&#34;: &#34;something&#34;
}</code></pre></div>
<p>Create producer with kafka-confluent API</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">producer = Producer(config)</code></pre></div>
<p>producer.produce is an asychronous call so we create a callback to be notified when messages are delivered</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">def delivery_callback(err, msg):
    &#34;&#34;&#34;
    Optional per-message delivery callback (triggered by poll() or flush())
    when a message has been successfully delivered or permanently
    failed delivery (after retries).
    &#34;&#34;&#34;
    if err:
        print(&#34;Message failed delivery: {}&#34;.format(err))
    else:
        print(&#39;Message: {} delivered to topic: {}, partition: {}, offset: {}, timestamp: {}&#39;.format(msg.value(), msg.topic(), msg.partition(), msg.offset(), msg.timestamp()))</code></pre></div>
<p>Produce 100 random number-messages to the topic</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">normal_rnd_dist = np.random.normal(0, 0.1, 100)
for i in range(0, 100):
    producer.produce(TOPIC_NAME, str(normal_rnd_dist[i]), &#34;key&#34;, callback=delivery_callback)
    
# Trigger the sending of all messages to the brokers, 20sec timeout
producer.flush(20) </code></pre></div>
<pre><code>Message: -0.113515966706715 delivered to topic: test, partition: 1, offset: 450, timestamp: (1, 1538570713617L)
Message: -0.13372629136129505 delivered to topic: test, partition: 1, offset: 451, timestamp: (1, 1538570713617L)
Message: -0.030109530115267787 delivered to topic: test, partition: 1, offset: 452, timestamp: (1, 1538570713617L)
Message: 0.09914600908518259 delivered to topic: test, partition: 1, offset: 453, timestamp: (1, 1538570713617L)
Message: -0.13942848544599754 delivered to topic: test, partition: 1, offset: 454, timestamp: (1, 1538570713617L)
Message: 0.06244323355006787 delivered to topic: test, partition: 1, offset: 455, timestamp: (1, 1538570713617L)
Message: 0.12423451639758504 delivered to topic: test, partition: 1, offset: 456, timestamp: (1, 1538570713617L)
Message: 0.01979165601399879 delivered to topic: test, partition: 1, offset: 457, timestamp: (1, 1538570713617L)
Message: 0.13896608728374968 delivered to topic: test, partition: 1, offset: 458, timestamp: (1, 1538570713617L)
Message: -0.22441498120997885 delivered to topic: test, partition: 1, offset: 459, timestamp: (1, 1538570713617L)
Message: 0.1099915077238914 delivered to topic: test, partition: 1, offset: 460, timestamp: (1, 1538570713617L)
Message: 0.11565688359003806 delivered to topic: test, partition: 1, offset: 461, timestamp: (1, 1538570713617L)
Message: 0.01708963484491049 delivered to topic: test, partition: 1, offset: 462, timestamp: (1, 1538570713617L)
Message: -0.10090270372296102 delivered to topic: test, partition: 1, offset: 463, timestamp: (1, 1538570713617L)
Message: -0.05870498166968106 delivered to topic: test, partition: 1, offset: 464, timestamp: (1, 1538570713617L)
Message: -0.05802497535874125 delivered to topic: test, partition: 1, offset: 465, timestamp: (1, 1538570713617L)
Message: 0.06852280438233449 delivered to topic: test, partition: 1, offset: 466, timestamp: (1, 1538570713617L)
Message: -0.07384578426064682 delivered to topic: test, partition: 1, offset: 467, timestamp: (1, 1538570713617L)
Message: -0.049791032528996144 delivered to topic: test, partition: 1, offset: 468, timestamp: (1, 1538570713617L)
Message: -0.0812131295770192 delivered to topic: test, partition: 1, offset: 469, timestamp: (1, 1538570713617L)
Message: -0.16636558959634237 delivered to topic: test, partition: 1, offset: 470, timestamp: (1, 1538570713617L)
Message: -0.05029487377263785 delivered to topic: test, partition: 1, offset: 471, timestamp: (1, 1538570713617L)
Message: -0.0634611358539206 delivered to topic: test, partition: 1, offset: 472, timestamp: (1, 1538570713617L)
Message: 0.047822771574680845 delivered to topic: test, partition: 1, offset: 473, timestamp: (1, 1538570713617L)
Message: -0.035862408830423124 delivered to topic: test, partition: 1, offset: 474, timestamp: (1, 1538570713617L)
Message: -0.10771865791104468 delivered to topic: test, partition: 1, offset: 475, timestamp: (1, 1538570713617L)
Message: -0.09621414290887964 delivered to topic: test, partition: 1, offset: 476, timestamp: (1, 1538570713617L)
Message: -0.12471722823698611 delivered to topic: test, partition: 1, offset: 477, timestamp: (1, 1538570713617L)
Message: -0.04144248818411699 delivered to topic: test, partition: 1, offset: 478, timestamp: (1, 1538570713617L)
Message: -0.022852969314669194 delivered to topic: test, partition: 1, offset: 479, timestamp: (1, 1538570713617L)
Message: 0.03812687911561363 delivered to topic: test, partition: 1, offset: 480, timestamp: (1, 1538570713617L)
Message: 0.11600386213341707 delivered to topic: test, partition: 1, offset: 481, timestamp: (1, 1538570713617L)
Message: 0.09175683466848605 delivered to topic: test, partition: 1, offset: 482, timestamp: (1, 1538570713617L)
Message: -0.12798420753009673 delivered to topic: test, partition: 1, offset: 483, timestamp: (1, 1538570713617L)
Message: 0.008424637864889025 delivered to topic: test, partition: 1, offset: 484, timestamp: (1, 1538570713617L)
Message: 0.008146330017132953 delivered to topic: test, partition: 1, offset: 485, timestamp: (1, 1538570713617L)
Message: -0.02340418188111429 delivered to topic: test, partition: 1, offset: 486, timestamp: (1, 1538570713617L)
Message: -0.12962750536767612 delivered to topic: test, partition: 1, offset: 487, timestamp: (1, 1538570713617L)
Message: -0.07628248175485523 delivered to topic: test, partition: 1, offset: 488, timestamp: (1, 1538570713617L)
Message: -0.09094972953830724 delivered to topic: test, partition: 1, offset: 489, timestamp: (1, 1538570713617L)
Message: 0.04784824141202446 delivered to topic: test, partition: 1, offset: 490, timestamp: (1, 1538570713617L)
Message: -0.0008079695296911359 delivered to topic: test, partition: 1, offset: 491, timestamp: (1, 1538570713617L)
Message: -0.12966562414756075 delivered to topic: test, partition: 1, offset: 492, timestamp: (1, 1538570713617L)
Message: 0.15101829400472663 delivered to topic: test, partition: 1, offset: 493, timestamp: (1, 1538570713617L)
Message: 0.07271092045856317 delivered to topic: test, partition: 1, offset: 494, timestamp: (1, 1538570713617L)
Message: -0.14274124593717222 delivered to topic: test, partition: 1, offset: 495, timestamp: (1, 1538570713617L)
Message: -0.07738435882850264 delivered to topic: test, partition: 1, offset: 496, timestamp: (1, 1538570713617L)
Message: 0.017391910899179314 delivered to topic: test, partition: 1, offset: 497, timestamp: (1, 1538570713617L)
Message: 0.11182130735642559 delivered to topic: test, partition: 1, offset: 498, timestamp: (1, 1538570713617L)
Message: 0.17939618586484868 delivered to topic: test, partition: 1, offset: 499, timestamp: (1, 1538570713617L)
Message: -0.030430503280300954 delivered to topic: test, partition: 1, offset: 500, timestamp: (1, 1538570713617L)
Message: 0.010896595939977088 delivered to topic: test, partition: 1, offset: 501, timestamp: (1, 1538570713617L)
Message: 0.07153886574524339 delivered to topic: test, partition: 1, offset: 502, timestamp: (1, 1538570713617L)
Message: -0.027702479187509583 delivered to topic: test, partition: 1, offset: 503, timestamp: (1, 1538570713617L)
Message: -0.025904346778860443 delivered to topic: test, partition: 1, offset: 504, timestamp: (1, 1538570713617L)
Message: 0.054551892172761886 delivered to topic: test, partition: 1, offset: 505, timestamp: (1, 1538570713617L)
Message: -0.027343347495906986 delivered to topic: test, partition: 1, offset: 506, timestamp: (1, 1538570713617L)
Message: 0.14735111070401619 delivered to topic: test, partition: 1, offset: 507, timestamp: (1, 1538570713617L)
Message: 0.03346639447277224 delivered to topic: test, partition: 1, offset: 508, timestamp: (1, 1538570713617L)
Message: 0.1826423337750613 delivered to topic: test, partition: 1, offset: 509, timestamp: (1, 1538570713617L)
Message: 0.04600033520055015 delivered to topic: test, partition: 1, offset: 510, timestamp: (1, 1538570713617L)
Message: -0.3554881125189931 delivered to topic: test, partition: 1, offset: 511, timestamp: (1, 1538570713617L)
Message: -0.01938300986377106 delivered to topic: test, partition: 1, offset: 512, timestamp: (1, 1538570713617L)
Message: -0.06868591688742505 delivered to topic: test, partition: 1, offset: 513, timestamp: (1, 1538570713617L)
Message: 0.04342203531972067 delivered to topic: test, partition: 1, offset: 514, timestamp: (1, 1538570713617L)
Message: 0.026605428384062164 delivered to topic: test, partition: 1, offset: 515, timestamp: (1, 1538570713617L)
Message: -0.23891058137348586 delivered to topic: test, partition: 1, offset: 516, timestamp: (1, 1538570713617L)
Message: 0.063107528304626 delivered to topic: test, partition: 1, offset: 517, timestamp: (1, 1538570713617L)
Message: -0.06384750369644372 delivered to topic: test, partition: 1, offset: 518, timestamp: (1, 1538570713617L)
Message: 0.09966984328987485 delivered to topic: test, partition: 1, offset: 519, timestamp: (1, 1538570713617L)
Message: 0.19078821436517882 delivered to topic: test, partition: 1, offset: 520, timestamp: (1, 1538570713617L)
Message: 0.11490040429088477 delivered to topic: test, partition: 1, offset: 521, timestamp: (1, 1538570713617L)
Message: 0.003344215541099674 delivered to topic: test, partition: 1, offset: 522, timestamp: (1, 1538570713617L)
Message: 0.01482845788007928 delivered to topic: test, partition: 1, offset: 523, timestamp: (1, 1538570713617L)
Message: -0.03389124274730744 delivered to topic: test, partition: 1, offset: 524, timestamp: (1, 1538570713618L)
Message: -0.0017421597393584454 delivered to topic: test, partition: 1, offset: 525, timestamp: (1, 1538570713618L)
Message: 0.02696093734246954 delivered to topic: test, partition: 1, offset: 526, timestamp: (1, 1538570713618L)
Message: -0.0685537177874654 delivered to topic: test, partition: 1, offset: 527, timestamp: (1, 1538570713618L)
Message: 0.008024171552555073 delivered to topic: test, partition: 1, offset: 528, timestamp: (1, 1538570713618L)
Message: -0.007976413182145927 delivered to topic: test, partition: 1, offset: 529, timestamp: (1, 1538570713618L)
Message: -0.1760613222741813 delivered to topic: test, partition: 1, offset: 530, timestamp: (1, 1538570713618L)
Message: -0.04272057630253764 delivered to topic: test, partition: 1, offset: 531, timestamp: (1, 1538570713618L)
Message: -0.08676547080431635 delivered to topic: test, partition: 1, offset: 532, timestamp: (1, 1538570713618L)
Message: -0.005037102529253483 delivered to topic: test, partition: 1, offset: 533, timestamp: (1, 1538570713618L)
Message: -0.026670469314402163 delivered to topic: test, partition: 1, offset: 534, timestamp: (1, 1538570713618L)
Message: -0.07311083242634318 delivered to topic: test, partition: 1, offset: 535, timestamp: (1, 1538570713618L)
Message: -0.023465465848581793 delivered to topic: test, partition: 1, offset: 536, timestamp: (1, 1538570713618L)
Message: 0.11321026392440192 delivered to topic: test, partition: 1, offset: 537, timestamp: (1, 1538570713618L)
Message: -0.15336795722952007 delivered to topic: test, partition: 1, offset: 538, timestamp: (1, 1538570713618L)
Message: -0.2558586664453579 delivered to topic: test, partition: 1, offset: 539, timestamp: (1, 1538570713618L)
Message: 0.008079556492722571 delivered to topic: test, partition: 1, offset: 540, timestamp: (1, 1538570713618L)
Message: -0.0407650872978403 delivered to topic: test, partition: 1, offset: 541, timestamp: (1, 1538570713618L)
Message: -0.03444415125912261 delivered to topic: test, partition: 1, offset: 542, timestamp: (1, 1538570713618L)
Message: 0.05797445559620995 delivered to topic: test, partition: 1, offset: 543, timestamp: (1, 1538570713618L)
Message: -0.05231783634896588 delivered to topic: test, partition: 1, offset: 544, timestamp: (1, 1538570713618L)
Message: 0.11925067273119393 delivered to topic: test, partition: 1, offset: 545, timestamp: (1, 1538570713618L)
Message: -0.050746734607947985 delivered to topic: test, partition: 1, offset: 546, timestamp: (1, 1538570713618L)
Message: -0.12585044629192257 delivered to topic: test, partition: 1, offset: 547, timestamp: (1, 1538570713618L)
Message: -0.20932999866560886 delivered to topic: test, partition: 1, offset: 548, timestamp: (1, 1538570713618L)
Message: 0.07143653458156844 delivered to topic: test, partition: 1, offset: 549, timestamp: (1, 1538570713618L)
0
</code></pre>

<h2 id="consume-the-kafka-topic-using-spark-and-write-to-a-sink">Consume the Kafka Topic using Spark and Write to a Sink</h2>

<p>The below snippet creates a streaming DataFrame with Kafka as a data source. Spark is lazy so it will not start streaming the data from Kafka into the dataframe until we specify an output sink (which we do later on in this notebook)</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">#lazy
df = spark \
  .readStream \
  .format(&#34;kafka&#34;) \
  .option(&#34;kafka.bootstrap.servers&#34;, kafka.get_broker_endpoints()) \
  .option(&#34;kafka.security.protocol&#34;,kafka.get_security_protocol()) \
  .option(&#34;kafka.ssl.truststore.location&#34;, tls.get_trust_store()) \
  .option(&#34;kafka.ssl.truststore.password&#34;, tls.get_key_store_pwd()) \
  .option(&#34;kafka.ssl.keystore.location&#34;, tls.get_key_store()) \
  .option(&#34;kafka.ssl.keystore.password&#34;, tls.get_key_store_pwd()) \
  .option(&#34;kafka.ssl.key.password&#34;, tls.get_trust_store_pwd()) \
  .option(&#34;kafka.ssl.endpoint.identification.algorithm&#34;, &#34;&#34;) \
  .option(&#34;subscribe&#34;, TOPIC_NAME) \
  .load()</code></pre></div>
<p>When using Kafka as a data source, Spark gives us a default kafka schema as printed below</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">df.printSchema()</code></pre></div>
<pre><code>root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
</code></pre>

<p>We are using the Spark structured streaming engine, which means that we can express stream queries just as we would do in batch jobs.</p>

<p>Below we filter the input stream to select only the message values and their timestamp</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">messages = df.selectExpr(&#34;CAST(value AS STRING)&#34;, &#34;timestamp&#34;).selectExpr(&#34;CAST(value AS FLOAT)&#34;, &#34;timestamp&#34;)</code></pre></div>
<p>Specify the output query and the sink of the stream job to be a CSV file in HopsFS.</p>

<p>By using checkpointing and a WAL, spark gives us end-to-end exactly-once fault-tolerance</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">query = messages \
       .writeStream \
       .format(&#34;csv&#34;) \
       .option(&#34;path&#34;, OUTPUT_PATH) \
       .option(&#34;checkpointLocation&#34;, CHECKPOINT_PATH) \
       .start()</code></pre></div>
<p>Run the streaming job, in theory streaming jobs should run forever.</p>

<p>However for this notebook example we are just going to read for 10 seconds and dump the output to the sink CSV file</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">query.awaitTermination(timeout=20) # 20 seconds timeout
query.stop()</code></pre></div>
<p style="color:red">
Sometimes there is a delay before the spark job starts writing to the sink, </p>

<p style="color:red">before going on to the next step in this notebook, go to your HDFS `OUTPUT_PATH` 
and verify that the csv output is not empty.</p>

<p style="color:red">If it is empty, re-run the query above</p>

<h2 id="read-the-data-from-the-sink">Read the Data from the Sink</h2>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">schema = StructType([
    StructField(&#34;value&#34;, FloatType(), True),
    StructField(&#34;timestamp&#34;, TimestampType(), True)])

df1 = spark.read \
     .format(&#34;csv&#34;) \
     .option(&#34;header&#34;, &#34;false&#34;) \
     .schema(schema) \
     .load(OUTPUT_PATH)</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">df1.printSchema()</code></pre></div>
<pre><code>root
 |-- value: float (nullable = true)
 |-- timestamp: timestamp (nullable = true)
</code></pre>

<h2 id="visualize-the-dataframe-using-sparkmagic">Visualize the DataFrame using SparkMagic</h2>

<p style="color:red">This visualization currenly only works in Python 2.*</p>

<p>This command copies the spark dataframe from the cluster
to the local machine and converts it to a pandas dataframe named &ldquo;df1&rdquo;.
This pandas dataframe is available in all cells started with the sparkmagic: %%local and can be used for
visualizations and plotting.</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%spark -o df1</code></pre></div>
<p>Below is sparkmagics default plotting</p>
<div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
df1</code></pre></div>
<pre><code>VkJveChjaGlsZHJlbj0oSEJveChjaGlsZHJlbj0oSFRNTCh2YWx1ZT11J1R5cGU6JyksIEJ1dHRvbihkZXNjcmlwdGlvbj11J1RhYmxlJywgbGF5b3V0PUxheW91dCh3aWR0aD11JzcwcHgnKSzigKY=




Output()
</code></pre>

<p>Install matplotlib on the local machine in case it is not already installed</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">%%bash
pip install --user matplotlib</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-pyspark" data-lang="pyspark">%%local
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline 
from pylab import rcParams
rcParams[&#39;figure.figsize&#39;] = 15, 10
hist, bins = np.histogram(df1[&#34;value&#34;], bins=50)
width = 0.7 * (bins[1] - bins[0])
center = (bins[:-1] + bins[1:]) / 2
plt.bar(center, hist, align=&#39;center&#39;, width=width)
plt.title(&#34;Histogram of values&#34;)
plt.xlabel(&#34;value&#34;)
plt.ylabel(&#34;count&#34;)
plt.show()</code></pre></div>
<p><img src="KafkaSparkPython_files/KafkaSparkPython_37_0.png" alt="png" /></p>

    </div>
    <aside>
      <div class="bug_reporting">
	<h4>Find an error or bug?</h4>
	<p>Everything on this site is available on GitHub. Head to <a href='https://github.com/chrisalbon/notes/issues/new'>and submit a suggested change</a>. Include the tutorial's URL in the issue.</p>
      </div>
    </aside>

  </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">This project contains 59 pages and is available on <a href="https://github.com/napsterinblue/notes">GitHub</a>. Copyright &copy; NapsterInBlue, <time datetime="2018">2018</time>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>
