{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Store Quick Start\n",
    "\n",
    "This notebook gives you a quick overview of how you can intergrate the feature store service on Hopsworks into your machine learning pipeline. We'll go over four steps:\n",
    "\n",
    "1. Generate some sample data (rather than reading data from disk just to make this notebook stand-alone)\n",
    "2. Do some feature engineering on the data\n",
    "3. **Save the engineered features to the feature store**\n",
    "4. **Select a group of the features from the feature store and create a managed training dataset of tf records in the feature store**\n",
    "5. Train a model on the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We'll use numpy and pandas for data generation, pyspark for feature engineering, tensorflow and keras for model training, and the hops `featurestore` library for interacting with the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import Row\n",
    "from hops import featurestore\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "Lets generate two sample datasets:\n",
    "\n",
    "1. `houses_for_sale_data`:\n",
    "\n",
    "```bash\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "|area_id|house_id|       house_worth|         house_age|        house_size|\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "|      1|       0| 11678.15482418699|133.88670106643886|366.80067322738535|\n",
    "|      1|       1| 2290.436167500643|15994.969706808222|195.84014889823976|\n",
    "|      1|       2| 8380.774578431328|1994.8576926471007|1544.5164614303735|\n",
    "|      1|       3|11641.224696102923|23104.501275562343|1673.7222604337876|\n",
    "|      1|       4| 5382.089422436954| 13903.43637058141| 274.2912104765028|\n",
    "+-------+--------+------------------+------------------+------------------+\n",
    "\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- house_id: long (nullable = true)\n",
    " |-- house_worth: double (nullable = true)\n",
    " |-- house_age: double (nullable = true)\n",
    " |-- house_size: double (nullable = true)\n",
    "```\n",
    "2. `houses_sold_data``\n",
    "```bash\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "|      1|                0|                0| 70073.06059070028|\n",
    "|      1|                1|               15| 146.9198329740602|\n",
    "|      1|                2|                6|  594.802165433149|\n",
    "|      1|                3|               10| 77187.84123130841|\n",
    "|      1|                4|                1|110627.48922722359|\n",
    "+-------+-----------------+-----------------+------------------+\n",
    "\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- house_purchase_id: long (nullable = true)\n",
    " |-- number_of_bidders: long (nullable = true)\n",
    " |-- sold_for_amount: double (nullable = true)\n",
    "```\n",
    "\n",
    "We'll use this data for predicting what a house is sold for based on features about the **area** where the house is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of `houses_for_sale_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ids = list(range(1,51))\n",
    "house_sizes = []\n",
    "house_worths = []\n",
    "house_ages = []\n",
    "house_area_ids = []\n",
    "for i in area_ids:\n",
    "    for j in list(range(1,100)):\n",
    "        house_sizes.append(abs(np.random.normal()*1000)/i)\n",
    "        house_worths.append(abs(np.random.normal()*10000)/i)\n",
    "        house_ages.append(abs(np.random.normal()*10000)/i)\n",
    "        house_area_ids.append(i)\n",
    "house_ids = list(range(len(house_area_ids)))\n",
    "houses_for_sale_data  = pd.DataFrame({\n",
    "        'area_id':house_area_ids,\n",
    "        'house_id':house_ids,\n",
    "        'house_worth': house_worths,\n",
    "        'house_age': house_ages,\n",
    "        'house_size': house_sizes\n",
    "    })\n",
    "houses_for_sale_data_spark_df = sqlContext.createDataFrame(houses_for_sale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+------------------+------------------+\n",
      "|area_id|house_id|       house_worth|         house_age|        house_size|\n",
      "+-------+--------+------------------+------------------+------------------+\n",
      "|      1|       0| 531.9876527313653| 8114.466859391631|155.25702875144782|\n",
      "|      1|       1|15344.430783478807|3147.5665452473127| 83.71893254128386|\n",
      "|      1|       2|18357.569890383398|1199.1734241476324| 854.9770429830093|\n",
      "|      1|       3| 2807.313400895764| 540.1950456572356|31.025385992686427|\n",
      "|      1|       4|2160.0170197342927| 2563.566293972737|271.35295036614554|\n",
      "+-------+--------+------------------+------------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "houses_for_sale_data_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_id: long (nullable = true)\n",
      " |-- house_id: long (nullable = true)\n",
      " |-- house_worth: double (nullable = true)\n",
      " |-- house_age: double (nullable = true)\n",
      " |-- house_size: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "houses_for_sale_data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of `houses_sold_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_purchased_amounts = []\n",
    "house_purchases_bidders = []\n",
    "house_purchases_area_ids = []\n",
    "for i in area_ids:\n",
    "    for j in list(range(1,1000)):\n",
    "        house_purchased_amounts.append(abs(np.random.exponential()*100000)/i)\n",
    "        house_purchases_bidders.append(int(abs(np.random.exponential()*10)/i))\n",
    "        house_purchases_area_ids.append(i)\n",
    "house_purchase_ids = list(range(len(house_purchases_bidders)))\n",
    "houses_sold_data  = pd.DataFrame({\n",
    "        'area_id':house_purchases_area_ids,\n",
    "        'house_purchase_id':house_purchase_ids,\n",
    "        'number_of_bidders': house_purchases_bidders,\n",
    "        'sold_for_amount': house_purchased_amounts\n",
    "    })\n",
    "houses_sold_data_spark_df = sqlContext.createDataFrame(houses_sold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+\n",
      "|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "|      1|                0|                3| 77660.62755542179|\n",
      "|      1|                1|                6| 22192.75497087212|\n",
      "|      1|                2|                4| 12804.50521418885|\n",
      "|      1|                3|                6|204152.64327486677|\n",
      "|      1|                4|               21|59384.149525606976|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "houses_sold_data_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_id: long (nullable = true)\n",
      " |-- house_purchase_id: long (nullable = true)\n",
      " |-- number_of_bidders: long (nullable = true)\n",
      " |-- sold_for_amount: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "houses_sold_data_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Lets generate some aggregate features such as sum and averages from our datasets. \n",
    "\n",
    "1. `houses_for_sale_features`:\n",
    "\n",
    "```bash\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- avg_house_age: double (nullable = true)\n",
    " |-- avg_house_size: double (nullable = true)\n",
    " |-- avg_house_worth: double (nullable = true)\n",
    " |-- sum_house_age: double (nullable = true)\n",
    " |-- sum_house_size: double (nullable = true)\n",
    " |-- sum_house_worth: double (nullable = true)\n",
    "```\n",
    "\n",
    "2. `houses_sold_features`\n",
    "\n",
    "```bash\n",
    " |-- area_id: long (nullable = true)\n",
    " |-- avg_num_bidders: double (nullable = true)\n",
    " |-- avg_sold_for: double (nullable = true)\n",
    " |-- sum_number_of_bidders: long (nullable = true)\n",
    " |-- sum_sold_for_amount: double (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features From `houses_for_sale_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").sum()\n",
    "count_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").count()\n",
    "sum_count_houses_for_sale_df = sum_houses_for_sale_df.join(count_houses_for_sale_df, \"area_id\")\n",
    "sum_count_houses_for_sale_df = sum_count_houses_for_sale_df \\\n",
    "    .withColumnRenamed(\"sum(house_age)\", \"sum_house_age\") \\\n",
    "    .withColumnRenamed(\"sum(house_worth)\", \"sum_house_worth\") \\\n",
    "    .withColumnRenamed(\"sum(house_size)\", \"sum_house_size\") \\\n",
    "    .withColumnRenamed(\"count\", \"num_rows\")\n",
    "def compute_average_features_house_for_sale(row):\n",
    "    avg_house_worth = row.sum_house_worth/float(row.num_rows)\n",
    "    avg_house_size = row.sum_house_size/float(row.num_rows)\n",
    "    avg_house_age = row.sum_house_age/float(row.num_rows)\n",
    "    return Row(\n",
    "        sum_house_worth=row.sum_house_worth, \n",
    "        sum_house_age=row.sum_house_age,\n",
    "        sum_house_size=row.sum_house_size,\n",
    "        area_id = row.area_id,\n",
    "        avg_house_worth = avg_house_worth,\n",
    "        avg_house_size = avg_house_size,\n",
    "        avg_house_age = avg_house_age\n",
    "       )\n",
    "houses_for_sale_features_df = sum_count_houses_for_sale_df.rdd.map(\n",
    "    lambda row: compute_average_features_house_for_sale(row)\n",
    ").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_id: long (nullable = true)\n",
      " |-- avg_house_age: double (nullable = true)\n",
      " |-- avg_house_size: double (nullable = true)\n",
      " |-- avg_house_worth: double (nullable = true)\n",
      " |-- sum_house_age: double (nullable = true)\n",
      " |-- sum_house_size: double (nullable = true)\n",
      " |-- sum_house_worth: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "houses_for_sale_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features from `houses_sold_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").sum()\n",
    "count_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").count()\n",
    "sum_count_houses_sold_df = sum_houses_sold_df.join(count_houses_sold_df, \"area_id\")\n",
    "sum_count_houses_sold_df = sum_count_houses_sold_df \\\n",
    "    .withColumnRenamed(\"sum(number_of_bidders)\", \"sum_number_of_bidders\") \\\n",
    "    .withColumnRenamed(\"sum(sold_for_amount)\", \"sum_sold_for_amount\") \\\n",
    "    .withColumnRenamed(\"count\", \"num_rows\")\n",
    "def compute_average_features_houses_sold(row):\n",
    "    avg_num_bidders = row.sum_number_of_bidders/float(row.num_rows)\n",
    "    avg_sold_for = row.sum_sold_for_amount/float(row.num_rows)\n",
    "    return Row(\n",
    "        sum_number_of_bidders=row.sum_number_of_bidders, \n",
    "        sum_sold_for_amount=row.sum_sold_for_amount,\n",
    "        area_id = row.area_id,\n",
    "        avg_num_bidders = avg_num_bidders,\n",
    "        avg_sold_for = avg_sold_for\n",
    "       )\n",
    "houses_sold_features_df = sum_count_houses_sold_df.rdd.map(\n",
    "    lambda row: compute_average_features_houses_sold(row)\n",
    ").toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_id: long (nullable = true)\n",
      " |-- avg_num_bidders: double (nullable = true)\n",
      " |-- avg_sold_for: double (nullable = true)\n",
      " |-- sum_number_of_bidders: long (nullable = true)\n",
      " |-- sum_sold_for_amount: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "houses_sold_features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Features to the Feature Store\n",
    "\n",
    "The Featue store has an abstraction of a **feature group** which is a set of features that naturally belong together that typically are computed using the same feature engineering job and the same raw dataset. \n",
    "\n",
    "Lets create two feature groups:\n",
    "\n",
    "1. `houses_for_sale_featuregroup`\n",
    "\n",
    "2. `houses_sold_featuregroup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use fs_demo_featurestore\n",
      "Running sql: use fs_demo_featurestore\n",
      "Running sql: SELECT * FROM houses_for_sale_featuregroup_1"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    houses_for_sale_features_df,\n",
    "    \"houses_for_sale_featuregroup\",\n",
    "    description=\"aggregate features of houses for sale per area\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use fs_demo_featurestore\n",
      "Running sql: use fs_demo_featurestore\n",
      "Running sql: SELECT * FROM houses_sold_featuregroup_1"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    houses_sold_features_df,\n",
    "    \"houses_sold_featuregroup\",\n",
    "    description=\"aggregate features of sold houses per area\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training Dataset\n",
    "\n",
    "The feature store has an abstraction of a **training dataset**, which is a dataset with a set of features (potentially from many different feature groups) and labels (in case of supervised learning). \n",
    "\n",
    "Let's create a training dataset called *predict_house_sold_for_dataset* using the following features:\n",
    "\n",
    "- avg_house_age\n",
    "- avg_house_size\n",
    "- avg_house_worth\n",
    "- avg_num_bidders\n",
    "\n",
    "and the target variable is:\n",
    "\n",
    "- avg_sold_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use fs_demo_featurestore\n",
      "Running sql: SELECT avg_sold_for, avg_house_age, avg_house_size, avg_num_bidders, avg_house_worth FROM houses_sold_featuregroup_1 JOIN houses_for_sale_featuregroup_1 ON houses_sold_featuregroup_1.`area_id`=houses_for_sale_featuregroup_1.`area_id`"
     ]
    }
   ],
   "source": [
    "features_df = featurestore.get_features([\"avg_house_age\", \"avg_house_size\", \n",
    "                                         \"avg_house_worth\", \"avg_num_bidders\", \n",
    "                                         \"avg_sold_for\"])\n",
    "featurestore.create_training_dataset(\n",
    "    features_df, \"predict_house_sold_for_dataset\",\n",
    "    data_format=\"tfrecords\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Training Dataset to Train a Model\n",
    "\n",
    "When creating training datasets through the feature store API, the training dataset becomes *managed* by Hopsworks, meaning that it will get automatic versioning, documentation, API support, and analysis. \n",
    "\n",
    "Let's create a simple neural network and train it for the regression task of predicting the target variable `avg_sold_for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 5s - loss: 256998272.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 269ms/step - loss: 318853652.0000\n",
      "Epoch 2/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 1057380992.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 165ms/step - loss: 316778487.6000\n",
      "Epoch 3/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 365105088.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 168ms/step - loss: 315049218.4000\n",
      "Epoch 4/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 1023043904.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 141ms/step - loss: 313515144.8000\n",
      "Epoch 5/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 69518528.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 157ms/step - loss: 311750453.6000\n",
      "Epoch 6/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 148772384.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 171ms/step - loss: 309745111.2000\n",
      "Epoch 7/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 1042954752.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 146ms/step - loss: 307866987.2000\n",
      "Epoch 8/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 980451136.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 163ms/step - loss: 307098876.8000\n",
      "Epoch 9/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 990150272.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 173ms/step - loss: 304209784.8000\n",
      "Epoch 10/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 5s - loss: 1018515840.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 279ms/step - loss: 300723418.8000\n",
      "Epoch 11/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 28366332.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 133ms/step - loss: 299241663.6000\n",
      "Epoch 12/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 171075424.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 162ms/step - loss: 292467079.2000\n",
      "Epoch 13/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 234267184.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 174ms/step - loss: 293558402.0000\n",
      "Epoch 14/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 51009784.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 139ms/step - loss: 297141411.2000\n",
      "Epoch 15/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 53580968.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 151ms/step - loss: 292988932.8000\n",
      "Epoch 16/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 994651008.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 153ms/step - loss: 288210571.0000\n",
      "Epoch 17/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 984006848.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 148ms/step - loss: 288963857.2000\n",
      "Epoch 18/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 9049484.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 197ms/step - loss: 293260554.4000\n",
      "Epoch 19/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 974764224.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 129ms/step - loss: 294834430.4000\n",
      "Epoch 20/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 5s - loss: 1008284672.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 261ms/step - loss: 289262722.2000\n",
      "Epoch 21/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 62939784.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 129ms/step - loss: 290962498.2000\n",
      "Epoch 22/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 18215226.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 176ms/step - loss: 285368861.2000\n",
      "Epoch 23/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 63381960.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 162ms/step - loss: 280700470.4000\n",
      "Epoch 24/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 240755840.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 128ms/step - loss: 288380582.0000\n",
      "Epoch 25/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 317189184.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 175ms/step - loss: 290109016.6000\n",
      "Epoch 26/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 12058823.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 160ms/step - loss: 279065190.4000\n",
      "Epoch 27/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 3s - loss: 41473128.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 153ms/step - loss: 291474369.6000\n",
      "Epoch 28/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 63973880.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 149ms/step - loss: 292751405.6000\n",
      "Epoch 29/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 2s - loss: 63128664.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 126ms/step - loss: 292010703.0000\n",
      "Epoch 30/30\n",
      "\r",
      "1/5 [=====>........................] - ETA: 4s - loss: 993138304.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "5/5 [==============================] - 1s 210ms/step - loss: 292993635.2000"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.001\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "dataset_dir = featurestore.get_training_dataset_path(\"predict_house_sold_for_dataset\")\n",
    "input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "dataset = tf.data.TFRecordDataset(input_files)\n",
    "tf_record_schema = featurestore.get_training_dataset_tf_record_schema(\"predict_house_sold_for_dataset\")\n",
    "feature_names = [\"avg_house_age\", \"avg_house_size\", \"avg_house_worth\", \"avg_num_bidders\"]\n",
    "label_name = \"avg_sold_for\"\n",
    "def decode(example_proto):\n",
    "    example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "    x = []\n",
    "    for feature_name in feature_names:\n",
    "        x.append(example[feature_name])\n",
    "    y = tf.cast(example[label_name], tf.float32)\n",
    "    return x,y\n",
    "dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "model = tf.keras.Sequential([\n",
    "layers.Dense(64, activation='relu'),\n",
    "layers.Dense(64, activation='relu'),\n",
    "layers.Dense(1)])\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(LEARNING_RATE), loss='mse')\n",
    "history = model.fit(dataset, epochs=NUM_EPOCHS, steps_per_epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
