{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature Store Tour - Python API\n",
    " \n",
    "This notebook contains a tour/reference for the feature store Python API on hopsworks. The notebook is ment to be run from feature store demo projects on Hopsworks. We will go over best practices for using the API as well as common pitfalls.\n",
    " \n",
    "The notebook is designed to be used in combination with the Feature Store Tour on Hopsworks, it assumes that you have run the following feature engineering job: [job](https://github.com/logicalclocks/hops-examples/tree/master/featurestore) (**the job is added automatically when you start the feature store tour in Hopsworks. You can run the job by going to the 'Jobs' tab to the left in the Hopsworks project home page**). \n",
    "\n",
    "Which will produce the following model of feature groups in your project's feature store:\n",
    "\n",
    "![Feature Store Model](./images/model.png \"Feature Store Model\")\n",
    "\n",
    "In this notebook we will run queries over this feature store model. We will also create new feature groups and training datasets.\n",
    "\n",
    "We will go from (1) features to (2) training datasets to (3) A trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store 101\n",
    "\n",
    "The simplest way to think about the feature store is as a central place to store curated /features/ within an organization. A feature is a measurable property of some phenomenon. It could be for example an image-pixel, a word from a piece of text, the age of a person, a coordinate emitted from a sensor, or an aggregate value like the average number of purchases within the last hour.\n",
    "\n",
    "A feature store is a data management layer for machine learning that can optimize the machine learning workflow and provide an interface between data engineering and data science.\n",
    "\n",
    "![Feature Store Overview](./images/overview.png \"Feature Store Overview\")\n",
    "\n",
    "A feature store is not a pure storage service, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models.\n",
    "\n",
    "There are two interfaces to the feature store:\n",
    "\n",
    "- Writing to the feature store, at the end of the feature engineering pipeline the features are written to the feature store, e.g:\n",
    "```python\n",
    "raw_data = spark.read.format(\"csv\").load(filename)\n",
    "polynomial_features = raw_data.map(lambda x: x^2)\n",
    "from hops import featurestore\n",
    "featurestore.insert_into_featuregroup(polynomial_features, \"polynomial_features\")\n",
    "```\n",
    "- Reading from the feature store, to train a model on a set of features, the features can be read from the feature store, e.g:\n",
    "```python\n",
    "from hops import featurestore\n",
    "features_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"])\n",
    "```\n",
    "\n",
    "As a data engineer/data scientist, you can think of the feature store as a middle-layer. Once you have computed a set of features, instead of writing them locally to a csv file, insert them in the feature store so that the features can get documented/versioned, backfilled, **and so that your colleagues can re-use your features!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hops library is automatically installed in all Hopsworks-projects. This library has a module called `featurestore` that provides an API for the feature store. You can find API documentation [here](http://hops-py.logicalclocks.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1548937825622_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1548937825622_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1548937825622_0009_01_000001/demo_featurestore_admin000__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Name of The Project's Feature Store\n",
    "\n",
    "Each project with the feature store service enabled automatically gets its own feature store created. This feature store is only accessible within the project unless you decide to share it with other projects. The name of the feature store is `<project_name>_featurestore`, and you can get the name with the API method `project_featurestore()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.project_featurestore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of All Feature Stores Accessible in the Current Project \n",
    "\n",
    "Feature Stores can be shared across projects in a multi-tenant manner, just like any Hopsworks-dataset can. You can read more about sharing datasets at [hops.io](hops.io), but in essence to share a dataset you just have to right click on it in your project. The feature groups in the feature store are located in a dataset called `<project_name>_featurestore.db` in your project.\n",
    "\n",
    "![Share Feature Store](./images/share_featurestore.png \"Share Feature Store\")\n",
    "\n",
    "The Training Datasets in your feature store are located in a datased called `<project_name>_Training_Datasets` inside your project. Typically if you want to share a feature store with another project you will share both the `<project_name>_featurestore.db` dataset and the `<project_name>_Training_Datasets` dataset.\n",
    "\n",
    "![Share Feature Store](./images/share_featurestore2.png \"Share Feature Store\")\n",
    "\n",
    "To list all feature stores accessible in the current project, you can use the API method `get_project_featurestores()`. You can also view the list of accessible feature stores in the feature registry UI:\n",
    "\n",
    "![Share Feature Store](./images/select_fs.png \"Share Feature Store\")\n",
    "\n",
    "By using multiple feature stores and feature store sharing across projects you can enforce access rights to features.\n",
    "\n",
    "![Multi-Tenant Feature Stores](./images/multitenant.png \"Multi-Tenant Feature Stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_project_featurestores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying The Feature Store\n",
    "\n",
    "The feature store can be queried programmatically and with raw SQL. When you query the feature store programmatically, the library will infer how to fetch the different features using a **query planner**. \n",
    "\n",
    "![Feature Store Query Planner](./images/query_optimizer.png \"Feature Store Query Planner\")\n",
    "\n",
    "When interacting with the feature store it is sufficient to be familiar with three concepts:\n",
    "\n",
    "- The **feature**, this refer to an individual versioned and documented feature in the feature store, e.g the age of a person.\n",
    "- The **feature group**, this refer to a documented and versioned group of features stored as a Hive table that is linked to a specific Spark/Numpy/Pandas job that takes in raw data and outputs the computed features.\n",
    "- The **training dataset**, this refer to a versioned and managed dataset of features, stored in HopsFS as tfrecords, .csv, .tsv, or parquet.\n",
    "\n",
    "A feature group contains a group of features and a training dataset contains a set of features, potentially from many different feature groups.\n",
    "\n",
    "![Feature Store Concepts](./images/concepts.png \"Feature Store Contents\")\n",
    "\n",
    "When you query the feature store you will always get back the results in a spark dataframe. This is for scalability reasons. If the dataset is small and you want to work with it in memory you can convert it into a pandas dataframe or a numpy matrix using one line of code as we will demonstrate later on in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Individual Feature\n",
    "\n",
    "When retrieving a single feature from the featurestore, the hops-util-py library will infer in which feature group the feature belongs to by querying the metastore, but you can also explicitly specify which featuregroup and version to query. \n",
    "\n",
    "If there are multiple features of the same name in the featurestore, it is required to specify enough information to uniquely identify the feature (e.g specify feature group and version). If no featurestore is provided it will default to the project's featurestore.\n",
    "\n",
    "To read an individual feature, use the method `get_feature(feature_name)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the feature store, feature group and version, the library will infer it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_feature(\"team_budget\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify the feature store, feature group, the version, and the return format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_feature(\n",
    "    \"team_budget\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup=\"teams_features\", \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Entire Feature Group\n",
    "\n",
    "You can get an entire featuregroup from the API. If no feature store is provided the API will default to the project's feature store, if no version is provided it will default to version 1 of the feature group. The default return format is as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"teams_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameters can be overriden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\n",
    "    \"teams_features\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version = 1,\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch A Set of Features\n",
    "\n",
    "When retrieving a list of features from the featurestore, the hops-util-py library will infer which featuregroup the features belongs to by querying the metastore. If the features reside in different featuregroups, the library will also try to infer how to join the features together based on common columns. If the JOIN query cannot be inferred due to existence of multiple features with the same name or non-obvious JOIN query, the user need to supply enough information to the API call to be able to query the featurestore. If the user already knows the JOIN query it can also run featurestore.sql(joinQuery) directly (an example of this is shown further down in this notebook). If no featurestore is provided the API will default to the project's featurestore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of querying the feature store for a list of features without specifying the feature groups and feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly specify the feature groups where the features reside. Either the feature groups and versions can be specified by prepending feature names with `<feature group name>_<feature group version.`, or by providing a dict with entries of `<feature group name> -> <feature group version>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"teams_features_1.team_budget\", \n",
    "     \"attendances_features_1.average_attendance\", \n",
    "     \"players_features_1.average_player_age\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"],\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1,\n",
    "        \"players_features\": 1\n",
    "    }\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lot of name collisions and it is not obvious how to infer the JOIN query to get the features from the feature store. You can explicitly specify the argument `join_key` to the API (or you can provide the entire SQL query using the API method `.sql` as we will demonstrate later on in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\"],\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroups_version_dict={\n",
    "        \"teams_features\": 1, \n",
    "        \"attendances_features\": 1,\n",
    "        \"players_features\": 1\n",
    "    },\n",
    "    join_key = \"team_id\",\n",
    "    dataframe_type = \"spark\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Eamples of Fetching Sets of Features and Common Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting 12 features from 4 different feature groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\",\n",
    "    \"team_position\", \"sum_attendance\", \n",
    "     \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n",
    "     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \n",
    "     \"average_position\"\n",
    "    ]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example of a common error that can occur when you query the feature store.\n",
    "\n",
    "If you try to query the feature store for a feature that exists in multiple feature groups, it is impossible for the query planner to infer from which feature group to fetch the feature so it will throw an exception. When this error happen you should specify which feature group to fetch from so that the query planner knows how to get the feature.\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"team_id\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"team_id\"],\n",
    "    featuregroups_version_dict = {\n",
    "        \"teams_features\" : 1\n",
    "    }\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common error is that you try to fetch features from feature groups that are not compatible, they do not got any natural join column. Typically in this case you need to either provide the join key your self or use SQL directly with `featurestore.sql()`.\n",
    "\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features(\n",
    "    [\"team_budget\", \"score\"]\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fix the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.sql(\n",
    "    \"SELECT team_budget, score \" \\\n",
    "    \"FROM teams_features_1 JOIN games_features_1 ON \" \\\n",
    "    \"games_features_1.home_team_id = teams_features_1.team_id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Text SQL Query from the Feature Store\n",
    "\n",
    "For complex queries that cannot be inferred by the helper functions, enter the sql directly to the method `featurestore.sql()` it will default to the project specific feature store but you can also specify it explicitly. If you are proficient in SQL, this is the most efficient and preferred way to query the feature store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without specifying the feature store the query will by default be run against the project's feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify the featurestore to query and the return format explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featurestore.sql(\"SELECT * FROM teams_features_1 WHERE team_position < 5\",\n",
    "                featurestore=featurestore.project_featurestore(), \n",
    "                 dataframe_type = \"spark\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to the Feature Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Feature Groups\n",
    "\n",
    "In most cases it is recommended that feature groups are created in the UI on Hopsworks and that care is taken in documenting the feature group. \n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg.png \"Create Feature Group from the UI\")\n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg_2.png \"Create Feature Group from the UI\")\n",
    "\n",
    "However, sometimes it is practical to create a feature group directly from a spark dataframe and fill in the metadata about the featuregroup later in the UI. This can be done through the `create_featuregroup()` API function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new featuregroup called **teams_features_spanish** that contains the same contents as the feature group teams_features except the the columns are renamed to spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_features_1_df = featurestore.get_featuregroup(\"teams_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_features_2_df = teams_features_1_df.withColumnRenamed(\n",
    "    \"team_id\", \"equipo_id\").withColumnRenamed(\n",
    "    \"team_budget\", \"equipo_presupuesto\").withColumnRenamed(\n",
    "    \"team_position\", \"equipo_posicion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_features_2_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a new featuregroup using the transformed dataframe (we'll explain the statistics part later on in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the new featuregroup will be created in the project's featurestore and the statistics for the new featuregroup will be computed based on the provided spark dataframe. You can configure this behaviour by modifying the default arguments and filling in extra metadata.\n",
    "\n",
    "The dependencies argument takes a list of HDFS file names that the feature group depends on, i.e when the datasets that a featuregroup depends on have been modified, the feature group should be recomputed. The dependencies can also be updated and viewed in the feature registry UI. \n",
    "\n",
    "![Feature group dependencies](./images/deps.png \"Feature group dependencies\")\n",
    "\n",
    "![Feature group dependencies](./images/deps2.png \"Feature group dependencies\")\n",
    "\n",
    "The jobId argument takes an integer that identifies the job id to compute the features. Once you have created a job that creates/inserts features in the feature store you can use the Featurestore UI to link that job to the featuregroup:\n",
    "\n",
    "![Feature group jobs](./images/jobs1.png \"Feature group jobs\")\n",
    "\n",
    "![Feature group jobs](./images/jobs2.png \"Feature group jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    featuregroup_version=1,\n",
    "    job_name=None,\n",
    "    dependencies=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Feature Group\n",
    "\n",
    "To create a new version, simply use the `create_featuregroup` method and specify the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    teams_features_2_df,\n",
    "    \"teams_features_spanish\",\n",
    "    description=\"a spanish version of teams_features\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    featuregroup_version=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Feature Group Version](./images/create_fg_version.png \"Create Feature Group Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Feature Group (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_version = featurestore.get_latest_featuregroup_version(\"teams_features_spanish\")\n",
    "latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into Existing Feature Groups\n",
    "\n",
    "A best practice when working with features in HopsML is to first figure out a model of feature groups and create them  using the Feature Registry UI. This will prepare the feature group schema and create the Hive tables. Once the empty feature groups are created, then you can insert into these tables directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get some sample data to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, IntegerType, FloatType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"equipo_id\", IntegerType(), True),\n",
    "                     StructField(\"equipo_presupuesto\", FloatType(), True),\n",
    "                     StructField(\"equipo_posicion\", IntegerType(), True)\n",
    "                        ])\n",
    "sample_df = sqlContext.createDataFrame([(999, 41251.52, 1), (998, 1319.4, 8), (997, 21219.1, 2)], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the contents of the featuregroup `teams_features_spanish` that we are going to insert the sample data into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can insert the sample data and verify the new contents of the featuregroup. By default the insert mode is \"append\", the featurestore is the project's featurestore, the version is 1 and statistics will be updated (we cover statistics later on in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now inspect the contents of the feature group to verify that the insertion was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df_updated = featurestore.get_featuregroup(\n",
    "    \"teams_features_spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df_updated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_team_features_df_updated.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify featurestore, featuregroup version, the insert mode and what statistics to compute (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\", \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    featuregroup_version=1, \n",
    "    mode=\"append\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False, \n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False, \n",
    "    stat_columns=None, \n",
    "    num_bins=20, \n",
    "    corr_method='pearson',\n",
    "    num_clusters=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two supported insert modes are \"append\" and \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_featuregroup(\n",
    "    sample_df, \n",
    "    \"teams_features_spanish\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"teams_features_spanish\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the Feature Store with Pandas and Numpy\n",
    "\n",
    "The Hops Feature Store works natively with Spark, but you can easily connect it to your numpy/pandas/pure python pipelines as well (just recall that if you are working with big data, using libraries like numpy/pandas that are not distributed will not scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Features from the Featue Store into a Pandas or Numpy Table\n",
    "\n",
    "To read from the feature store into pandas and numpy we just have to specify the optional argument `dataframe_type` that defaults to \"spark\". This argument decides the format of the returning dataframe. If you want the features returned to be in either pandas or numpy format you can specfy `dataframe_type='pandas'` or `dataframe_type='numpy'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Numpy 2D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Features into a Python 2D List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df = featurestore.get_features([\"team_budget\", \"average_attendance\", \"average_player_age\"], \n",
    "                                      dataframe_type=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(python_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(python_df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Pandas or Numpy Tables into the Feature Store\n",
    "\n",
    "The feature store API natively supports dataframes in spark, pandas, python or numpy format for writing to the feature store. \n",
    "\n",
    "**Note** that since the feature store contains feature groups with documented schemas to allow easy joining of thousands of features in the future, inserting raw numpy arrays is not recommended. Numpy arrays and python lists do not have any associated schema so the library will infer the schema to be \"col_1, col_2, col_3... etc\". It is recommended that you think carefully in how to model your feature schema with naming of columns **before** you insert into the feature store so that it becomed easy to later join features across several feature groups by using a common join-column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Pandas DataFrame to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename the columns to differentiate this feature group from existing ones in the feature store\n",
    "pandas_df.columns = [\"average_player_age_test\", \"team_budget_test\", \"average_attendance_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    pandas_df,\n",
    "    \"pandas_test_example\",\n",
    "    description=\"test featuregroup created from pandas dataframe\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    pandas_df, \n",
    "    \"pandas_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Numpy 2D Array to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    numpy_df,\n",
    "    \"numpy_test_example\",\n",
    "    description=\"test featuregroup created from numpy matrix\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Numpy arrays do not have an associated schema the resulting feature group will have default column names. A best practice if you are working with numpy is to convert it to a pandas or spark dataframe and specify an explicit schema before you save it to the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"numpy_test_example\", dataframe_type=\"spark\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    numpy_df, \n",
    "    \"numpy_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a Python 2D List to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(\n",
    "    python_df,\n",
    "    \"python_test_example\",\n",
    "    description=\"test featuregroup created from python 2D list\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All insert/create methods in the API supports spark, pandas, and numpy dataframes\n",
    "featurestore.insert_into_featuregroup(\n",
    "    python_df, \n",
    "    \"python_test_example\",\n",
    "    descriptive_statistics=False, \n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As python lists do not have an associated schema the resulting feature group will have default column names. A best practice if you are working with python lists is to convert it to a pandas or spark dataframe and specify an explicit schema before you save it to the feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroup(\"python_test_example\", dataframe_type=\"spark\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Errors\n",
    "\n",
    "To insert numpy/python arrays you must make sure that the dimensions match, i.e it needs to be in two dimensions\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_fail_test = [1,2,3]\n",
    "featurestore.create_featuregroup(\n",
    "    python_fail_test,\n",
    "    \"python_fail_test\",\n",
    "    description=\"\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group Statistics\n",
    "\n",
    "Statistics about a featuregroup can be useful in the stage of feature engineering and when deciding which features to use for training. If statistics have been computed for a feature group, it can be viewed in the Hopsworks Feature Registry UI. \n",
    "\n",
    "This is particularly useful within large organizations where data scientists from different teams can re-use and explore new features by browsing features in the feature store and analyzing the statistics.\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_1.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_2.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_3.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_4.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_5.png \"Feature Registry Statistics Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have notived earlier in this notebook, both the `insert_into_featuregroup` and `create_featuregroup` methods have arguments for updating the statistics as new data is added. \n",
    "\n",
    "You can also use the `update_featuregroup_stats()` method to update the statistics of a feature group without inserting any new data. By default it will compute all statistics (descriptive, feature correlation, histograms, and cluster analysis), use the project's featurestore, use version 1 of the featuregroup and use all columns for computing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.update_featuregroup_stats(\"teams_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also be explicitly specify featuregroup details and what statistics to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.update_featuregroup_stats(\n",
    "    \"teams_features\", \n",
    "    featuregroup_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compute statistics for certain set of columns and exclude surrogate key-columns for example, you can use the optional argument `stat_columns` to specify which columns to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.update_featuregroup_stats(\n",
    "    \"teams_features\", \n",
    "    featuregroup_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=['team_budget', 'team_position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Datasets\n",
    "\n",
    "To group data in the feature store we use three concepts:\n",
    "\n",
    "- Feature\n",
    "- Feature group\n",
    "- Training Dataset\n",
    "\n",
    "Typically during the feature engineering phase of a machine learning project, you compute a set of features for each type of data that you have, these features are naturally grouped into a documented and versioned **feature group**. \n",
    "\n",
    "In practice, it is common that organizations have many different type of datasets that they can extract features from, for example if you are building a recommendation system you might have demographic data about each user as well as user-activity data. \n",
    "\n",
    "When you train a machine learning model, you want to use all features that have predictive power and that the model can learn from. At this point, we can create a training dataset of features from several different feature groups and use that for training. That is the purpose of the training dataset abstraction. \n",
    "\n",
    "Of course you can always just save a group of features anywhere inside your project, e.g as a csv, or .tfrecords file. However, by using the feature store you can create **managed** training datasets. Managed training datasets will show up in the feature registry UI and will automatically be versioned, documented and reproducible. \n",
    "\n",
    "![Feature Engineering Pipeline](./images/pipeline.png \"Feature Engineering Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata for a training dataset can be created from the Hopsworks UI or directly from the API with the function `create_training_dataset`. The training datasets in a project are stored in a top-level dataset called `<ProjectName>_Training_Datasets`, (i.e `hdfs:///Projects/<ProjectName>/<ProjectName>_Training_Datasets`.\n",
    "\n",
    "Once a training dataset have been created you can find it in the featurestore UI in hopsworks under the tab `Training datasets`, from there you can also edit the metadata if necessary. \n",
    "\n",
    "![Find Training Datasets](./images/find_training_datasets.png \"Find Training Datasets\")\n",
    "After a training dataset have been created with the necessary metadata you can save the actual data in the training dataset by using the API function `insert_into_training_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataset called `team_position_prediction` by using a set of relevant features from the featurestore. We will combine features from four different feature groups to form this training dataset: `teams_features`, `attendances_features`, `players_features`, `season_scores_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = featurestore.get_features(\n",
    "    [\"team_budget\", \"average_attendance\", \"average_player_age\",\n",
    "    \"team_position\", \"sum_attendance\", \n",
    "     \"average_player_rating\", \"average_player_worth\", \"sum_player_age\",\n",
    "     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \n",
    "     \"average_position\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Training Dataset (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_version = featurestore.get_latest_training_dataset_version(\"team_position_prediction\")\n",
    "latest_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in TFRecords Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a training dataset from the dataframe with some extended metadata such as schema (automatically inferred). By default when you create a training dataset it will be in \"tfrecords\" format and statistics will be computed for all features. After the dataset have been created you can view and/or update the metadata about the training dataset from the Hopsworks featurestore UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version = latest_version + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now go to the Feature Registy UI in Hopsworks we can see that the training dataset have been created for us and things like versioning, documentation, and recomputation is managed for us. We can also easily edit the metadata from the UI if necesssary.\n",
    "\n",
    "![Training Dataset UI](./images/training_dataset.png \"Training Dataset UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can override the default configuration if necessary:\n",
    "\n",
    "Supported data formats are:\n",
    "\n",
    "- csv (written with spark distributed)\n",
    "- tsv (written with spark distributed)\n",
    "- parquet (written with spark distributed)\n",
    "- tfrecords (written with spark distributed)\n",
    "- hdf5 (written with single-machine, must fit into memory)\n",
    "- npy (written with single-machine, must fit into memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_csv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"csv\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  TSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_tsv\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"tsv\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_tsv\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_parquet\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"parquet\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_parquet\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_hdf5\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"hdf5\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_hdf5\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Training Dataset in  .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction_npy\",\n",
    "    description=\"a dataset with features for football teams, used for training a model to predict league-position\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    data_format=\"npy\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_npy\") + 1,\n",
    "    job_name=None,\n",
    "    dependencies=[],\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Errors\n",
    "\n",
    "Lets look at a common error that can occur when you create training datasets. If you try to create a training dataset that already exists, you will get an error.\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version = featurestore.get_latest_training_dataset_version(\"team_position_prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this error, either choose a different name for your training dataset or delete the existing training dataset before creating the new one. To delete a training dataset, use the feature store UI or delete the training dataset folder directly.\n",
    "\n",
    "**Option 1, use the feature store UI:**\n",
    "\n",
    "![Delete Training Dataset](./images/delete_training_dataset.png \"Delete Training Dataset\")\n",
    "\n",
    "**Option 2, delete the dataset folder directly:**\n",
    "\n",
    "![Delete Training Dataset](./images/delete_td_folder_1.png \"Delete Training Dataset\")\n",
    "\n",
    "![Delete Training Dataset](./images/delete_td_folder_2.png \"Delete Training Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Training Dataset\n",
    "\n",
    "To create a new version, simply use the `create_training_dataset` method and specify the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_training_dataset(\n",
    "    features_df, \"team_position_prediction\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction\") + 1,\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Training Dataset Version](./images/create_td_version.png \"Create Training Dataset Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into an Existing Training Dataset\n",
    "\n",
    "Once a dataset have been created, its metadata is browsable in the featurestore registry in the Hopsworks UI. If you don't want to create a new training dataset but just overwrite or insert new data into an existing training dataset, you can use the API function `insert_into_training_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_training_dataset(\n",
    "    features_df, \n",
    "    \"team_position_prediction_csv\",\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `insert_into_training_dataset` will use the project's featurestore, overwrite semantics, version 1 of the training dataset, and update the training dataset statistics, this configuration can be overridden.\n",
    "\n",
    "**Note**: \"append\" write mode is not supported for training datasets stored in tfrecords format, only \"overwrite\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.insert_into_training_dataset(\n",
    "    features_df,\n",
    "    \"team_position_prediction_csv\",\n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    descriptive_statistics=False,\n",
    "    feature_correlation=False,\n",
    "    feature_histograms=False,\n",
    "    cluster_analysis=False,\n",
    "    stat_columns=None,\n",
    "    write_mode=\"overwrite\",\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Dataset Path\n",
    "\n",
    "After a **managed dataset** have been created, it is easy to share it and re-use it for training various models. For example if the dataset have been materialized in tf-records format you can call the method `get_training_dataset_path(training_dataset)` to get the HDFS path and read it directly in your tensorflow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_path(\"team_position_prediction_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_path(\"team_position_prediction_hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the library will look for the training dataset in the project's featurestore and use version 1, but this can be overriden if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_path(\n",
    "    \"team_position_prediction_csv\", \n",
    "    featurestore=featurestore.project_featurestore(),\n",
    "    training_dataset_version=featurestore.get_latest_training_dataset_version(\"team_position_prediction_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Training Dataset Stats\n",
    "\n",
    "The API for updating training dataset stats is the same as for updating feature group stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.update_training_dataset_stats(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.update_training_dataset_stats(\n",
    "    \"team_position_prediction\", \n",
    "    training_dataset_version=1, \n",
    "    featurestore=featurestore.project_featurestore(), \n",
    "    descriptive_statistics=True,\n",
    "    feature_correlation=True, \n",
    "    feature_histograms=True,\n",
    "    cluster_analysis=True,\n",
    "    stat_columns=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Training Dataset into a Spark Dataframe\n",
    "\n",
    "Typically training datasets are served into deep learning frameworks such as pytorch or tensorflow. However, training datasets can also be read into spark dataframes using the api method `get_training_dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset(\"team_position_prediction_csv\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the library will read the training dataset from the project's feature store, use version 1 and return the data in a spark dataframe. This can be overriden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset(\"team_position_prediction_csv\",\n",
    "                                  featurestore=featurestore.project_featurestore(),\n",
    "                                  training_dataset_version=1,\n",
    "                                  dataframe_type=\"pandas\"\n",
    "                                 ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_arr = featurestore.get_training_dataset(\"team_position_prediction_hdf5\",\n",
    "                                  dataframe_type=\"numpy\")\n",
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Managed Training Dataset Without Using the API \n",
    "\n",
    "To create a **managed** training dataset without using the API, e.g to create a managed training dataset from a .tfrecords file downloaded from Kaggle or from a .csv file from Kaggle, first go to the Feature Registry UI and create a new training dataset and fill in the metadata:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_1.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_2.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Once the dataset have been created from the UI, you can find that inside the \"Projectname_Training Datasets\" folder in your project a new folder for the dataset have showed up that is called `training_datasetname_version`:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_3.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Simply upload your dataset inside that folder, e.g you can upload for example a single .csv file or a folder with part-r-X.csv files. **It is important that you name the folder/file the name of your training dataset, e.g sample_dataset or sample_dataset.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Featurestore Metadata\n",
    "To explore the contents of the featurestore we recommend using the featurestore page in the Hopsworks UI but you can also get the metadata programmatically from the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Stores Accessible In the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_project_featurestores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Groups in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_featuregroups()` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featuregroups(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Features in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default get_features_list() will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_features_list(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Training Datasets in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_training_datasets()` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_datasets(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Metadata (Features, Feature groups, Training Datasets) for a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_featurestore_metadata` will use the project's feature store, but this can also be specified with the optional argument featurestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_featurestore_metadata(featurestore=featurestore.project_featurestore())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Raw Data to Features to Training Dataset to Model\n",
    "\n",
    "Once a training dataset have been materialized, we can use it to train a model. The featurestore API makes the integration with libraries such as Tensorflow simple. In this section we will train an example model using the training dataset `team_position_prediction` that we created earlier. We will use the column **\"team_position\"** as the target to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TFRecords Schema from a Spark Dataframe\n",
    "\n",
    "This utility method can be used when parsing training datasets in the tfrecords format. Note that this method will try to infer the tensorflow example schema from the schema of the spark dataframe. If you want full control of the tf-record schema you should define it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_dataframe_tf_record_schema(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TFRecords Schema from a Training Dataset\n",
    "\n",
    "When a training dataset is saved in the tfrecords format, the tfrecord schema is stored together with the training dataset in a file called `tf_record_schema.txt`. This schema can be retrieved when the tfrecords need to be parsed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12"
     ]
    }
   ],
   "source": [
    "featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `get_training_dataset_tf_record_schema` will use the project's feature store and version 1 of the feature grup, this can be overriden if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.get_training_dataset_tf_record_schema(\n",
    "    \"team_position_prediction\", \n",
    "    training_dataset_version=1,\n",
    "    featurestore = featurestore.project_featurestore()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In this example we will use Tensorflow and Keras. However, the feature store is in theory agnostic to which framework or method you use for training the model, it works with PyTorch, spark-mllib etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import json\n",
    "from hops import hdfs\n",
    "from hops import experiment\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from hops import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "INPUT_SHAPE = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse TFRecords into A TF-Dataset\n",
    "\n",
    "The dataset is stored in `.tfrecords` format, which essentially means it is stored in protobuf format. Moreover, to be able to read and write datasets in the petabyte-scale, the feature store uses distrbuted write/read to HopsFS with Spark, so the dataset is spread out in a large number of files prefixed with `part-r` (if you are unfamiliar with Spark partitions you can read up on Spark [here](https://spark.apache.org/docs/2.1.0/programming-guide.html)). \n",
    "\n",
    "Despite that our dataset is stored in a binary format and stored disributed in HopsFSs the amount of code to read the data into a tensorflow dataset is very little, thanks to \n",
    "\n",
    "- `featurestore.et_training_dataset_path`: gets the path in HopsFS where the tfrecords files are stored \n",
    "- `featurestore.get_training_dataset_tf_record_schema`: gets the tf-record schema to parse the binary data\n",
    "- `tf.gfile.Glob`: Gets a list of file names from a file-pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset():\n",
    "    dataset_dir = featurestore.get_training_dataset_path(\"team_position_prediction\")\n",
    "    input_files = tf.gfile.Glob(dataset_dir + \"/part-r-*\")\n",
    "    dataset = tf.data.TFRecordDataset(input_files)\n",
    "    tf_record_schema = featurestore.get_training_dataset_tf_record_schema(\"team_position_prediction\")\n",
    "    feature_names = [\"team_budget\", \"average_attendance\", \"average_player_age\", \"sum_attendance\", \n",
    "         \"average_player_rating\", \"average_player_worth\", \"sum_player_age\", \"sum_player_rating\", \"sum_player_worth\", \n",
    "         \"sum_position\", \"average_position\"\n",
    "        ]\n",
    "    label_name = \"team_position\"\n",
    "\n",
    "    def decode(example_proto):\n",
    "        example = tf.parse_single_example(example_proto, tf_record_schema)\n",
    "        x = []\n",
    "        for feature_name in feature_names:\n",
    "            x.append(example[feature_name])\n",
    "        y = [tf.cast(example[label_name], tf.float32)]\n",
    "        return x,y\n",
    "\n",
    "    dataset = dataset.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(NUM_EPOCHS)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Model using Keras and Tensorflow \n",
    "\n",
    "We will use a three-layer neural network for regression on our dataset. In this tutorial we work with so little data that using a larger model does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_neurons = 64, learning_rate = 0.001):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(num_neurons, activation='relu', \n",
    "                     input_shape = (INPUT_SHAPE,),\n",
    "                    batch_size=BATCH_SIZE),\n",
    "        layers.Dense(num_neurons, activation='relu'),\n",
    "        layers.Dense(1)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Function\n",
    "\n",
    "We define the train code in a separate function so that it can be distributed in the cluster across different executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(num_neurons_per_layer, learning_rate):\n",
    "    dataset = create_tf_dataset()\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate), loss='mse', metrics=['accuracy'])\n",
    "    tb_callback = TensorBoard(log_dir=tensorboard.logdir(), histogram_freq=0,\n",
    "                             write_graph=True, write_images=True)\n",
    "    callbacks = [tb_callback]\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint(tensorboard.logdir() + '/checkpoint-{epoch}.h5'))\n",
    "    history = model.fit(dataset, epochs=NUM_EPOCHS, steps_per_epoch = 5, callbacks=callbacks)\n",
    "    return history.history[\"acc\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search \n",
    "\n",
    "We will create a reproducible experiment to search for the best hyperparameters for our model using the hops `Ã¨xperiment` module and evolutionary search. \n",
    "\n",
    "When the experiment is running you can view the progress in the SparkUI, from there you can also find the tensorboards for the executors:\n",
    "\n",
    "![Open Spark UI to monitor Experiment 1](./images/open_sparkui_0.png \"Open Spark Ui to Monitor Experiment 1\")\n",
    "\n",
    "![Open Spark UI to monitor Experiment 2](./images/open_sparkui_2.png \"Open Spark Ui to Monitor Experiment 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict = {\"num_neurons_per_layer\" : [64,128], \"learning_rate\": [0.001, 0.01]}\n",
    "log_dir, best_params = experiment.differential_evolution(\n",
    "    train_fn, \n",
    "    search_dict, \n",
    "    name='team_position_prediction_hyperparam_search', \n",
    "    description='Evolutionary search through the search space of hyperparameters with parallel executors to find the best parameters',\n",
    "    local_logdir=True, \n",
    "    population=6,\n",
    "    generations = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing old Experiments\n",
    "\n",
    "To view the result of an old experiment, go to the \"Experiments\" tab.\n",
    "\n",
    "![Open Spark UI to monitor Experiment 3](./images/open_sparkui_3.png \"Open Spark Ui to Monitor Experiment 3\")\n",
    "\n",
    "![Open Spark UI to monitor Experiment 4](./images/open_sparkui_4.png \"Open Spark Ui to Monitor Experiment 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the Best Hyperparameters\n",
    "\n",
    "Now we can train for longer amount of epochs when we have found the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'best_params' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'best_params' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 200\n",
    "result = experiment.launch(\n",
    "    train_fn(num_neurons_per_layer=best_params[\"num_neurons_per_layer\"], learning_rate=best_params[\"learning_rate\"]), \n",
    "    name='team_position_prediction_hyperparam_search',\n",
    "    description=\"experiment to train model for team position prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = hdfs.project_path() + \"Logs/featurestore_tour_model_results_loss.txt\"\n",
    "hdfs.dump(json.dumps(history.history), results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results\n",
    "\n",
    "The code in this notebook is executed inside the Hops cluster, we can load the results into the local machine for plotting. To read more about the setup with jupyter notebooks on Hopsworks, look [here](https://hops.readthedocs.io/en/latest/user_guide/hopsworks/jupyter.html#plotting-with-pyspark-kernel).\n",
    "\n",
    "We are going to use matplotlib for plotting, if you have not already installed it in your project, you can do so from the python-tab in the project UI.\n",
    "\n",
    "![Install plt](./images/install_plt.png \"Install plt\")\n",
    "\n",
    "If you just installed matplotlib, you have to restart the Jupyter kernel for the changes to take effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import json\n",
    "from hops import hdfs\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "results_path = hdfs.project_path() + \"Logs/featurestore_tour_model_results_loss.txt\"\n",
    "results = json.loads(hdfs.load(results_path))\n",
    "y = results[\"loss\"] #loss\n",
    "x = list(range(1, len(y)+1))#epoch\n",
    "plt.title(\"Loss per Epoch - Football team Position Prediction\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}