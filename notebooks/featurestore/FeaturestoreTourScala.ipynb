{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature Store Tour - Scala API\n",
    " \n",
    "This notebook contains a tour/reference for the feature store Scala API on hopsworks. We will go over best practices for using the API as well as common pitfalls.\n",
    " \n",
    "The notebook is designed to be used in combination with the Feature Store Tour on Hopsworks, it assumes that you have run the following feature engineering job: [job](https://github.com/Limmen/hops-examples/tree/HOPSWORKS-721/featurestore). \n",
    "\n",
    "Which will produce the following model of feature groups in your project's feature store:\n",
    "\n",
    "![Feature Store Model](./images/model.png \"Feature Store Model\")\n",
    "\n",
    "In this notebook we will run queries over this feature store model. We will also create new feature groups and training datasets.\n",
    "\n",
    "We will go from (1) features to (2) training datasets to (3) A trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store 101\n",
    "\n",
    "The simplest way to think about the feature store is as a central place to store curated /features/ within an organization. A feature is a measurable property of some phenomenon. It could be for example an image-pixel, a word from a piece of text, the age of a person, a coordinate emitted from a sensor, or an aggregate value like the average number of purchases within the last hour.\n",
    "\n",
    "A feature store is a data management layer for machine learning that can optimize the machine learning workflow and provide an interface between data engineering and data science.\n",
    "\n",
    "![Feature Store Overview](./images/overview.png \"Feature Store Overview\")\n",
    "\n",
    "A feature store is not a pure service concept, it goes hand-in-hand with feature computation. Feature engineering is the process of transforming raw data into a format that is compatible and understandable for predictive models.\n",
    "\n",
    "There are two interfaces to the feature store:\n",
    "\n",
    "- Writing to the feature store, at the end of the feature engineering pipeline the features are written to the feature store, e.g:\n",
    "\n",
    "```scala\n",
    "val rawData = spark.read.format(\"csv\").load(filename)\n",
    "val polynomialFeatures = rawData.map((x: Float) => scala.math.pow(x, 2))\n",
    "import io.hops.util.Hops\n",
    "Hops.insertIntoFeaturegroup(\n",
    "    polynomialFeatures, \n",
    "    spark, \n",
    "    \"polynomial_features\",\n",
    "    featurestore,\n",
    "    featuregroupVersion,\n",
    "    mode,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters\n",
    ")\n",
    "```\n",
    "- Reading from the feature store, to train a model on a set of features, the features can be read from the feature store, e.g:\n",
    "\n",
    "```scala\n",
    "import io.hops.util.Hops\n",
    "val features = List(\"team_budget\", \"average_attendance\", \"average_player_age\")\n",
    "val featuresDf = Hops.getFeatures(spark, features, Hops.getProjectFeaturestore)\n",
    "```\n",
    "\n",
    "As a data engineer/data scientist, you can think of the feature store as a middle-layer. Once you have computed a set of features, instead of writing them locally to a csv file, insert them in the feature store so that the features can get documented/versioned, backfilled, **and so that your colleagues can re-use your features!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hops library is automatically installed in all Hopsworks-projects.You can find API documentation [here](http://snurran.sics.se/hops/hops-util-javadoc/0.6.0-SNAPSHOT/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1546546924580_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1546546924580_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e01_1546546924580_0003_01_000001/fs_demo__meb10000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "import io.hops.util.Hops\n",
      "import scala.collection.JavaConversions._\n",
      "import collection.JavaConverters._\n"
     ]
    }
   ],
   "source": [
    "import io.hops.util.Hops\n",
    "import scala.collection.JavaConversions._\n",
    "import collection.JavaConverters._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Name of The Project's Feature Store\n",
    "\n",
    "Each project with the feature store service enabled automatically gets its own feature store created. This feature store is only accessible within the project unless you decide to share it with other projects. The name of the feature store is `<project_name>_featurestore`, and you can get the name with the API method `getProjectFeaturestore`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = fs_demo_featurestore\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a List of All Feature Stores Accessible in the Current Project \n",
    "\n",
    "Feature Stores can be shared across projects in a multi-tenant manner, just like any Hopsworks-dataset can. You can read more about sharing datasets at [hops.io](hops.io), but in essence to share a dataset you just have to right click on it in your project. The features and featuregroups in the feature store are stored in a dataset called `<project_name>_featurestore.db` in your project.\n",
    "\n",
    "![Share Feature Store](./images/share_featurestore.png \"Share Feature Store\")\n",
    "\n",
    "The training datasets in the feature store are stored in a dataset called `project_name_Training_Datasets`. \n",
    "\n",
    "![Share Feature Store](./images/share_featurestore.png \"Share Feature Store\")\n",
    "\n",
    "Typically, if you are sharing a feature store with another project, you want to share both the `<project_name>_featurestore.db` dataset and the `project_name_Training_Datasets` dataset.\n",
    "\n",
    "To list all feature stores accessible in the current project, you can use the API method `getProjectFeaturestores()`. You can also view the list of accessible feature stores in the feature registry UI:\n",
    "\n",
    "![Share Feature Store](./images/select_fs.png \"Share Feature Store\")\n",
    "\n",
    "By using multiple feature stores and feature store sharing across projects you can enforce access rights to features.\n",
    "\n",
    "![Multi-Tenant Feature Stores](./images/multitenant.png \"Multi-Tenant Feature Stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: java.util.List[String] = [fs_demo_featurestore]\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying The Feature Store\n",
    "\n",
    "The feature store can be queried programmatically and with raw SQL. When you query the feature store programmatically, the library will infer how to fetch the different features using a **query planner**. \n",
    "\n",
    "![Feature Store Query Planner](./images/query_optimizer.png \"Feature Store Query Planner\")\n",
    "\n",
    "When interacting with the feature store it is sufficient to be familiar with three concepts:\n",
    "\n",
    "- The **feature**, this refer to an individual versioned and documented feature in the feature store, e.g the age of a person.\n",
    "- The **feature group**, this refer to a documented and versioned group of features stored as a Hive table that is linked to a specific Spark/Numpy/Pandas job that takes in raw data and outputs the computed features.\n",
    "- The **training dataset**, this refer to a versioned and managed dataset of features, stored in HopsFS as tfrecords, .csv, .tsv, or parquet.\n",
    "\n",
    "A feature group contains a group of features and a training dataset contains a set of features, potentially from many different feature groups.\n",
    "\n",
    "![Feature Store Concepts](./images/concepts.png \"Feature Store Contents\")\n",
    "\n",
    "When you query the feature store you will always get back the results in a spark dataframe. This is for scalability reasons. If the dataset is small and you want to work with it in memory you can convert it into a pandas dataframe or a numpy matrix using one line of code as we will demonstrate later on in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Individual Feature\n",
    "\n",
    "When retrieving a single feature from the featurestore, the hops-util-py library will infer in which feature group the feature belongs to by querying the metastore, but you can also explicitly specify which featuregroup and version to query. \n",
    "\n",
    "If there are multiple features of the same name in the featurestore, it is required to specify enough information to uniquely identify the feature (e.g specify feature group and version). If no featurestore is provided it will default to the project's featurestore.\n",
    "\n",
    "To read an individual feature, use the method `getFeature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|team_budget|\n",
      "+-----------+\n",
      "|  12957.076|\n",
      "|  2403.3704|\n",
      "|  3390.3755|\n",
      "|  13547.429|\n",
      "|   9678.333|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeature(spark, \"team_budget\", Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explicitly specify the feature store, feature group and version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|team_budget|\n",
      "+-----------+\n",
      "|  12957.076|\n",
      "|  2403.3704|\n",
      "|  3390.3755|\n",
      "|  13547.429|\n",
      "|   9678.333|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeature(spark, \"team_budget\", Hops.getProjectFeaturestore, \"teams_features\", 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch an Entire Feature Group\n",
    "\n",
    "You can get an entire featuregroup from the API. If no feature store is provided the API will default to the project's feature store, if no version is provided it will default to version 1 of the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "|   9678.333|      5|            5|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"teams_features\", Hops.getProjectFeaturestore, 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch A Set of Features\n",
    "\n",
    "When retrieving a list of features from the featurestore, the hops-util-py library will infer which featuregroup the features belongs to by querying the metastore. If the features reside in different featuregroups, the library will also try to infer how to join the features together based on common columns. If the JOIN query cannot be inferred due to existence of multiple features with the same name or non-obvious JOIN query, the user need to supply enough information to the API call to be able to query the featurestore. If the user already knows the JOIN query it can also run featurestore.sql(joinQuery) directly (an example of this is shown further down in this notebook). If no featurestore is provided the API will default to the project's featurestore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of querying the feature store for a list of features without specifying the feature groups and feature store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: List[String] = List(team_budget, average_attendance, average_player_age)\n",
      "+-----------+------------------+------------------+\n",
      "|team_budget|average_attendance|average_player_age|\n",
      "+-----------+------------------+------------------+\n",
      "|  12514.562|         3587.5015|             24.63|\n",
      "|  1587.0897|         2532.1638|             25.71|\n",
      "|  3839.0754|         3397.8066|             25.63|\n",
      "|  16758.066|          3271.934|             25.65|\n",
      "|  3966.3591|         4074.8047|              25.5|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features = List(\"team_budget\", \"average_attendance\", \"average_player_age\")\n",
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly specify the feature groups where the features reside. Either the feature groups and versions can be specified by prepending feature names with `<feature group name>_<feature group version.`, or by providing a Map with entries of `<feature group name> -> <feature group version>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: List[String] = List(teams_features_1.team_budget, attendances_features_1.average_attendance, players_features_1.average_player_age)\n",
      "+-----------+------------------+------------------+\n",
      "|team_budget|average_attendance|average_player_age|\n",
      "+-----------+------------------+------------------+\n",
      "|  12514.562|         3587.5015|             24.63|\n",
      "|  1587.0897|         2532.1638|             25.71|\n",
      "|  3839.0754|         3397.8066|             25.63|\n",
      "|  16758.066|          3271.934|             25.65|\n",
      "|  3966.3591|         4074.8047|              25.5|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features = List(\"teams_features_1.team_budget\", \n",
    "                    \"attendances_features_1.average_attendance\", \n",
    "                    \"players_features_1.average_player_age\")\n",
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroupsMap: scala.collection.immutable.Map[String,Integer] = Map(teams_features -> 1, attendances_features -> 1, players_features -> 1)\n",
      "javaFeaturegroupsMap: java.util.HashMap[String,Integer] = {attendances_features=1, players_features=1, teams_features=1}\n",
      "+-----------+------------------+------------------+\n",
      "|team_budget|average_attendance|average_player_age|\n",
      "+-----------+------------------+------------------+\n",
      "|  12514.562|         3587.5015|             24.63|\n",
      "|  1587.0897|         2532.1638|             25.71|\n",
      "|  3839.0754|         3397.8066|             25.63|\n",
      "|  16758.066|          3271.934|             25.65|\n",
      "|  3966.3591|         4074.8047|              25.5|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val featuregroupsMap = Map[String, Integer](\n",
    "    \"teams_features\"->1,\n",
    "    \"attendances_features\"->1,\n",
    "    \"players_features\"->1\n",
    ")\n",
    "val javaFeaturegroupsMap = new java.util.HashMap[String, Integer](featuregroupsMap)\n",
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore, javaFeaturegroupsMap).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lot of name collisions and it is not obvious how to infer the JOIN query to get the features from the feature store. You can explicitly specify the argument `joinKey` to the API (or you can provide the entire SQL query using the API method `.sql` as we will demonstrate later on in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+\n",
      "|team_budget|average_attendance|average_player_age|\n",
      "+-----------+------------------+------------------+\n",
      "|  12514.562|         3587.5015|             24.63|\n",
      "|  1587.0897|         2532.1638|             25.71|\n",
      "|  3839.0754|         3397.8066|             25.63|\n",
      "|  16758.066|          3271.934|             25.65|\n",
      "|  3966.3591|         4074.8047|              25.5|\n",
      "+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeatures(spark, features, Hops.getProjectFeaturestore, javaFeaturegroupsMap, \"team_id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Eamples of Fetching Sets of Features and Common Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting 12 features from 4 different feature groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features1: List[String] = List(team_budget, average_attendance, average_player_age, team_position, sum_attendance, average_player_rating, average_player_worth, sum_player_age, sum_player_rating, sum_player_worth, sum_position, average_position)\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|team_budget|average_attendance|average_player_age|team_position|sum_attendance|average_player_rating|average_player_worth|sum_player_age|sum_player_rating|sum_player_worth|sum_position|average_position|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|  12514.562|         3587.5015|             24.63|           31|      71750.03|            240.30573|            231.8708|        2463.0|        24030.572|        23187.08|      1188.0|            59.4|\n",
      "|  1587.0897|         2532.1638|             25.71|           34|     50643.277|            240.39302|           223.71338|        2571.0|        24039.303|       22371.338|      1213.0|           60.65|\n",
      "|  3839.0754|         3397.8066|             25.63|           28|      67956.13|            297.92935|           280.11465|        2563.0|        29792.936|       28011.465|      1072.0|            53.6|\n",
      "|  16758.066|          3271.934|             25.65|           26|      65438.68|            322.69797|           307.87268|        2565.0|        32269.797|       30787.268|      1103.0|           55.15|\n",
      "|  3966.3591|         4074.8047|              25.5|           27|      81496.09|            297.79196|           298.78235|        2550.0|        29779.197|       29878.234|      1142.0|            57.1|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features1 = List(\"team_budget\", \"average_attendance\", \"average_player_age\", \"team_position\", \n",
    "                     \"sum_attendance\", \"average_player_rating\", \"average_player_worth\", \"sum_player_age\", \n",
    "                     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \"average_position\")\n",
    "Hops.getFeatures(spark, features1, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example of a common error that can occur when you query the feature store.\n",
    "\n",
    "If you try to query the feature store for a feature that exists in multiple feature groups, it is impossible for the query planner to infer from which feature group to fetch the feature so it will throw an exception. When this error happen you should specify which feature group to fetch from so that the query planner knows how to get the feature.\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Found the feature with name: team_id in more than one of the featuregroups of the featurestore fs_demo_featurestore please specify featuregroup that you want to get the feature from. The matched featuregroups are: games_features_1, season_scores_features_1, attendances_features_1, players_features_1, teams_features_1, teams_features_spanish_1, teams_features_spanish_2, pandas_test_example_1, numpy_test_example_1, python_test_example_1\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeature(FeaturestoreHelper.java:407)\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.findFeaturegroupsThatContainsFeatures(FeaturestoreHelper.java:267)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1617)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features1 = List(\"team_budget\", \"team_id\")\n",
    "Hops.getFeatures(spark, features1, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features1: List[String] = List(team_budget, team_id)\n",
      "featuregroupsMap: scala.collection.immutable.Map[String,Integer] = Map(teams_features -> 1)\n",
      "javaFeaturegroupsMap: java.util.HashMap[String,Integer] = {teams_features=1}\n",
      "+-----------+-------+\n",
      "|team_budget|team_id|\n",
      "+-----------+-------+\n",
      "|  12957.076|      1|\n",
      "|  2403.3704|      2|\n",
      "|  3390.3755|      3|\n",
      "|  13547.429|      4|\n",
      "|   9678.333|      5|\n",
      "+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features1 = List(\"team_budget\", \"team_id\")\n",
    "val featuregroupsMap = Map[String, Integer](\n",
    "    \"teams_features\"->1\n",
    ")\n",
    "val javaFeaturegroupsMap = new java.util.HashMap[String, Integer](featuregroupsMap)\n",
    "Hops.getFeatures(spark, features1, Hops.getProjectFeaturestore, javaFeaturegroupsMap).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common error is that you try to fetch features from feature groups that are not compatible, they do not got any natural join column. Typically in this case you need to either provide the join key your self or use SQL directly with `featurestore.sql()`.\n",
    "\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalArgumentException: Could not find any common columns in featuregroups to join on, searched through the following featuregroups: teams_features, games_features\n",
      "  at io.hops.util.featurestore.FeaturestoreHelper.getJoinColumn(FeaturestoreHelper.java:549)\n",
      "  at io.hops.util.Hops.getFeatures(Hops.java:1618)\n",
      "  ... 54 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val features1 = List(\"team_budget\", \"score\")\n",
    "Hops.getFeatures(spark, features1, Hops.getProjectFeaturestore).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fix the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|team_budget|score|\n",
      "+-----------+-----+\n",
      "|  11296.577|    1|\n",
      "|   4969.735|    3|\n",
      "|  21319.533|    2|\n",
      "|  15072.062|    1|\n",
      "|  12957.076|    3|\n",
      "|   760.8729|    1|\n",
      "|  20347.281|    2|\n",
      "|   2248.776|    3|\n",
      "|  11296.577|    1|\n",
      "|  12514.562|    1|\n",
      "|  16758.066|    1|\n",
      "|  13547.429|    3|\n",
      "|  11169.979|    1|\n",
      "|  1583.5911|    1|\n",
      "|   3555.235|    1|\n",
      "|  8154.7256|    1|\n",
      "|   2248.776|    1|\n",
      "|  7683.7227|    1|\n",
      "|  910.39325|    1|\n",
      "|   9775.455|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.queryFeaturestore(spark,\n",
    "    \"SELECT team_budget, score \" +\n",
    "    \"FROM teams_features_1 JOIN games_features_1 ON \" +\n",
    "    \"games_features_1.home_team_id = teams_features_1.team_id\", Hops.getProjectFeaturestore).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Text SQL Query from the Feature Store\n",
    "\n",
    "For complex queries that cannot be inferred by the helper functions, enter the sql directly to the method `featurestore.sql()` it will default to the project specific feature store but you can also specify it explicitly. If you are proficient in SQL, this is the most efficient and preferred way to query the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "+-----------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.queryFeaturestore(spark,\n",
    "                       \"SELECT * FROM teams_features_1 WHERE team_position < 5\", \n",
    "                       Hops.getProjectFeaturestore).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to the Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Feature Groups\n",
    "\n",
    "In most cases it is recommended that feature groups are created in the UI on Hopsworks and that care is taken in documenting the feature group. \n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg.png \"Create Feature Group from the UI\")\n",
    "\n",
    "![Create Feature Group from the UI](./images/create_fg_2.png \"Create Feature Group from the UI\")\n",
    "\n",
    "However, sometimes it is practical to create a feature group directly from a spark dataframe and fill in the metadata about the featuregroup later in the UI. This can be done through the `createFeaturegroup()` API function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new featuregroup called **teams_features_spanish** that contains the same contents as the feature group teams_features except the the columns are renamed to spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teamsFeaturesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [team_budget: float, team_id: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val teamsFeaturesDf = Hops.getFeaturegroup(spark, \"teams_features\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teamsFeaturesDf2: org.apache.spark.sql.DataFrame = [equipo_presupuesto: float, equipo_id: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val teamsFeaturesDf2 = teamsFeaturesDf.withColumnRenamed(\n",
    "    \"team_id\", \"equipo_id\").withColumnRenamed(\n",
    "    \"team_budget\", \"equipo_presupuesto\").withColumnRenamed(\n",
    "    \"team_position\", \"equipo_posicion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "|          9678.333|        5|              5|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamsFeaturesDf2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a new featuregroup using the transformed dataframe (we'll explain the statistics part later on in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = a spanish version of teams_features\n"
     ]
    }
   ],
   "source": [
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a spanish version of teams_features\"\n",
    "\n",
    "Hops.createFeaturegroup(\n",
    "    spark, teamsFeaturesDf2, \"teams_features_spanish\", Hops.getProjectFeaturestore,\n",
    "    1, description, jobId,\n",
    "    dependencies, primaryKey, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the new featuregroup will be created in the project's featurestore and the statistics for the new featuregroup will be computed based on the provided spark dataframe. You can configure this behaviour by modifying the default arguments and filling in extra metadata.\n",
    "\n",
    "The dependencies argument takes a list of HDFS file names that the feature group depends on, i.e when the datasets that a featuregroup depends on have been modified, the feature group should be recomputed. The dependencies can also be updated and viewed in the feature registry UI. \n",
    "\n",
    "![Feature group dependencies](./images/deps.png \"Feature group dependencies\")\n",
    "\n",
    "![Feature group dependencies](./images/deps2.png \"Feature group dependencies\")\n",
    "\n",
    "The jobId argument takes an integer that identifies the job id to compute the features. Once you have created a job that creates/inserts features in the feature store you can use the Featurestore UI to link that job to the featuregroup:\n",
    "\n",
    "![Feature group jobs](./images/jobs1.png \"Feature group jobs\")\n",
    "\n",
    "![Feature group jobs](./images/jobs2.png \"Feature group jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Feature Group\n",
    "\n",
    "To create a new version, simply use the `createFeaturegroup` method and change the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = a spanish version of teams_features\n"
     ]
    }
   ],
   "source": [
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a spanish version of teams_features\"\n",
    "\n",
    "Hops.createFeaturegroup(\n",
    "    spark, teamsFeaturesDf2, \"teams_features_spanish\", Hops.getProjectFeaturestore,\n",
    "    2, description, jobId,\n",
    "    dependencies, primaryKey, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Feature Group Version](./images/create_fg_version.png \"Create Feature Group Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Feature Group (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latestVersion: Int = 2\n",
      "res75: Int = 2\n"
     ]
    }
   ],
   "source": [
    "val latestVersion = Hops.getLatestFeaturegroupVersion(\"teams_features_spanish\", Hops.getProjectFeaturestore)\n",
    "latestVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into Existing Feature Groups\n",
    "\n",
    "A best practice when working with features in HopsML is to first figure out a model of feature groups and create them  using the Feature Registry UI. This will prepare the feature group schema and create the Hive tables. Once the empty feature groups are created, then you can insert into these tables directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get some sample data to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql._\n",
      "import spark.implicits._\n",
      "import org.apache.spark.sql.types._\n",
      "sampleData: Seq[org.apache.spark.sql.Row] = List([999,41251.52,1], [998,1319.4,8], [997,21219.1,2])\n",
      "sampleSchema: List[org.apache.spark.sql.types.StructField] = List(StructField(equipo_id,IntegerType,true), StructField(equipo_presupuesto,FloatType,true), StructField(equipo_posicion,IntegerType,true))\n",
      "sampleDF: org.apache.spark.sql.DataFrame = [equipo_id: int, equipo_presupuesto: float ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "val sampleData = Seq(\n",
    "    Row(999, 41251.52f, 1),\n",
    "    Row(998, 1319.4f, 8),\n",
    "    Row(997, 21219.1f, 2)\n",
    ")\n",
    "val sampleSchema = List(\n",
    "  StructField(\"equipo_id\", IntegerType, true),\n",
    "  StructField(\"equipo_presupuesto\", FloatType, true),\n",
    "  StructField(\"equipo_posicion\", IntegerType, true)\n",
    ")\n",
    "val sampleDF = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(sampleData),\n",
    "  StructType(sampleSchema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "|equipo_id|equipo_presupuesto|equipo_posicion|\n",
      "+---------+------------------+---------------+\n",
      "|      999|          41251.52|              1|\n",
      "|      998|            1319.4|              8|\n",
      "|      997|           21219.1|              2|\n",
      "+---------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res77: Long = 3\n"
     ]
    }
   ],
   "source": [
    "sampleDF.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the contents of the featuregroup `teams_features_spanish` that we are going to insert the sample data into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanishTeamsFeaturesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [equipo_presupuesto: float, equipo_id: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val spanishTeamsFeaturesDf = Hops.getFeaturegroup(spark, \"teams_features_spanish\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "|          9678.333|        5|              5|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spanishTeamsFeaturesDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res79: Long = 50\n"
     ]
    }
   ],
   "source": [
    "spanishTeamsFeaturesDf.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can insert the sample data and verify the new contents of the featuregroup. By default the insert mode is \"append\", the featurestore is the project's featurestore, the version is 1 and statistics will be updated (we cover statistics later on in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = teams_features_spanish\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "mode: String = append\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"teams_features_spanish\"\n",
    "val featurestore = Hops.getProjectFeaturestore \n",
    "val featuregroupVersion = 1 \n",
    "val mode = \"append\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "\n",
    "Hops.insertIntoFeaturegroup(\n",
    "    spark, \n",
    "    sampleDF, \n",
    "    featuregroup,\n",
    "    featurestore,\n",
    "    featuregroupVersion,\n",
    "    mode,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fetch the updated feature group from the feature store and verify that the update was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanishTeamsFeaturesUpdatedDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [equipo_presupuesto: float, equipo_id: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val spanishTeamsFeaturesUpdatedDf = Hops.getFeaturegroup(spark, \"teams_features_spanish\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------------+\n",
      "|equipo_presupuesto|equipo_id|equipo_posicion|\n",
      "+------------------+---------+---------------+\n",
      "|          41251.52|      999|              1|\n",
      "|         12957.076|        1|              1|\n",
      "|         2403.3704|        2|              2|\n",
      "|         3390.3755|        3|              3|\n",
      "|         13547.429|        4|              4|\n",
      "+------------------+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spanishTeamsFeaturesUpdatedDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res83: Long = 53\n"
     ]
    }
   ],
   "source": [
    "spanishTeamsFeaturesUpdatedDf.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two supported insert modes are \"append\" and \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = teams_features_spanish\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "mode: String = overwrite\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"teams_features_spanish\"\n",
    "val featurestore = Hops.getProjectFeaturestore \n",
    "val featuregroupVersion = 1 \n",
    "val mode = \"overwrite\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "\n",
    "Hops.insertIntoFeaturegroup(\n",
    "    spark, \n",
    "    sampleDF, \n",
    "    featuregroup,\n",
    "    featurestore,\n",
    "    featuregroupVersion,\n",
    "    mode,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------------------+\n",
      "|equipo_id|equipo_posicion|equipo_presupuesto|\n",
      "+---------+---------------+------------------+\n",
      "|      999|              1|          41251.52|\n",
      "|      998|              8|            1319.4|\n",
      "|      997|              2|           21219.1|\n",
      "+---------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"teams_features_spanish\", Hops.getProjectFeaturestore, 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res87: Long = 3\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroup(spark, \"teams_features_spanish\", Hops.getProjectFeaturestore, 1).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Group Statistics\n",
    "\n",
    "Statistics about a featuregroup can be useful in the stage of feature engineering and when deciding which features to use for training. If statistics have been computed for a feature group, it can be viewed in the Hopsworks Feature Registry UI. \n",
    "\n",
    "This is particularly useful within large organizations where data scientists from different teams can re-use and explore new features by browsing features in the feature store and analyzing the statistics.\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_1.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_2.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_3.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_4.png \"Feature Registry Statistics Visualization\")\n",
    "\n",
    "![Feature Registry Statistics Visualization](./images/fg_stats_5.png \"Feature Registry Statistics Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have notived earlier in this notebook, both the `insertIntoFeaturegroup` and `createFeaturegroup` methods have arguments for updating the statistics as new data is added. \n",
    "\n",
    "You can also use the `updateFeaturegroupStats()` method to update the statistics of a feature group without inserting any new data. By default it will compute all statistics (descriptive, feature correlation, histograms, and cluster analysis), use the project's featurestore, use version 1 of the featuregroup and use all columns for computing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = teams_features\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "descriptiveStats: Boolean = true\n",
      "featureCorr: Boolean = true\n",
      "featureHistograms: Boolean = true\n",
      "clusterAnalysis: Boolean = true\n",
      "statColumns: Null = null\n",
      "numBins: Int = 20\n",
      "corrMethod: String = pearson\n",
      "numClusters: Int = 5\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"teams_features\"\n",
    "val featurestore = Hops.getProjectFeaturestore\n",
    "val featuregroupVersion = 1\n",
    "val descriptiveStats = true\n",
    "val featureCorr = true\n",
    "val featureHistograms = true\n",
    "val clusterAnalysis = true\n",
    "val statColumns = null // null means all columns will be used\n",
    "val numBins = 20\n",
    "val corrMethod = \"pearson\"\n",
    "val numClusters = 5\n",
    "Hops.updateFeaturegroupStats(\n",
    "    spark, featuregroup, Hops.getProjectFeaturestore, featuregroupVersion,\n",
    "    descriptiveStats, featureCorr, featureHistograms, clusterAnalysis, statColumns,\n",
    "    numBins, corrMethod, numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compute statistics for certain set of columns and exclude surrogate key-columns for example, you can use the argument `statColumns` to specify which columns to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featuregroup: String = teams_features\n",
      "featurestore: String = fs_demo_featurestore\n",
      "featuregroupVersion: Int = 1\n",
      "descriptiveStats: Boolean = true\n",
      "featureCorr: Boolean = true\n",
      "featureHistograms: Boolean = true\n",
      "clusterAnalysis: Boolean = true\n",
      "statColumns: java.util.List[String] = [team_budget, team_position]\n",
      "numBins: Int = 20\n",
      "corrMethod: String = pearson\n",
      "numClusters: Int = 5\n"
     ]
    }
   ],
   "source": [
    "val featuregroup = \"teams_features\"\n",
    "val featurestore = Hops.getProjectFeaturestore\n",
    "val featuregroupVersion = 1\n",
    "val descriptiveStats = true\n",
    "val featureCorr = true\n",
    "val featureHistograms = true\n",
    "val clusterAnalysis = true\n",
    "val statColumns = List[String](\"team_budget\", \"team_position\").asJava\n",
    "val numBins = 20\n",
    "val corrMethod = \"pearson\"\n",
    "val numClusters = 5\n",
    "Hops.updateFeaturegroupStats(\n",
    "    spark, featuregroup, Hops.getProjectFeaturestore, featuregroupVersion,\n",
    "    descriptiveStats, featureCorr, featureHistograms, clusterAnalysis, statColumns,\n",
    "    numBins, corrMethod, numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Datasets\n",
    "\n",
    "To group data in the feature store we use three concepts:\n",
    "\n",
    "- Feature\n",
    "- Feature group\n",
    "- Training Dataset\n",
    "\n",
    "Typically during the feature engineering phase of a machine learning project, you compute a set of features for each type of data that you have, these features are naturally grouped into a documented and versioned **feature group**. \n",
    "\n",
    "In practice, it is common that organizations have many different type of datasets that they can extract features from, for example if you are building a recommendation system you might have demographic data about each user as well as user-activity data. \n",
    "\n",
    "When you train a machine learning model, you want to use all features that have predictive power and that the model can learn from. At this point, we can create a training dataset of features from several different feature groups and use that for training. That is the purpose of the training dataset abstraction. \n",
    "\n",
    "Of course you can always just save a group of features anywhere inside your project, e.g as a csv, or .tfrecords file. However, by using the feature store you can create **managed** training datasets. Managed training datasets will show up in the feature registry UI and will automatically be versioned, documented and reproducible. \n",
    "\n",
    "![Feature Engineering Pipeline](./images/pipeline.png \"Feature Engineering Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata for a training dataset can be created from the Hopsworks UI or directly from the API with the function `createTrainingDataset`. The training datasets in a project are stored in a top-level dataset called `projectName_Training_Datasets`, (i.e `hdfs:///Projects/<ProjectName>/<ProjectName>_Training_Datasets`.\n",
    "\n",
    "Once a training dataset have been created you can find it in the featurestore UI in hopsworks under the tab `Training datasets`, from there you can also edit the metadata if necessary. \n",
    "\n",
    "![Find Training Datasets](./images/find_training_datasets.png \"Find Training Datasets\")\n",
    "After a training dataset have been created with the necessary metadata you can save the actual data in the training dataset by using the API function `insertIntoTrainingDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dataset called `team_position_prediction` by using a set of relevant features from the featurestore. We will combine features from four different feature groups to form this training dataset: `teams_features`, `attendances_features`, `players_features`, `season_scores_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: List[String] = List(team_budget, average_attendance, average_player_age, team_position, sum_attendance, average_player_rating, average_player_worth, sum_player_age, sum_player_rating, sum_player_worth, sum_position, average_position)\n",
      "featuresDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [team_budget: float, average_attendance: float ... 10 more fields]\n"
     ]
    }
   ],
   "source": [
    "val features = List(\"team_budget\", \"average_attendance\", \"average_player_age\", \"team_position\", \n",
    "                     \"sum_attendance\", \"average_player_rating\", \"average_player_worth\", \"sum_player_age\", \n",
    "                     \"sum_player_rating\", \"sum_player_worth\", \"sum_position\", \"average_position\")\n",
    "val featuresDf = Hops.getFeatures(spark, features, Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|team_budget|average_attendance|average_player_age|team_position|sum_attendance|average_player_rating|average_player_worth|sum_player_age|sum_player_rating|sum_player_worth|sum_position|average_position|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|  12514.562|         3587.5015|             24.63|           31|      71750.03|            240.30573|            231.8708|        2463.0|        24030.572|        23187.08|      1188.0|            59.4|\n",
      "|  1587.0897|         2532.1638|             25.71|           34|     50643.277|            240.39302|           223.71338|        2571.0|        24039.303|       22371.338|      1213.0|           60.65|\n",
      "|  3839.0754|         3397.8066|             25.63|           28|      67956.13|            297.92935|           280.11465|        2563.0|        29792.936|       28011.465|      1072.0|            53.6|\n",
      "|  16758.066|          3271.934|             25.65|           26|      65438.68|            322.69797|           307.87268|        2565.0|        32269.797|       30787.268|      1103.0|           55.15|\n",
      "|  3966.3591|         4074.8047|              25.5|           27|      81496.09|            297.79196|           298.78235|        2550.0|        29779.197|       29878.234|      1142.0|            57.1|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Latest Version of a Training Dataset (0 if no version exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latestVersion: Int = 0\n",
      "res37: Int = 0\n"
     ]
    }
   ],
   "source": [
    "val latestVersion = Hops.getLatestTrainingDatasetVersion(\"team_position_prediction\", Hops.getProjectFeaturestore)\n",
    "latestVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as Training Dataset in TFRecords Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a training dataset from the dataframe with some extended metadata such as schema (automatically inferred). By default when you create a training dataset it will be in \"tfrecords\" format and statistics will be computed for all features. After the dataset have been created you can view and/or update the metadata about the training dataset from the Hopsworks featurestore UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDatasetName: String = team_position_prediction\n",
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "dataFormat: String = tfrecords\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = a dataset with features for football teams, used for training a model to predict league-position\n",
      "trainingDatasetVersion: Int = 1\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"team_position_prediction\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"tfrecords\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a dataset with features for football teams, used for training a model to predict league-position\"\n",
    "val trainingDatasetVersion = latestVersion + 1\n",
    "Hops.createTrainingDataset(\n",
    "    spark, featuresDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    trainingDatasetVersion, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latestVersion: Int = 0\n",
      "res39: Int = 0\n"
     ]
    }
   ],
   "source": [
    "val latestVersion = Hops.getLatestTrainingDatasetVersion(\"team_position_prediction_csv\", Hops.getProjectFeaturestore)\n",
    "latestVersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDatasetName: String = team_position_prediction_csv\n",
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "dataFormat: String = csv\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = a dataset with features for football teams, used for training a model to predict league-position\n",
      "trainingDatasetVersion: Int = 1\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"team_position_prediction_csv\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"csv\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a dataset with features for football teams, used for training a model to predict league-position\"\n",
    "val trainingDatasetVersion = latestVersion + 1\n",
    "Hops.createTrainingDataset(\n",
    "    spark, featuresDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    trainingDatasetVersion, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now go to the Feature Registy UI in Hopsworks we can see that the training dataset have been created for us and things like versioning, documentation, and recomputation is managed for us. We can also easily edit the metadata from the UI if necesssary.\n",
    "\n",
    "![Training Dataset UI](./images/training_dataset.png \"Training Dataset UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Errors\n",
    "\n",
    "Lets look at a common error that can occur when you create training datasets. If you try to create a training dataset that already exists, you will get an error.\n",
    "\n",
    "**Note**: <font color='red'>This cell should fail, don't panic :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "io.hops.util.exceptions.TrainingDatasetCreationError: Could not create trainingDataset:team_position_prediction , error code: 260017 error message: The provided training dataset name already exists, user message: The path to create the dataset already exists: /Projects/fs_demo/fs_demo_Training_Datasets/team_position_prediction_1, delete the directory and try again.\n",
      "  at io.hops.util.Hops.createTrainingDatasetRest(Hops.java:1137)\n",
      "  at io.hops.util.Hops.createTrainingDataset(Hops.java:1977)\n",
      "  ... 60 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"team_position_prediction\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"tfrecords\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a dataset with features for football teams, used for training a model to predict league-position\"\n",
    "val trainingDatasetVersion = Hops.getLatestTrainingDatasetVersion(\"team_position_prediction\", Hops.getProjectFeaturestore)\n",
    "latestVersion\n",
    "Hops.createTrainingDataset(\n",
    "    spark, featuresDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    trainingDatasetVersion, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this error, either choose a different name for your training dataset or delete the existing training dataset before creating the new one. To delete a training dataset, use the feature store UI or delete the training dataset folder directly.\n",
    "\n",
    "**Option 1, use the feature store UI:**\n",
    "\n",
    "![Delete Training Dataset](./images/delete_training_dataset.png \"Delete Training Dataset\")\n",
    "\n",
    "**Option 2, delete the dataset folder directly:**\n",
    "\n",
    "![Delete Training Dataset](./images/delete_td_folder_1.png \"Delete Training Dataset\")\n",
    "\n",
    "![Delete Training Dataset](./images/delete_td_folder_2.png \"Delete Training Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a New Version of A Training Dataset\n",
    "\n",
    "To create a new version, simply use the `createTrainingDataset` method and specify the version argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDatasetName: String = team_position_prediction\n",
      "jobId: Null = null\n",
      "dependencies: java.util.List[String] = []\n",
      "primaryKey: Null = null\n",
      "dataFormat: String = tfrecords\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n",
      "description: String = a dataset with features for football teams, used for training a model to predict league-position\n",
      "trainingDatasetVersion: Int = 2\n"
     ]
    }
   ],
   "source": [
    "val trainingDatasetName = \"team_position_prediction\"\n",
    "val jobId = null\n",
    "val dependencies = List[String]().asJava\n",
    "val primaryKey = null\n",
    "val dataFormat = \"tfrecords\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "val description = \"a dataset with features for football teams, used for training a model to predict league-position\"\n",
    "val trainingDatasetVersion = Hops.getLatestTrainingDatasetVersion(\"team_position_prediction\", Hops.getProjectFeaturestore) + 1\n",
    "Hops.createTrainingDataset(\n",
    "    spark, featuresDf, trainingDatasetName, Hops.getProjectFeaturestore,\n",
    "    trainingDatasetVersion, description, jobId, dataFormat,\n",
    "    dependencies, descriptiveStats, featureCorr,\n",
    "      featureHistograms, clusterAnalysis, statColumns, numBins,\n",
    "      corrMethod, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the new version in the feature store UI:\n",
    "\n",
    "![Create Training Dataset Version](./images/create_td_version.png \"Create Training Dataset Version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Into an Existing Training Dataset\n",
    "\n",
    "Once a dataset have been created, its metadata is browsable in the featurestore registry in the Hopsworks UI. If you don't want to create a new training dataset but just overwrite or insert new data into an existing training dataset, you can use the API function `insertIntoTrainingDataset`.  \n",
    "\n",
    "**Note**: \"append\" write mode is not supported for training datasets stored in tfrecords format, only \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDataset: String = team_position_prediction_csv\n",
      "featurestore: String = fs_demo_featurestore\n",
      "trainingDatasetVersion: Int = 1\n",
      "mode: String = overwrite\n",
      "descriptiveStats: Boolean = false\n",
      "featureCorr: Boolean = false\n",
      "featureHistograms: Boolean = false\n",
      "clusterAnalysis: Boolean = false\n",
      "statColumns: java.util.List[String] = []\n",
      "numBins: Null = null\n",
      "corrMethod: Null = null\n",
      "numClusters: Null = null\n"
     ]
    }
   ],
   "source": [
    "val trainingDataset = \"team_position_prediction_csv\"\n",
    "val featurestore = Hops.getProjectFeaturestore \n",
    "val trainingDatasetVersion = latestVersion + 1\n",
    "val mode = \"overwrite\"\n",
    "val descriptiveStats = false\n",
    "val featureCorr = false\n",
    "val featureHistograms = false\n",
    "val clusterAnalysis = false\n",
    "val statColumns = List[String]().asJava\n",
    "val numBins = null\n",
    "val corrMethod = null\n",
    "val numClusters = null\n",
    "\n",
    "Hops.insertIntoTrainingDataset(\n",
    "    spark, \n",
    "    featuresDf,\n",
    "    trainingDataset,\n",
    "    featurestore,\n",
    "    trainingDatasetVersion,\n",
    "    descriptiveStats, \n",
    "    featureCorr,\n",
    "    featureHistograms, \n",
    "    clusterAnalysis, \n",
    "    statColumns, \n",
    "    numBins,\n",
    "    corrMethod, \n",
    "    numClusters,\n",
    "    mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training Dataset Path\n",
    "\n",
    "After a **managed dataset** have been created, it is easy to share it and re-use it for training various models. For example if the dataset have been materialized in tf-records format you can call the method `getTrainingDatasetPath()` to get the HDFS path and read it directly in your model training (e.g tensorflow) code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res46: String = hdfs://default/Projects/fs_demo/fs_demo_Training_Datasets/team_position_prediction_csv_1/team_position_prediction_csv\n"
     ]
    }
   ],
   "source": [
    "Hops.getTrainingDatasetPath(\"team_position_prediction_csv\", Hops.getProjectFeaturestore, Hops.getLatestTrainingDatasetVersion(\"team_position_prediction_csv\", Hops.getProjectFeaturestore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Training Dataset into a Spark Dataframe\n",
    "\n",
    "Typically training datasets are served into deep learning frameworks such as pytorch or tensorflow. However, training datasets can also be read into spark dataframes using the api method `getTrainingDataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|team_budget|average_attendance|average_player_age|team_position|sum_attendance|average_player_rating|average_player_worth|sum_player_age|sum_player_rating|sum_player_worth|sum_position|average_position|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "|  10290.323|         4964.6475|             26.63|           23|     99292.945|             327.6003|            315.4794|        2663.0|        32760.031|       31547.941|      1100.0|            55.0|\n",
      "|  20347.281|         2420.8076|             26.18|           39|     48416.152|            205.15598|           194.33916|        2618.0|        20515.598|       19433.916|      1232.0|            61.6|\n",
      "|  16758.066|          3271.934|             25.65|           26|      65438.68|            322.69797|           307.87268|        2565.0|        32269.797|       30787.268|      1103.0|           55.15|\n",
      "|  3966.3591|         4074.8047|              25.5|           27|      81496.09|            297.79196|           298.78235|        2550.0|        29779.197|       29878.234|      1142.0|            57.1|\n",
      "|  4888.2324|         10853.007|             25.66|           11|     217060.14|             657.3932|           674.28235|        2566.0|         65739.32|       67428.234|       645.0|           32.25|\n",
      "+-----------+------------------+------------------+-------------+--------------+---------------------+--------------------+--------------+-----------------+----------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Hops.getTrainingDataset(spark, \"team_position_prediction_csv\", Hops.getProjectFeaturestore, Hops.getLatestTrainingDatasetVersion(\"team_position_prediction_csv\", Hops.getProjectFeaturestore)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Training Dataset Stats\n",
    "\n",
    "The API is similar to the one for updating the stats of a feature group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingDataset: String = team_position_prediction\n",
      "featurestore: String = fs_demo_featurestore\n",
      "trainingDatasetVersion: Int = 1\n",
      "descriptiveStats: Boolean = true\n",
      "featureCorr: Boolean = true\n",
      "featureHistograms: Boolean = true\n",
      "clusterAnalysis: Boolean = true\n",
      "statColumns: Null = null\n",
      "numBins: Int = 20\n",
      "corrMethod: String = pearson\n",
      "numClusters: Int = 5\n"
     ]
    }
   ],
   "source": [
    "val trainingDataset = \"team_position_prediction\"\n",
    "val featurestore = Hops.getProjectFeaturestore\n",
    "val trainingDatasetVersion = 1\n",
    "val descriptiveStats = true\n",
    "val featureCorr = true\n",
    "val featureHistograms = true\n",
    "val clusterAnalysis = true\n",
    "val statColumns = null // null means all columns will be used\n",
    "val numBins = 20\n",
    "val corrMethod = \"pearson\"\n",
    "val numClusters = 5\n",
    "Hops.updateTrainingDatasetStats(\n",
    "    spark, trainingDataset, Hops.getProjectFeaturestore, trainingDatasetVersion,\n",
    "    descriptiveStats, featureCorr, featureHistograms, clusterAnalysis, statColumns,\n",
    "    numBins, corrMethod, numClusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Managed Training Dataset Without Using the API \n",
    "\n",
    "To create a **managed** training dataset without using the API, e.g to create a managed training dataset from a .tfrecords file downloaded from Kaggle or from a .csv file from Kaggle, first go to the Feature Registry UI and create a new training dataset and fill in the metadata:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_1.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_2.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Once the dataset have been created from the UI, you can find that inside the \"Training Datasets\" folder in your project a new folder for the dataset have showed up that is called `training_datasetname_version`:\n",
    "\n",
    "![Create Training Dataset From the UI](./images/create_td_3.png \"Create Training Dataset From the UI\")\n",
    "\n",
    "Simply upload your dataset inside that folder, e.g you can upload for example a single .csv file or a folder with part-r-X.csv files. **It is important that you name the folder/file the name of your training dataset, e.g sample_dataset or sample_dataset.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Featurestore Metadata\n",
    "To explore the contents of the featurestore we recommend using the featurestore page in the Hopsworks UI but you can also get the metadata programmatically from the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Stores Accessible In the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res49: java.util.List[String] = [fs_demo_featurestore]\n"
     ]
    }
   ],
   "source": [
    "Hops.getProjectFeaturestores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Feature Groups in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res50: java.util.List[String] = [games_features_1, season_scores_features_1, attendances_features_1, players_features_1, teams_features_1, teams_features_spanish_1, teams_features_spanish_2, pandas_test_example_1, numpy_test_example_1, python_test_example_1]\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturegroups(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Features in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res51: java.util.List[String] = [away_team_id, home_team_id, score, average_position, sum_position, team_id, average_attendance, sum_attendance, team_id, average_player_age, average_player_rating, average_player_worth, sum_player_age, sum_player_rating, sum_player_worth, team_id, team_budget, team_id, team_position, equipo_id, equipo_posicion, equipo_presupuesto, equipo_id, equipo_posicion, equipo_presupuesto, average_attendance_test, average_player_age_test, team_budget_test, col_0, col_1, col_2, col_0, col_1, col_2]\n"
     ]
    }
   ],
   "source": [
    "Hops.getFeaturesList(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Training Datasets in a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res52: java.util.List[String] = [team_position_prediction_csv_1, team_position_prediction_1, team_position_prediction_tsv_1, team_position_prediction_parquet_1, team_position_prediction_hdf5_1, team_position_prediction_npy_1, team_position_prediction_2, team_position_prediction_parquet_2]\n"
     ]
    }
   ],
   "source": [
    "Hops.getTrainingDatasets(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All Metadata (Features, Feature groups, Training Datasets) for a Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res53: io.hops.util.featurestore.FeaturegroupsAndTrainingDatasetsDTO = FeaturegroupsAndTrainingDatasetsDTO{featuregroups=[FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/games_features_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/season_scores_features_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/attendances_features_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/players_features_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/apps/hive/warehouse/fs_demo_featurestore.db/teams_features_1]}, FeaturegroupDTO{, hdfsStorePaths=[hdfs://10.0.2.15:8020/app..."
     ]
    }
   ],
   "source": [
    "Hops.getFeaturestoreMetadata(Hops.getProjectFeaturestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Raw Data to Features to Training Dataset to Model\n",
    "\n",
    "Once a training dataset have been materialized, we can use it to train a model. In this section we will train an example model using the training dataset `team_position_prediction` that we just created. We will use the column **\"team_position\"** as the target to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In this example we will use Spark MLLib. However, the feature store is in theory agnostic to which framework or method you use for training the model, it works with PyTorch, Tensorflow, MxNet etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "import org.apache.spark.ml.regression.LinearRegression\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ITER: Int = 1000\n",
      "ELASTIC_REG_PARAM: Double = 0.8\n",
      "REG_LAMBDA_PARAM: Double = 0.3\n"
     ]
    }
   ],
   "source": [
    "val NUM_ITER = 1000\n",
    "val ELASTIC_REG_PARAM = 0.8\n",
    "val REG_LAMBDA_PARAM = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TFRecords Dataset into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [average_player_rating: float, average_attendance: float ... 10 more fields]\n"
     ]
    }
   ],
   "source": [
    "val dataset_df = Hops.getTrainingDataset(spark, \"team_position_prediction\", Hops.getProjectFeaturestore, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+-----------------+------------+----------------+------------------+-------------+--------------------+-----------+----------------+--------------+--------------+\n",
      "|average_player_rating|average_attendance|sum_player_rating|sum_position|sum_player_worth|average_player_age|team_position|average_player_worth|team_budget|average_position|sum_player_age|sum_attendance|\n",
      "+---------------------+------------------+-----------------+------------+----------------+------------------+-------------+--------------------+-----------+----------------+--------------+--------------+\n",
      "|            240.30573|         3587.5015|        24030.572|      1188.0|        23187.08|             24.63|           31|            231.8708|  12514.562|            59.4|        2463.0|      71750.03|\n",
      "|            240.39302|         2532.1638|        24039.303|      1213.0|       22371.338|             25.71|           34|           223.71338|  1587.0897|           60.65|        2571.0|     50643.277|\n",
      "|            297.92935|         3397.8066|        29792.936|      1072.0|       28011.465|             25.63|           28|           280.11465|  3839.0754|            53.6|        2563.0|      67956.13|\n",
      "|            322.69797|          3271.934|        32269.797|      1103.0|       30787.268|             25.65|           26|           307.87268|  16758.066|           55.15|        2565.0|      65438.68|\n",
      "|            297.79196|         4074.8047|        29779.197|      1142.0|       29878.234|              25.5|           27|           298.78235|  3966.3591|            57.1|        2550.0|      81496.09|\n",
      "+---------------------+------------------+-----------------+------------+----------------+------------------+-------------+--------------------+-----------+----------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Dataframe into Spark MLLib data format\n",
    "\n",
    "Spark MLLib models typically expect all features to be grouped into a single column instead of having one column per feature, we can use Spark's `VectorAssembler` to group our features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_player_rating: float (nullable = true)\n",
      " |-- average_attendance: float (nullable = true)\n",
      " |-- sum_player_rating: float (nullable = true)\n",
      " |-- sum_position: float (nullable = true)\n",
      " |-- sum_player_worth: float (nullable = true)\n",
      " |-- average_player_age: float (nullable = true)\n",
      " |-- team_position: long (nullable = true)\n",
      " |-- average_player_worth: float (nullable = true)\n",
      " |-- team_budget: float (nullable = true)\n",
      " |-- average_position: float (nullable = true)\n",
      " |-- sum_player_age: float (nullable = true)\n",
      " |-- sum_attendance: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformedDf: org.apache.spark.sql.DataFrame = [team_position: bigint, features: vector]\n"
     ]
    }
   ],
   "source": [
    "val transformedDf = new VectorAssembler().\n",
    "  setInputCols(Array( \"average_player_rating\",\"average_attendance\", \"sum_player_rating\", \n",
    "                     \"sum_position\", \"sum_player_worth\", \"average_player_age\", \"average_player_worth\",\n",
    "                    \"team_budget\", \"average_position\", \"sum_player_age\", \"sum_attendance\")).\n",
    "  setOutputCol(\"features\").\n",
    "  transform(dataset_df).\n",
    "    drop(\"average_player_rating\").\n",
    "    drop(\"average_attendance\").\n",
    "    drop(\"sum_player_rating\").\n",
    "    drop(\"sum_player_worth\").\n",
    "    drop(\"average_player_age\").\n",
    "    drop(\"average_player_worth\").\n",
    "    drop(\"team_budget\").\n",
    "    drop(\"average_position\").\n",
    "    drop(\"sum_player_age\").\n",
    "    drop(\"sum_attendance\").\n",
    "    drop(\"sum_position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- team_position: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformedDf.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Model Using Spark MLLib\n",
    "\n",
    "We will use a linear regression model. In this tutorial we work with so little data that using a larger model does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: org.apache.spark.ml.regression.LinearRegression = linReg_8386df8afedf\n"
     ]
    }
   ],
   "source": [
    "val lr = new LinearRegression().\n",
    "    setLabelCol(\"team_position\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setMaxIter(NUM_ITER).\n",
    "    setRegParam(REG_LAMBDA_PARAM).\n",
    "    setElasticNetParam(ELASTIC_REG_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model using The Parsed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_8386df8afedf\n"
     ]
    }
   ],
   "source": [
    "val lrModel = lr.fit(transformedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------------+\n",
      "|            features|team_position|        prediction|\n",
      "+--------------------+-------------+------------------+\n",
      "|[240.305725097656...|           31| 33.70558553749568|\n",
      "|[240.393020629882...|           34| 34.79688811239048|\n",
      "|[297.929351806640...|           28|28.590203155619697|\n",
      "|[322.697967529296...|           26|29.921433049368147|\n",
      "|[297.791961669921...|           27|31.646190793603715|\n",
      "|[160.234771728515...|           44| 39.71408244897785|\n",
      "|[602.1064453125,9...|           12| 13.17064619555105|\n",
      "|[401.686798095703...|           22|25.747417689822896|\n",
      "|[178.677520751953...|           47|46.464197846092816|\n",
      "|[7191.86328125,92...|            1|1.0090139372748013|\n",
      "|[589.413146972656...|           13| 9.427614974389698|\n",
      "|[1311.23840332031...|            6|  5.46625465494315|\n",
      "|[502.241394042968...|           16|18.323688419895817|\n",
      "|[2814.01806640625...|            3|  4.73758303607206|\n",
      "|[372.345794677734...|           20|17.566396134917298|\n",
      "|[226.293045043945...|           40|37.123184809082744|\n",
      "|[150.963272094726...|           48|48.322429007238924|\n",
      "|[1654.07312011718...|            5| 6.601212823267879|\n",
      "|[401.167175292968...|           19|23.696084798498788|\n",
      "|[218.279800415039...|           41|39.094815641764555|\n",
      "+--------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// print the output column and the input column and the truth label\n",
    "lrModel.transform(transformedDf).select(\"features\", \"team_position\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@1cdfa82f\n",
      "numIterations: 56\n",
      "objectiveHistory: [0.5,0.4112410289397031,0.12518994433439817,0.08777706436856189,0.03913658284734977,0.03902366004873292,0.03902163129558643,0.03901820891749733,0.03899877146854554,0.038996622590289305,0.038986570501617614,0.0389851971409976,0.038985172369978296,0.03898515439811421,0.03898512325511451,0.03898510959220512,0.038985069161691754,0.038985041128822284,0.03898500402608199,0.038984965224531244,0.03898488771327016,0.03898469837573477,0.038983155982845144,0.03898302661967812,0.038981810415349566,0.038981644705064517,0.03898063375157004,0.03898005548848621,0.03897885683980405,0.03897755337185462,0.03897735366403644,0.03897705829013814,0.038977003369343836,0.038976960084926085,0.038976909759630335,0.03897688838839456,0.038976872666544066,0.03897681862521917,0.038976773319025576,0.038976764382013446,0.038976761236793354,0.038976759514787136,0.0389767582422512,0.038976757866287896,0.0389767576845524,0.03897675761791368,0.038976757596554376,0.03897675758512335,0.03897675758223342,0.03897675758024393,0.038976757579727725,0.038976757579506416,0.03897675757940564,0.03897675757933266,0.03897675757931047,0.038976757579303245]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -2.705585537495679|\n",
      "| -0.7968881123904765|\n",
      "| -0.5902031556196974|\n",
      "| -3.9214330493681473|\n",
      "|  -4.646190793603715|\n",
      "|   4.285917551022152|\n",
      "| -1.1706461955510505|\n",
      "|  -3.747417689822896|\n",
      "|  0.5358021539071842|\n",
      "|-0.00901393727480...|\n",
      "|   3.572385025610302|\n",
      "|  0.5337453450568503|\n",
      "| -2.3236884198958165|\n",
      "| -1.7375830360720599|\n",
      "|   2.433603865082702|\n",
      "|   2.876815190917256|\n",
      "|-0.32242900723892376|\n",
      "|  -1.601212823267879|\n",
      "|  -4.696084798498788|\n",
      "|   1.905184358235445|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 3.0024268807083074\n",
      "r2: 0.9567127626698688\n"
     ]
    }
   ],
   "source": [
    "val trainingSummary = lrModel.summary\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n",
    "trainingSummary.residuals.show()\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}