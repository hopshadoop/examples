{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with S3 and the Feature Store\n",
    "\n",
    "This tutorial notebook will help you get started with working with the Hopsworks feature store and S3.\n",
    "\n",
    "To execute this tutorial, you can use the sample data from [here](./data/Sacramentorealestatetransactions.csv) - and place it in a S3 bucket.\n",
    "\n",
    "Before starting with the execution, you should also create a S3 storage connector pointing to the bucket where you uploaded the data. You can follow the [Hopsworks documentation](https://hopsworks.readthedocs.io/en/latest/featurestore/featurestore.html#configuring-storage-connectors-for-the-feature-store) to see how you can create the storage connector from the feature store UI.\n",
    "\n",
    "The tutorial is divided in 3 parts: \n",
    "* [Import already feature engineered data from S3](#already_eng)\n",
    "* [Import raw data, do feature engineering and create a feature group](#raw)\n",
    "* [Export training dataset to S3](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import already feature engineered data from S3<a name=\"already_eng\"></a>\n",
    "\n",
    "In this section we are going to assume that the feature engineering process has already happended outside Hopsworks. In other words, the data in S3 is already feature engineered and we only want to import it into the feature store to be made available to data scientistis.\n",
    "\n",
    "To do that we can use the `featurestore` module of the hops python library. The Hops python library is already available in the environment and you can simply import it. You can find the documentation of the library [here](http://hops-py.logicalclocks.com/hops.html#module-hops.featurestore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>3</td><td>application_1580897737677_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-22-206.eu-north-1.compute.internal:8088/proxy/application_1580897737677_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-22-206.eu-north-1.compute.internal:8042/node/containerlogs/container_e01_1580897737677_0005_01_000001/demo_featurestore_admin000__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/demo_featurestore_admin000/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])"
     ]
    }
   ],
   "source": [
    "from hops import featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the feature data into the feature store we are going to use the following method: `featurestore.import_featuregroup_s3`. \n",
    "\n",
    "I called my connector `house-bucket` and I located the file in the `fg` subdirectory. The sample data is in CSV format. The method will infer the schema and the feature names from the file itself. In this case, the first line of the `csv` file contains the feature names.\n",
    "\n",
    "We are going to store this feature group in the feature store of the project we are currently working in, and it is going to be the first version of the feature group. \n",
    "\n",
    "The call below will also compute statistics which will be available from the Hopsworks UI or through the `get_featuregroup_statistics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : sacramento_houses_raw, version: 1\n",
      "computing feature correlation for: sacramento_houses_raw, version: 1\n",
      "computing feature histograms for: sacramento_houses_raw, version: 1\n",
      "computing cluster analysis for: sacramento_houses_raw, version: 1\n",
      "Registering feature metadata...\n",
      "Registering feature metadata... [COMPLETE]\n",
      "Writing feature data to offline feature group (Hive)...\n",
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "Writing feature data to offline feature group (Hive)... [COMPLETE]\n",
      "Feature group created successfully\n",
      "Feature group imported successfully"
     ]
    }
   ],
   "source": [
    "featurestore.import_featuregroup_s3(\"house-bucket\", \"fg\", \"sacramento_houses_raw\", \n",
    "                                    description=\"House sale transactions in Sacramento\",\n",
    "                                    featurestore=featurestore.project_featurestore(),\n",
    "                                    featuregroup_version=1,\n",
    "                                    data_format=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the feature store UI you should now be able to see that the feature group has been created, browse its schema and statistics. You can now use it to [build training datasets](#training).\n",
    "\n",
    "## Import raw data, do feature engineering and create a feature group<a name=\"raw\"></a>\n",
    "\n",
    "In the next session we are going to assume that the data in the S3 bucket is raw data that needs to be feature engineered before it can be used by data scientists to build models.\n",
    "\n",
    "Hopsworks feature store relies on Apache Spark to provide a scalabale framework for feature engineering processing. Hopsworks allows users to write both PySpark and Scala code. To know more about how to work with Spark code in Hopsworks you can have a look at [Apache Spark documentation](https://spark.apache.org/docs/latest/index.html) and at the [Hopsworks Jupyter documentation](https://hopsworks.readthedocs.io/en/1.1/user_guide/hopsworks/jupyter.html).\n",
    "\n",
    "For the sake of the tutorial, in this section we are going to read the CSV file in a dataframe, convert the `type` feature from a string to a categorical numerical feature and write the new feature group in the feature store.\n",
    "\n",
    "To instruct Spark to read from S3 we build the path to the file in the bucket. Please note the file system - `s3a://`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_data_path = os.path.join(\"s3a://\", featurestore.get_storage_connector(\"house-bucket\").bucket, 'fg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----+-----+----+-----+------+-----------+--------------------+-----+---------+-----------+\n",
      "|          street|      city|  zip|state|beds|baths|sq__ft|       type|           sale_date|price| latitude|  longitude|\n",
      "+----------------+----------+-----+-----+----+-----+------+-----------+--------------------+-----+---------+-----------+\n",
      "|    3526 HIGH ST|SACRAMENTO|95838|   CA|   2|    1|   836|Residential|Wed May 21 00:00:...|59222|38.631913|-121.434879|\n",
      "|     51 OMAHA CT|SACRAMENTO|95823|   CA|   3|    1|  1167|Residential|Wed May 21 00:00:...|68212|38.478902|-121.431028|\n",
      "|  2796 BRANCH ST|SACRAMENTO|95815|   CA|   2|    1|   796|Residential|Wed May 21 00:00:...|68880|38.618305|-121.443839|\n",
      "|2805 JANETTE WAY|SACRAMENTO|95815|   CA|   2|    1|   852|Residential|Wed May 21 00:00:...|69307|38.616835|-121.439146|\n",
      "| 6001 MCMAHON DR|SACRAMENTO|95824|   CA|   2|    1|   797|Residential|Wed May 21 00:00:...|81900| 38.51947|-121.435768|\n",
      "+----------------+----------+-----+-----+----+-----+------+-----------+--------------------+-----+---------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(raw_data_path)\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- baths: integer (nullable = true)\n",
      " |-- sq__ft: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- sale_date: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "index_table = raw_data.select(\"type\").distinct()\\\n",
    "                    .withColumn('type_class', monotonically_increasing_id())\n",
    "\n",
    "fg_data = raw_data.join(index_table, raw_data.type == index_table.type).drop(\"type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we are passing `fg_data` to the `create_featuregroup` method of the `featurestore` module. This is going to create a new feature group based on the schema of the dataframe, insert the data in the feature group itself and compute the statistics.\n",
    "\n",
    "At the end of the execution, the feature group will be available in the Feature Store UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing descriptive statistics for : sacramento_houses_fgeng, version: 1\n",
      "computing feature correlation for: sacramento_houses_fgeng, version: 1\n",
      "computing feature histograms for: sacramento_houses_fgeng, version: 1\n",
      "computing cluster analysis for: sacramento_houses_fgeng, version: 1\n",
      "Registering feature metadata...\n",
      "Registering feature metadata... [COMPLETE]\n",
      "Writing feature data to offline feature group (Hive)...\n",
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "Writing feature data to offline feature group (Hive)... [COMPLETE]\n",
      "Feature group created successfully"
     ]
    }
   ],
   "source": [
    "featurestore.create_featuregroup(fg_data, \"sacramento_houses_fgeng\",\n",
    "                                 featuregroup_version=1,\n",
    "                                 description=\"House sale transactions in Sacramento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training dataset to S3<a name=\"training\"></a>\n",
    "\n",
    "Once the feature groups have been created, you can join them together to build a training dataset to train a machine learning model.\n",
    "\n",
    "While Hopsworks provides [capabilities](https://hopsworks.readthedocs.io/en/latest/hopsml/index.html) to train and serve machine learning models, traning datasets can also be exported to S3 to be used from SageMaker or other ML systems in AWS.\n",
    "\n",
    "To export the training dataset we are going to use the `create_training_dataset` method which accepts a Spark dataframe.\n",
    "In this tutorial we are going to create a training dataset containing features from a single feature group. In real world use cases, feature can be extracted from different feature groups by joining them. You can have a look at [this notebook](../FeaturestoreTourPython.ipynb) for some examples.\n",
    "\n",
    "The data can be exported in multiple format, in this tutorial we are going to export it in CSV format, but tfrecords, parquet and other formats are available as well.\n",
    "\n",
    "As for feature groups, statistics are computed and recorded also for training datasets. They will be available in the Feature Store UI at the end of the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sql: use demo_featurestore_admin000_featurestore against offline feature store\n",
      "SQL string for the query created successfully\n",
      "Running sql: SELECT * FROM sacramento_houses_fgeng_1 against offline feature store\n",
      "computing descriptive statistics for : house_price_model_training_data, version: 1\n",
      "computing feature correlation for: house_price_model_training_data, version: 1\n",
      "computing feature histograms for: house_price_model_training_data, version: 1\n",
      "computing cluster analysis for: house_price_model_training_data, version: 1\n",
      "Training Dataset created successfully"
     ]
    }
   ],
   "source": [
    "td = featurestore.get_featuregroup(\"sacramento_houses_fgeng\", featuregroup_version=1)\n",
    "featurestore.create_training_dataset(td, \"house_price_model_training_data\", \n",
    "                                     data_format=\"csv\", sink=\"house-bucket\",\n",
    "                                     path=\"house_price_model_training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}