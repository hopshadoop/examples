{"cells":[{"cell_type":"markdown","source":["# Feature Store Quick Start\n\nThis notebook gives you a quick overview of how you can intergrate the Feature Store on Hopsworks with Databricks and S3. We'll go over four steps:\n\n1. Generate some sample data and store it on S3\n2. Do some feature engineering with Databricks and the data from S3\n3. **Save the engineered features to the Feature Store**\n4. **Select a group of the features from the Feature Store and create a training dataset of tf records stored on S3**\n\n**This requries configuring the Databricks cluster to be able to interact with Hopsworks Feature Store, see [Databricks Quick Start](https://docs.hopsworks.ai/integrations/databricks/configuration/).**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73d411fe-e2ad-4cf6-a949-8b1408087f9c"}}},{"cell_type":"markdown","source":["## Imports\n\nWe'll use numpy and pandas for data generation, pyspark for feature engineering, tensorflow and keras for model training, and the `hsfs` library to interact with the Hopsworks Feature Store."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a81aaf2-11ef-4045-bc9d-334f8364eb8a"}}},{"cell_type":"code","source":["import hsfs\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nsqlContext = SQLContext(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fd8b152-3fa8-4fc5-9c9e-58f33234ec03"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Connecting to the Feature Store"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c738692-4859-4a9e-92ad-f5e5b86ee7ac"}}},{"cell_type":"code","source":["# Connect to the feature store, see https://docs.hopsworks.ai/generated/api/connection_api/#connection_1 for more information\n# On AWS secrets can be stored also on the Secrets Manager or Parameter Store.\nconnection = hsfs.connection(\n  host=\"10.0.0.247\",\n  project=\"dataai\",\n  port=\"443\",\n  api_key_value=\"pduNksE2VMuSYdCY.K4AsDoxMBX5luisgb3pB7FimpEO7iyOuYvLPqWQcnXUs51RlBrLyAYXpKDoWH9Cm\",\n  hostname_verification=False\n)\n\nfs = connection.get_feature_store()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4447e6c9-1f34-493c-9b56-935b0d931cd9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Mounting an S3 bucket to Databricks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00b67492-299b-4a8e-bc4e-5caa453e460d"}}},{"cell_type":"code","source":["# Mount a bucket so that we can simulate a Datalake based on S3\n# This requires IAM roles to be set up for Databricks, see https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#access-s3-buckets-directly\nAWS_BUCKET_NAME = \"steffendatabricks\" # Ensure to replace with your bucket\nMOUNT_NAME = \"/mnt/demo_db_hsfs\"\ndbutils.fs.mount(\"s3a://%s\" % AWS_BUCKET_NAME, MOUNT_NAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f212fc99-c4ce-46ff-b825-164690407187"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Generate Sample Data\n\nLets generate two sample datasets and store them on S3:\n\n1. `houses_for_sale_data`:\n\n```bash\n+-------+--------+------------------+------------------+------------------+\n|area_id|house_id|       house_worth|         house_age|        house_size|\n+-------+--------+------------------+------------------+------------------+\n|      1|       0| 11678.15482418699|133.88670106643886|366.80067322738535|\n|      1|       1| 2290.436167500643|15994.969706808222|195.84014889823976|\n|      1|       2| 8380.774578431328|1994.8576926471007|1544.5164614303735|\n|      1|       3|11641.224696102923|23104.501275562343|1673.7222604337876|\n|      1|       4| 5382.089422436954| 13903.43637058141| 274.2912104765028|\n+-------+--------+------------------+------------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_id: long (nullable = true)\n |-- house_worth: double (nullable = true)\n |-- house_age: double (nullable = true)\n |-- house_size: double (nullable = true)\n```\n2. `houses_sold_data``\n```bash\n+-------+-----------------+-----------------+------------------+\n|area_id|house_purchase_id|number_of_bidders|   sold_for_amount|\n+-------+-----------------+-----------------+------------------+\n|      1|                0|                0| 70073.06059070028|\n|      1|                1|               15| 146.9198329740602|\n|      1|                2|                6|  594.802165433149|\n|      1|                3|               10| 77187.84123130841|\n|      1|                4|                1|110627.48922722359|\n+-------+-----------------+-----------------+------------------+\n\n |-- area_id: long (nullable = true)\n |-- house_purchase_id: long (nullable = true)\n |-- number_of_bidders: long (nullable = true)\n |-- sold_for_amount: double (nullable = true)\n```\n\nWe'll use this data for predicting what a house is sold for based on features about the **area** where the house is."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe1db1c6-0dc7-4c0c-9af0-1fb2ef9fd6e8"}}},{"cell_type":"markdown","source":["### Generation of `houses_for_sale_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a6c83c3-eaa6-4d95-bc58-4f40db4ee42a"}}},{"cell_type":"code","source":["area_ids = list(range(1,51))\nhouse_sizes = []\nhouse_worths = []\nhouse_ages = []\nhouse_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,100)):\n        house_sizes.append(abs(np.random.normal()*1000)/i)\n        house_worths.append(abs(np.random.normal()*10000)/i)\n        house_ages.append(abs(np.random.normal()*10000)/i)\n        house_area_ids.append(i)\nhouse_ids = list(range(len(house_area_ids)))\nhouses_for_sale_data  = pd.DataFrame({\n        'area_id':house_area_ids,\n        'house_id':house_ids,\n        'house_worth': house_worths,\n        'house_age': house_ages,\n        'house_size': house_sizes\n    })\nhouses_for_sale_data_spark_df = sqlContext.createDataFrame(houses_for_sale_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f17b70c-f9ff-4830-a142-6ac2071abdfb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_for_sale_data_spark_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"252379e7-d04b-4656-b678-0be33845dddd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_for_sale_data_spark_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61bb85ab-bcb0-44e7-837c-fab37f54a9af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_for_sale_data_spark_df.write.format(\"parquet\").save(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8218315-20d2-45c0-bc3e-948164e264e9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Generation of `houses_sold_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2e5b048-0cab-4878-b795-4c3f726d03a8"}}},{"cell_type":"code","source":["house_purchased_amounts = []\nhouse_purchases_bidders = []\nhouse_purchases_area_ids = []\nfor i in area_ids:\n    for j in list(range(1,1000)):\n        house_purchased_amounts.append(abs(np.random.exponential()*100000)/i)\n        house_purchases_bidders.append(int(abs(np.random.exponential()*10)/i))\n        house_purchases_area_ids.append(i)\nhouse_purchase_ids = list(range(len(house_purchases_bidders)))\nhouses_sold_data  = pd.DataFrame({\n        'area_id':house_purchases_area_ids,\n        'house_purchase_id':house_purchase_ids,\n        'number_of_bidders': house_purchases_bidders,\n        'sold_for_amount': house_purchased_amounts\n    })\nhouses_sold_data_spark_df = sqlContext.createDataFrame(houses_sold_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa9037dc-cf72-4e19-bea3-b0a1c8c2590e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_sold_data_spark_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"783a88b9-a1cd-46e7-a248-9ce7e6b20374"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_sold_data_spark_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a59a4790-6d7f-4f17-8e54-6585b1dcde5c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_sold_data_spark_df.write.format(\"parquet\").save(\"%s/houses_sold.parquet\" % MOUNT_NAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ceeac4-4a83-42b8-8f90-d3e14b5abc38"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Feature Engineering\n\nLets generate some aggregate features such as sum and averages from our datasets on S3. \n\n1. `houses_for_sale_features`:\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_house_age: double (nullable = true)\n |-- avg_house_size: double (nullable = true)\n |-- avg_house_worth: double (nullable = true)\n |-- sum_house_age: double (nullable = true)\n |-- sum_house_size: double (nullable = true)\n |-- sum_house_worth: double (nullable = true)\n```\n\n2. `houses_sold_features`\n\n```bash\n |-- area_id: long (nullable = true)\n |-- avg_num_bidders: double (nullable = true)\n |-- avg_sold_for: double (nullable = true)\n |-- sum_number_of_bidders: long (nullable = true)\n |-- sum_sold_for_amount: double (nullable = true)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cddb161-444f-4d8a-a87f-18227353f128"}}},{"cell_type":"code","source":["display(dbutils.fs.ls(MOUNT_NAME))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"720d1cbe-f285-4aa5-af5d-68a7c861455e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Generate Features From `houses_for_sale_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77dc56fd-9acc-407d-938b-e7f91f723770"}}},{"cell_type":"code","source":["houses_for_sale_data_spark_df = spark.read.parquet(\"%s/houses_for_sale.parquet\" % MOUNT_NAME)\nsum_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_for_sale_df = houses_for_sale_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_for_sale_df = sum_houses_for_sale_df.join(count_houses_for_sale_df, \"area_id\")\nsum_count_houses_for_sale_df = sum_count_houses_for_sale_df \\\n    .withColumnRenamed(\"sum(house_age)\", \"sum_house_age\") \\\n    .withColumnRenamed(\"sum(house_worth)\", \"sum_house_worth\") \\\n    .withColumnRenamed(\"sum(house_size)\", \"sum_house_size\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_house_for_sale(row):\n    avg_house_worth = row.sum_house_worth/float(row.num_rows)\n    avg_house_size = row.sum_house_size/float(row.num_rows)\n    avg_house_age = row.sum_house_age/float(row.num_rows)\n    return Row(\n        sum_house_worth=row.sum_house_worth, \n        sum_house_age=row.sum_house_age,\n        sum_house_size=row.sum_house_size,\n        area_id = row.area_id,\n        avg_house_worth = avg_house_worth,\n        avg_house_size = avg_house_size,\n        avg_house_age = avg_house_age\n       )\nhouses_for_sale_features_df = sum_count_houses_for_sale_df.rdd.map(\n    lambda row: compute_average_features_house_for_sale(row)\n).toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d54c292-d5d1-49b7-8f75-c765c289cced"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_for_sale_features_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21d1ff78-dddf-4609-a4dd-162b366cb8b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Generate Features from `houses_sold_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b6011cd-de2a-4767-a607-000a9bd3cfa0"}}},{"cell_type":"code","source":["houses_sold_data_spark_df = spark.read.parquet(\"%s/houses_sold.parquet\" % MOUNT_NAME)\nsum_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").sum()\ncount_houses_sold_df = houses_sold_data_spark_df.groupBy(\"area_id\").count()\nsum_count_houses_sold_df = sum_houses_sold_df.join(count_houses_sold_df, \"area_id\")\nsum_count_houses_sold_df = sum_count_houses_sold_df \\\n    .withColumnRenamed(\"sum(number_of_bidders)\", \"sum_number_of_bidders\") \\\n    .withColumnRenamed(\"sum(sold_for_amount)\", \"sum_sold_for_amount\") \\\n    .withColumnRenamed(\"count\", \"num_rows\")\ndef compute_average_features_houses_sold(row):\n    avg_num_bidders = row.sum_number_of_bidders/float(row.num_rows)\n    avg_sold_for = row.sum_sold_for_amount/float(row.num_rows)\n    return Row(\n        sum_number_of_bidders=row.sum_number_of_bidders, \n        sum_sold_for_amount=row.sum_sold_for_amount,\n        area_id = row.area_id,\n        avg_num_bidders = avg_num_bidders,\n        avg_sold_for = avg_sold_for\n       )\nhouses_sold_features_df = sum_count_houses_sold_df.rdd.map(\n    lambda row: compute_average_features_houses_sold(row)\n).toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61488a5a-b97a-43dd-bdac-2ba51b71fa77"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["houses_sold_features_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01f05441-7ae6-48ce-a6dc-717e7209969b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Save Features to the Feature Store\n\nThe Featue store has an abstraction of a **feature group** which is a set of features that naturally belong together that are computed using the same feature engineering job.\n\nLets create two feature groups:\n\n1. `houses_for_sale_featuregroup`\n\n2. `houses_sold_featuregroup`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20c108b3-d7a4-4c1d-8a20-27ddd3ccb9ff"}}},{"cell_type":"code","source":["# Refer to https://docs.hopsworks.ai/generated/api/feature_store_api/#create_feature_group for the different parameters for creating a feature group\nhouse_sale_fg = fs.create_feature_group(\"houses_for_sale_featuregroup\",\n                       version=1,\n                       description=\"aggregate features of houses for sale per area\",\n                       primary_key=['area_id'],\n                       online_enabled=False,\n                       time_travel_format=None,\n                       statistics_config={\"histograms\": True, \"correlations\": True})\n\nhouse_sale_fg.save(houses_for_sale_features_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"376d4287-78fa-44c3-bb11-49218919670b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["house_sold_fg = fs.create_feature_group(\"houses_sold_featuregroup\",\n                       version=1,\n                       description=\"aggregate features of sold houses per area\",\n                       primary_key=['area_id'],\n                       online_enabled=False,\n                       time_travel_format=None,\n                       statistics_config={\"histograms\": True, \"correlations\": True})\n\nhouse_sold_fg.save(houses_sold_features_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1bd059a-5427-45b6-95cf-a113c79e3d3c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create a Training Dataset\n\nThe feature store has an abstraction of a **training dataset**, which is a dataset with a set of features (potentially from many different feature groups) and labels (in case of supervised learning) stored in a ML Framework friendly format (CSV, Tfrecords, ...)\n\nLet's create a training dataset called *predict_house_sold_for_dataset* using the following features:\n\n- `avg_house_age`\n- `avg_house_size`\n- `avg_house_worth`\n- `avg_num_bidders`\n\nand the target variable is:\n\n- `avg_sold_for`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92c94a2a-34b9-4dc1-8397-125a60d3a07e"}}},{"cell_type":"code","source":["# Get the metadata for the storage connector from the Feature store.\ns3_bucket = fs.get_storage_connector(\"house_model_data\", \"S3\")\n\n# Join features and feature groups to create the training dataset\nfeature_query = house_sale_fg.select([\"avg_house_age\", \"avg_house_size\", \"avg_house_worth\"])\\\n                            .join(house_sold_fg.select([\"avg_num_bidders\", \"avg_sold_for\"]))\n  \n# Create the training dataset metadata\ntd = fs.create_training_dataset(name=\"predict_house_sold_for_dataset_two\",\n                           version=2,\n                           data_format=\"csv\",\n                           label=['avg_sold_for'],\n                           storage_connector=s3_bucket,\n                           statistics_config={\"histograms\": True, \"correlations\": True})\n\n# Save the training dataset\ntd.save(feature_query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1642693c-acaf-410e-a3e6-91eb3ce40f33"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"FeatureStoreQuickStartDatabricks","dashboards":[],"language":"python","widgets":{},"notebookOrigID":37084615428578}},"nbformat":4,"nbformat_minor":0}
