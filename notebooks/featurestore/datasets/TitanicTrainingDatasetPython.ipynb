{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset for the Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the Titanic dataset to be used with the feature store.\n",
    "\n",
    "The Titanic dataset contains information about the passengers of the famous Titanic ship. The training and test data come in form of two CSV files, which can be downloaded from the Titanic Competition page on [Kaggle](https://www.kaggle.com/c/titanic/data).\n",
    "\n",
    "Download the `train.csv` and `test.csv` files, and upload them to the `Resources` folder of your Hopsworks Project. If you prefer doing things using GUIs, then you can find the `Resources` by opening the **Data Sets** tab on the left menu bar.\n",
    "\n",
    "Once you have the two files uploaded on the `Resources` folder, you can proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>22</td><td>application_1614082217334_0028</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8088/proxy/application_1614082217334_0028/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_1614082217334_0028_01_000001/test2__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "from pyspark.sql import functions as F\n",
    "import hsfs\n",
    "\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by reading the training data into a Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/titanic-train.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do some simple preprocessing. Rather than registering the whole dataset with the Feature Store, we just select a few of the columns, and cast all columns to `int`. Since the values of the `sex` column are either `male` or `female`, we also convert them to `0` or `1`, respectively. We also fill the missing values of the `age` column with `30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple preprocessing:\n",
    "#     1 - selecting a few of the columns\n",
    "#     2 - Filling the missing 'age' values with 30\n",
    "#     3 - changing sex to 0 or 1\n",
    "#     4 - casting all columns to int\n",
    "\n",
    "clean_train_df = training_csv.select('survived', 'pclass', 'sex', 'fare', 'age', 'sibsp', 'parch') \\\n",
    "                    .fillna({'age': 30}) \\\n",
    "                    .withColumn('sex',\n",
    "                        F.when(F.col('sex')=='male', 0)\n",
    "                        .otherwise(1))\\\n",
    "                    .withColumn('survived',\n",
    "                               F.col('survived').cast('int')) \\\n",
    "                    .withColumn('pclass',\n",
    "                               F.col('pclass').cast('int')) \\\n",
    "                    .withColumn('fare',\n",
    "                                F.col('fare').cast('int')) \\\n",
    "                    .withColumn('age',\n",
    "                               F.col('age').cast('int')) \\\n",
    "                    .withColumn('sibsp',\n",
    "                               F.col('sibsp').cast('int')) \\\n",
    "                    .withColumn('parch',\n",
    "                               F.col('parch').cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to create a metadata object for the feature group, in order to materialize it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the metadata object\n",
    "\n",
    "titanic_all_fg_meta = fs.create_feature_group(name=\"titanic_all_features\",\n",
    "                                       version=1,\n",
    "                                       description=\"Titanic training features\",\n",
    "                                       time_travel_format=None,\n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the metadata object, the next step would be to create a *feature group*, and to register it with the Project's Feature Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fc17f94e110>"
     ]
    }
   ],
   "source": [
    "titanic_all_fg_meta.save(clean_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a *training dataset* from the feature group. This is a very simple task using the Feature Store API. You can provide a name, and the data format for the dataset. For now, let's stick with `tfrecord`, TensorFlow's own file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fc17f4c7f10>"
     ]
    }
   ],
   "source": [
    "# create training dataset\n",
    "\n",
    "titanic_all_fg = fs.get_feature_group('titanic_all_features', version=1)\n",
    "\n",
    "query = titanic_all_fg.select_all()\n",
    "\n",
    "td = fs.create_training_dataset(name=\"titanic_train_dataset\",\n",
    "                               description=\"Titanic training dataset with all features\",\n",
    "                               data_format=\"tfrecord\",\n",
    "                               version=1)\n",
    "\n",
    "td.save(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! you can now use the titanic training data in your Projects!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}