{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower Classification and Serving Using SkLearn, HopsML, and the Hopsworks Feature Store\n",
    "\n",
    "In this notebook we will, \n",
    "\n",
    "1. Load the Iris Flower dataset from HopsFS\n",
    "2. Do feature engineering on the dataset\n",
    "3. Save the features to the feature store\n",
    "4. Read the feature data from the feature store\n",
    "5. Train a KNN Model using SkLearn\n",
    "6. Save the trained model to HopsFS\n",
    "7. Launch a serving instance to serve the trained model\n",
    "8. Send some prediction requests to the served model\n",
    "9. Monitor the predictions through Kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from hops import kafka, hdfs, featurestore, serving, model, tls\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = hdfs.project_path()\n",
    "iris_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", True).load(\n",
    "    project_path + \"TourData/iris/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature  Engineering\n",
    "\n",
    "The dataset is already quite well prepared, the only thing we need to for feature engineering is to convert the `variety` column to numeric and save a lookup table so that we later on can convert the numeric representation back to the categorical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = StringIndexer(inputCol=\"variety\", outputCol=\"label\")\n",
    "fit_model = encoder.fit(iris_df)\n",
    "iris_df1 = fit_model.transform(iris_df)\n",
    "lookup_df = iris_df1.select([\"variety\", \"label\"]).distinct()\n",
    "iris_df2 = iris_df1.drop(\"variety\")\n",
    "iris_df3 = iris_df2.withColumn(\"label\", iris_df2[\"label\"].cast(IntegerType()))\n",
    "iris_df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Features to the Feature Store\n",
    "\n",
    "We can save two feature groups (hive tables), one called `iris_features` that contains the iris features and the corresponding numeric label, and another feature group called `iris_labels_lookup` for converting the numeric iris label back to categorical.\n",
    "\n",
    "**Note**: To be able to run the feature store code, you first have to enable the Feature Store Service in your project. To do this, go to the \"Settings\" tab in your project, select the feature store service and click \"Save\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(iris_df3, \"iris_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore.create_featuregroup(lookup_df, \"iris_labels_lookup\", feature_correlation=False, \n",
    "                                 feature_histograms=False, cluster_analysis=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Iris Training Dataset from the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = featurestore.get_featuregroup(\"iris_features\", dataframe_type=\"pandas\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y_df = df[[\"label\"]]\n",
    "X = x_df.values\n",
    "y = y_df.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a KNN Model using the Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = random.randint(3, 30)\n",
    "iris_knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "iris_knn.fit(X, y)\n",
    "y_pred = iris_knn.predict(X)\n",
    "acc = accuracy_score(y, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Trained Model to HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(iris_knn, \"iris_knn.pkl\")\n",
    "hdfs.mkdir(\"Resources/sklearn_model\")\n",
    "hdfs.copy_to_hdfs(\"iris_knn.pkl\", \"Resources/sklearn_model\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve the Trained Model\n",
    "\n",
    "To serve a SkLearn Model, write a python script that downloads the HDFS model in the constructor and saves it as a class variable and then implements the `Predict` class and the methods `predict`, `classify` and `regress`, like this:\n",
    "\n",
    "```python\n",
    "from sklearn.externals import joblib\n",
    "from hops import hdfs\n",
    "import os\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model from HDFS\"\"\"\n",
    "        self.model_path = \"Models/iris_knn.pkl\"\n",
    "        print(\"Copying SKLearn model from HDFS to local directory\")\n",
    "        hdfs.copy_to_local(self.model_path)\n",
    "        print(\"Reading local SkLearn model for serving\")\n",
    "        self.model = joblib.load(\"./iris_knn.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        return self.model.predict(inputs).tolist() # Numpy Arrays are not JSON serializable\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        \"\"\" Serves a classification request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "\n",
    "    def regress(self, inputs):\n",
    "        \"\"\" Serves a regression request using a trained model\"\"\"\n",
    "        return \"not implemented\"\n",
    "```\n",
    "\n",
    "Then upload this python script to some folder in your project and go to the \"Model Serving\" service in Hopsworks:\n",
    "\n",
    "![sklearn_serving1.png](./../../images/sklearn_serving1.png)\n",
    "\n",
    "Then click on \"create serving\" and configure your serving:\n",
    "\n",
    "![sklearn_serving2.png](./../../images/sklearn_serving2.png)\n",
    "\n",
    "Once the serving is created, you can start it and view information like server-logs and which kafka topic it is logging inference requests to.\n",
    "\n",
    "![sklearn_serving3.png](./../../images/sklearn_serving3.png)\n",
    "\n",
    "It is a best-practice to put the script together with the trained model, below is the code for exporting the script from `Jupyter/Serving/sklearn/iris_flower_classifier.py` to `Models/irisFlowerClassifier/1/iris_flower_classifier.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = \"Jupyter/Serving/sklearn/iris_flower_classifier.py\"\n",
    "hdfs.cp(\"Jupyter/End_To_End_Pipeline/sklearn/iris_flower_classifier.py\", \"Resources/sklearn_model/iris_flower_classifier.py\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Trained Model to the Models Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IrisFlowerClassifier\"\n",
    "model.export(\"Resources/sklearn_model\", MODEL_NAME, metrics={'accuracy': acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in hdfs.ls(\"Models/\" + MODEL_NAME, recursive=True):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model Repository for best IrisFlowerClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_METRIC=\"accuracy\"\n",
    "from hops.model import Metric\n",
    "\n",
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)\n",
    "\n",
    "print('Model name: ' + best_model['name'])\n",
    "print('Model version: ' + str(best_model['version']))\n",
    "print(best_model['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the files have been exported to the model directory, we can create a serving instance that points to the model files using `serving.create_or_update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = \"Models/\" + MODEL_NAME + \"/\" + str(best_model['version']) + \"/iris_flower_classifier.py\"\n",
    "serving.create_or_update(script_path, MODEL_NAME, model_version=best_model['version'], serving_type=\"SKLEARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the serving have been created, you can find it in the Hopsworks UI by going to the \"Model Serving\" tab. You can also use the python module to query the Hopsworks REST API about information on the existing servings using methods like: \n",
    "\n",
    "- `get_servings()`\n",
    "- `get_serving_id(serving_name)`\n",
    "- `get_serving_artifact_path(serving_name)`\n",
    "- `get_serving_type(serving_name)`\n",
    "- `get_serving_version(serving_name)`\n",
    "- `get_serving_kafka_topic(serving_name)`\n",
    "- `get_serving_status(serving_name)`\n",
    "- `exist(serving_name)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in serving.get_all():\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_id(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_artifact_path(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_type(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_version(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_kafka_topic(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.get_status(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start/stop the serving instance either from the Hopsworks UI or from the python/REST API as demonstrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down currently running serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if serving.get_status(MODEL_NAME) == \"Running\":\n",
    "    serving.stop(MODEL_NAME)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start new serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.start(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until serving is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while serving.get_status(MODEL_NAME) != \"Running\":\n",
    "    time.sleep(5) # Let the serving startup correctly\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Prediction Requests to the Served Model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = serving.get_kafka_topic(MODEL_NAME)\n",
    "NUM_FEATURES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making inference requests you can use the utility method `serving.make_inference_request`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    data = {\"inputs\" : [[random.uniform(1, 8) for i in range(NUM_FEATURES)]]}\n",
    "    response = serving.make_inference_request(MODEL_NAME, data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Prediction Requests and Responses using Kafka\n",
    "\n",
    "All prediction requestst are automatically logged to Kafka which means that you can keep track for yourr model's performance and its predictions in a scalable manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup Kafka Consumer and Subscribe to the Topic containing the Inference Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = kafka.get_kafka_default_config()\n",
    "config['default.topic.config'] = {'auto.offset.reset': 'earliest'}\n",
    "consumer = Consumer(config)\n",
    "topics = [TOPIC_NAME]\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Kafka Avro Schema From Hopsworks and setup an Avro Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = kafka.get_schema(TOPIC_NAME)\n",
    "avro_schema = kafka.convert_json_schema_to_avro(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Lookup Table from the Feature Store for Converting Numerical Labels to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_labels_lookup_df = featurestore.get_featuregroup(\"iris_labels_lookup\", dataframe_type=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read 10 Messages from the Kafka Topic, parse them with the Avro Schema and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    msg = consumer.poll(timeout=1.0)\n",
    "    if msg is not None:\n",
    "        value = msg.value()\n",
    "        try:\n",
    "            event_dict = kafka.parse_avro_msg(value, avro_schema)\n",
    "            prediction = json.loads(event_dict[\"inferenceResponse\"])[\"predictions\"][0]\n",
    "            prediction_label = iris_labels_lookup_df.loc[iris_labels_lookup_df['label'] == prediction, \n",
    "                                                         'variety'].iloc[0]\n",
    "            print(\"serving: {}, version: {}, timestamp: {},\"\\\n",
    "                  \"\\nrequest: {},\\nprediction:{}, prediction_label:{}, http_response_code: {},\"\\\n",
    "                  \" serving_type: {}\\n\".format(\n",
    "                       event_dict[\"modelName\"],\n",
    "                       event_dict[\"modelVersion\"],\n",
    "                       event_dict[\"requestTimestamp\"],\n",
    "                       event_dict[\"inferenceRequest\"],\n",
    "                       prediction,\n",
    "                       prediction_label,\n",
    "                       event_dict[\"responseHttpCode\"],\n",
    "                       event_dict[\"servingType\"]\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(\"A message was read but there was an error parsing it\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"timeout.. no more messages to read from topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}