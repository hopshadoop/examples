{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Inference (Batch) in PySpark Example\"\n",
    "date: 2021-02-24\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Batch Inference on HopsFS\n",
    "\n",
    "To run this notebook you must first install the following libraries in your project's conda environment (in addition to the base libraries):\n",
    "\n",
    "- Pillow\n",
    "- Matplotlib\n",
    "\n",
    "Moreover, the notebook assumes that you have access to the ImageNet dataset, this can either be uploaded to your project or shared from another project.\n",
    "\n",
    "You also need access to an internet connection so that the pre-trained model can be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from hops import experiment\n",
    "from hops import tensorboard\n",
    "from hops import featurestore\n",
    "from hops import hdfs\n",
    "from hops import util\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.models\n",
    "import types\n",
    "import tempfile\n",
    "from pyspark.sql import DataFrame, Row\n",
    "import pydoop.hdfs as pydoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pydoop.hdfs as pydoop\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT =224\n",
    "WIDTH = 224\n",
    "BATCH_SIZE = 100\n",
    "CHANNELS = 3\n",
    "INPUT_SHAPE = 12288\n",
    "NUM_CLASSES = 1000\n",
    "NUM_PARALLEL_CALLS = 8\n",
    "SAMPLE_IMAGE_DIR = pydoop.path.abspath(hdfs.project_path(\"labs\") + \"/imagenet_2016/ILSVRC2016_CLS-LOC/ILSVRC/Data/CLS-LOC/train/n03617480/\")\n",
    "SAMPLE_IMAGE_NAME = \"n03617480_28686.JPEG\"\n",
    "SAMPLE_IMAGE_PATH = SAMPLE_IMAGE_DIR + SAMPLE_IMAGE_NAME\n",
    "MODEL_NAME = \"resnet_imagenet.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "HEIGHT =224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "INPUT_SHAPE = 12288\n",
    "NUM_CLASSES = 1000\n",
    "BATCH_SIZE = 100\n",
    "NUM_PARALLEL_CALLS = 8\n",
    "SAMPLE_IMAGE_DIR = pydoop.path.abspath(hdfs.project_path(\"labs\") + \"/imagenet_2016/ILSVRC2016_CLS-LOC/ILSVRC/Data/CLS-LOC/train/n03617480/\")\n",
    "SAMPLE_IMAGE_NAME = \"n03617480_28686.JPEG\"\n",
    "SAMPLE_IMAGE_PATH = SAMPLE_IMAGE_DIR + SAMPLE_IMAGE_NAME\n",
    "MODEL_NAME = \"resnet_imagenet.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained ResNet50 Model Trained on ImageNet from Keras.applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    \"\"\"\n",
    "    Defines the model to use for image classification\n",
    "    \n",
    "    Returns:\n",
    "           ResNet50 model\n",
    "    \"\"\"\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "    model = ResNet50(weights=\"imagenet\", input_shape=(HEIGHT, WIDTH, CHANNELS), classes=NUM_CLASSES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pre-Trained model to HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    Save Pre-Trained ImageNet model to HDFS\n",
    "    \n",
    "    Args:\n",
    "         :model: the pre-trained model with weights trained on imagenet\n",
    "    Returns:\n",
    "          The HDFS path where it is saved\n",
    "    \"\"\"\n",
    "    # save trained model\n",
    "    model.save(MODEL_NAME) #Keras can't save to HDFS in the current version so save to local fs first\n",
    "    hdfs.copy_to_hdfs(MODEL_NAME, hdfs.project_path() + \"Resources/\", overwrite=True) # copy from local fs to hdfs\n",
    "    model_hdfs_path = hdfs.project_path() + \"Resources/\" + MODEL_NAME\n",
    "    return model_hdfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_model_path = save_model(define_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained model From HopsFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = hdfs.copy_to_local(hdfs_model_path, \"\", overwrite=True) + MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference on ImageNet using Spark + Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Images into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "df = spark.read.option(\"mode\", \"DROPMALFORMED\").format(\"image\").load(\"hdfs://10.0.104.196:8020/Projects/labs/imagenet_2016/ILSVRC2016_CLS-LOC/ILSVRC/Data/CLS-LOC/train/*/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.select(\"image.origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count how many images to perform batch inference on\n",
    "\n",
    "ImageNet2016 contains 1281167 images in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Inference using Spark Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(partition):\n",
    "    from hops import hdfs\n",
    "    try:\n",
    "        local_model_path = hdfs.copy_to_local(hdfs_model_path, \"\", overwrite=True) + MODEL_NAME\n",
    "        model = load_model(MODEL_NAME)\n",
    "    except:\n",
    "        print(\"could not copy model to local\")\n",
    "    for row in partition:\n",
    "        # some rows in imagenet are malformed so we skip those\n",
    "        try:\n",
    "            IMAGE_NAME = row.origin.rsplit('/', 1)[1]\n",
    "            local_sample_image_path = hdfs.copy_to_local(row.origin, \"\", overwrite=True) + IMAGE_NAME\n",
    "            img = image.load_img(local_sample_image_path, target_size=(HEIGHT, WIDTH))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            predictions = model.predict(x)\n",
    "            decoded_predictions = decode_predictions(predictions, top=3)\n",
    "            top_1_id = str(decoded_predictions[0][0][0])\n",
    "            top_1_label = str(decoded_predictions[0][0][1])\n",
    "            top_1_confidence = float(decoded_predictions[0][0][2])\n",
    "            top_2_id = str(decoded_predictions[0][1][0])\n",
    "            top_2_label = str(decoded_predictions[0][1][1])\n",
    "            top_2_confidence = float(decoded_predictions[0][1][2])\n",
    "            top_3_id = str(decoded_predictions[0][2][0])\n",
    "            top_3_label = str(decoded_predictions[0][2][1])\n",
    "            top_3_confidence = float(decoded_predictions[0][2][2])    \n",
    "            Example = Row(\"image_path\", \"top1_id\", \"top1_label\", \"top1_confidence\", \"top2_id\", \n",
    "                          \"top2_label\", \"top2_confidence\", \"top3_id\", \"top3_label\", \"top3_confidence\")\n",
    "            print(\"Labelled example successfully\")\n",
    "            yield Example(row.origin, top_1_id, top_1_label, top_1_confidence, top_2_id, top_2_label, top_2_confidence,\n",
    "                          top_3_id, top_3_label, top_3_confidence)\n",
    "        except:\n",
    "            print(\"Failed to label row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = df_filtered.limit(10000).repartition(util.num_executors()*3).rdd.mapPartitions(inference_fn).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.write.mode(\"overwrite\").parquet(hdfs.project_path() + \"Resources/labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Prediction against Image\n",
    "\n",
    "We can do a simple test to compare an image in the dataframe against a predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = labeled_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the HDFS path below to the %%local cell to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.top1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.top2_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.top3_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline \n",
    "with tf.Session() as sess:\n",
    "    sample_img = tf.image.decode_jpeg(tf.read_file(\"hdfs://10.0.104.196:8020/Projects/labs/imagenet_2016/ILSVRC2016_CLS-LOC/ILSVRC/Data/CLS-LOC/train/n04550184/n04550184_41732.JPEG\")).eval()\n",
    "    plt.imshow(sample_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
