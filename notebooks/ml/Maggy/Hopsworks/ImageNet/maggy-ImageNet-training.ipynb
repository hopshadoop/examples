{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "systematic-weekly",
   "metadata": {},
   "source": [
    "## Training ResNet50 with distributed training on Maggy\n",
    "In this notebook we will train a ResNet-50 model from scratch with data from ImageNet. Note that a PyTorch Dataset and DataLoader is employed which results in large I/O overhead and doesn't fully utilize the GPU capabilities. For higher throughput, see  `ImageNet_petastorm_training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "steady-henry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>184</td><td>application_1617699042861_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://moritzgpu-master-upgrade.internal.cloudapp.net:8088/proxy/application_1617699042861_0011/\">Link</a></td><td><a target=\"_blank\" href=\"http://moritzgpu-worker-2.internal.cloudapp.net:8042/node/containerlogs/container_e78_1617699042861_0011_01_000001/PyTorch_spark_minimal__realamac\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-relevance",
   "metadata": {},
   "source": [
    "### Creating the PyTorch Dataset\n",
    "The metadata of our dataset is stored in a .csv file located in the root folder. It contains the labels of each image and its source path. For convenience, we relabel the classes into integers. In the `__getitem__` function, we enable custom transformations after reading the image and its label. The advantage of defining our own dataset is that we have no problems performing I/O operations on our DFS, which would fail when simply calling `os.open()` (which is what PyTorch's predefined datasets do). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plastic-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transform=None, test_set=False):\n",
    "        super().__init__()\n",
    "        self.root = path\n",
    "        self.df = pd.read_csv(path + \"noisy_imagenette.csv\")\n",
    "        self.transform = transform\n",
    "        if test_set:\n",
    "            self.df = self.df[self.df.is_valid]\n",
    "        else:\n",
    "            self.df = self.df[self.df.is_valid == False]\n",
    "        self.df.drop([\"noisy_labels_\" + str(i) for i in [1, 5, 25,50]], axis=1, inplace=True)\n",
    "        self.labels = {\"n01440764\": 0,  # \"tench\" \n",
    "                       \"n02102040\": 1,  # \"English springer\"\n",
    "                       \"n02979186\": 2,  # \"cassette player\"\n",
    "                       \"n03000684\": 3,  # \"chain saw\"\n",
    "                       \"n03028079\": 4,  # \"church\"\n",
    "                       \"n03394916\": 5,  # \"French horn\"\n",
    "                       \"n03417042\": 6,  # \"garbage truck\"\n",
    "                       \"n03425413\": 7,  # \"gas pump\"\n",
    "                       \"n03445777\": 8,  # \"golf ball\"\n",
    "                       \"n03888257\": 9,  # \"parachute\"\n",
    "                      }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label = self.labels[row[\"noisy_labels_0\"]]\n",
    "        f = hdfs.open_file(self.root + row[\"path\"])\n",
    "        try:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "        finally:\n",
    "            f.close()\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sample = {\"image\": img, \"label\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = hdfs.project_path() + \"DataSets/ImageNet/imagenette/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-spouse",
   "metadata": {},
   "source": [
    "### Defining data transforms\n",
    "To increase the variety of our training samples, we employ data augmentation via torchvision's transforms API. For training images, in addition to resizing and randomly cropping, we also flip images horizontally. In the test set, we use a center crop and no flips to remove randomness. All images are normalized for numeric convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compound-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose(\n",
    "    [T.Resize(256),\n",
    "     T.RandomCrop(224),\n",
    "     T.RandomHorizontalFlip(),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_transform = T.Compose(\n",
    "    [T.Resize(256),\n",
    "     T.CenterCrop(224),\n",
    "     T.ToTensor(),\n",
    "     T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cloudy-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageNetDataset(path, transform=train_transform)\n",
    "test_ds = ImageNetDataset(path, transform=test_transform, test_set=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-rendering",
   "metadata": {},
   "source": [
    "### Defining the training function\n",
    "In order to use PyTorch with maggy, we need to define our training loop in a function. The function takes our module, its hyperparameters and both the train and test set as input. Note that the module should be a class that is instantiated in our training loop, since transferring the model weights at the beginning of the loop would result in a huge communicational overhead. Likewise, it is not advised to use datasets with large memory footprint over the function, but rather load it from the DFS when requested.\n",
    "Inside the training loop it is **mandatory for maggy** to use a torch DataLoader. Apart from these restrictions, you can freely implement your training loop as in normal PyTorch. Finally, we have to import all of the used libraries inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confidential-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import torch\n",
    "    import time\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    n_epochs = 3\n",
    "    batch_size = 64\n",
    "    lr = 0.1 * 2*batch_size/256\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = DataLoader(train_set, pin_memory=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_set, pin_memory=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    def eval_model(model, test_loader):\n",
    "        acc = 0\n",
    "        model.eval()\n",
    "        img_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate(test_loader):\n",
    "                img, label = data[\"image\"], data[\"label\"]\n",
    "                prediction = model(img)\n",
    "                acc += torch.sum(torch.argmax(prediction, dim=1) == label)\n",
    "                img_cnt += len(label)\n",
    "        acc = acc.detach()/float(img_cnt)\n",
    "        print(\"Test accuracy: {:.3f}\".format(acc))\n",
    "        print(\"-\"*20)\n",
    "        return acc\n",
    "\n",
    "    model.train()\n",
    "    t_0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"-\"*20 + \"\\nStarting new epoch\\n\")\n",
    "        t_start = time.time()\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            if idx%10 == 0:\n",
    "                print(f\"Working on batch {idx}.\")\n",
    "            img, label = data[\"image\"], data[\"label\"]\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(img)\n",
    "            output = loss_criterion(prediction, label.long())\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "        t_end = time.time()\n",
    "        print(\"Epoch training took {:.0f}s.\\n\".format(t_end-t_start))\n",
    "        acc = eval_model(model, test_loader)\n",
    "    t_1 = time.time()\n",
    "    minutes, seconds = divmod(t_1 - t_0, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    print(\"-\"*20 + \"\\nTotal training time: {:.0f}h {:.0f}m {:.0f}s.\".format(hours, minutes, seconds))\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-lexington",
   "metadata": {},
   "source": [
    "### Configuring maggy\n",
    "As a last step, we need to configure our maggy experiment. Here we pass our model class, our train and test dataset as well as the desired backend. Maggy supports either `torch` or `deepspeed`, with additional constraints on deepspeed. If using torch, you can employ the PyTorch version of the ZeRO optimizer and model sharding by changing the ZeRO levels in the config (either 1, 2 or 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "knowing-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "config = TorchDistributedConfig(name='ImageNet_training', module=models.resnet50, train_set=train_ds, test_set=test_ds, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-production",
   "metadata": {},
   "source": [
    "### Running the experiment\n",
    "Now that everything is configured, we are ready to run the experiment by calling the lagom function. You should be able to see the output of your workers in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beginning-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8369552830e24ad9a0c4aff6a1a81fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=1.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Awaiting worker reservations.\n",
      "1: Awaiting worker reservations.\n",
      "1: All executors registered: True\n",
      "1: Reservations complete, configuring PyTorch.\n",
      "1: Torch config is {'MASTER_ADDR': '10.0.0.4', 'MASTER_PORT': '50261', 'WORLD_SIZE': '2', 'RANK': '1', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: All executors registered: True\n",
      "0: Reservations complete, configuring PyTorch.\n",
      "0: Torch config is {'MASTER_ADDR': '10.0.0.4', 'MASTER_PORT': '50261', 'WORLD_SIZE': '2', 'RANK': '0', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: Starting distributed training.\n",
      "1: Starting distributed training.\n",
      "0: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "1: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "1: Working on batch 0.\n",
      "0: Working on batch 0.\n",
      "1: Working on batch 10.\n",
      "0: Working on batch 10.\n",
      "1: Working on batch 20.\n",
      "0: Working on batch 20.\n",
      "1: Working on batch 30.\n",
      "0: Working on batch 30.\n",
      "1: Working on batch 40.\n",
      "0: Working on batch 40.\n",
      "1: Working on batch 50.\n",
      "0: Working on batch 50.\n",
      "1: Working on batch 60.\n",
      "0: Working on batch 60.\n",
      "1: Working on batch 70.\n",
      "0: Working on batch 70.\n",
      "1: Epoch training took 443s.\n",
      "\n",
      "0: Epoch training took 443s.\n",
      "\n",
      "1: Test accuracy: 0.188\n",
      "1: --------------------\n",
      "1: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "1: Working on batch 0.\n",
      "0: Test accuracy: 0.180\n",
      "0: --------------------\n",
      "0: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "0: Working on batch 0.\n",
      "1: Working on batch 10.\n",
      "0: Working on batch 10.\n",
      "1: Working on batch 20.\n",
      "0: Working on batch 20.\n",
      "1: Working on batch 30.\n",
      "0: Working on batch 30.\n",
      "1: Working on batch 40.\n",
      "0: Working on batch 40.\n",
      "1: Working on batch 50.\n",
      "0: Working on batch 50.\n",
      "1: Working on batch 60.\n",
      "0: Working on batch 60.\n",
      "1: Working on batch 70.\n",
      "0: Working on batch 70.\n",
      "0: Epoch training took 436s.\n",
      "\n",
      "1: Epoch training took 470s.\n",
      "\n",
      "1: Test accuracy: 0.157\n",
      "1: --------------------\n",
      "1: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "1: Working on batch 0.\n",
      "0: Test accuracy: 0.144\n",
      "0: --------------------\n",
      "0: --------------------\n",
      "Starting new epoch\n",
      "\n",
      "0: Working on batch 0.\n",
      "1: Working on batch 10.\n",
      "0: Working on batch 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://10.0.0.7:8998/sessions/184/statements/8 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(train_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-market",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
