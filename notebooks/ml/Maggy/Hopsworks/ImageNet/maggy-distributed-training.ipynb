{
 "cells": [
  {
   "source": [
    "## Mixed precision training with Maggy\n",
    "From the Pascal line on, NVIDIA GPUs are equipped with so called Tensor Cores. These cores accelerate computations with half precision and can be used to significantly speed up training of neural networks without loss of accuracy. This notebook shows a brief example on how to train an MNIST classifier with mixed precision on Maggy. For more information about mixed precision, see [here](https://pytorch.org/docs/stable/notes/amp_examples.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a classifier CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Conv2d(1,10,5)\n",
    "        self.l2 = torch.nn.Conv2d(10,20,5)\n",
    "        self.l3 = torch.nn.Linear(20*20*20,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.softmax(self.l3(x.flatten(start_dim=1)), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training function\n",
    "As you can see from the example below, mixed precision in Maggy is distribution transparent and can be employed just as in your normal PyTorch code. Note however that you currently can not combine the `GradScaler` with ZeRO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import time    \n",
    "    import torch\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "    from maggy.core.patching import MaggyPetastormDataLoader\n",
    "\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    n_epochs = 11\n",
    "    batch_size = 64\n",
    "    lr_base =  0.01\n",
    "    \n",
    "    # Parameters as in https://arxiv.org/pdf/1706.02677.pdf\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_base, momentum=0.5)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = MaggyPetastormDataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader = MaggyPetastormDataLoader(test_set, batch_size=batch_size)\n",
    "        \n",
    "    def eval_model(model, test_loader):\n",
    "        acc = 0\n",
    "        model.eval()\n",
    "        img_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                img, label = data[\"image\"].float(), data[\"label\"].float()\n",
    "                with autocast():\n",
    "                    prediction = model(img)\n",
    "                acc += torch.sum(torch.argmax(prediction, dim=1) == label).detach()\n",
    "                img_cnt += len(label.detach())\n",
    "        acc = acc/float(img_cnt)\n",
    "        print(\"Test accuracy: {:.3f}\\n\".format(acc) + 20*\"-\")\n",
    "        return acc\n",
    "\n",
    "    # Initialize a gradient scaler to keep precision of small gradients.\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    t_0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            img, label = data[\"image\"].float(), data[\"label\"].long()\n",
    "            with autocast():\n",
    "                prediction = model(img)\n",
    "                loss = loss_criterion(prediction, label)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        if epoch % 10 == 0:\n",
    "            acc = eval_model(model, test_loader)\n",
    "    t_1 = time.time()\n",
    "    minutes, seconds = divmod(t_1 - t_0, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    print(\"-\"*20 + \"\\nTotal training time: {:.0f}h {:.0f}m {:.0f}s.\".format(hours, minutes, seconds))\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/train_set\"\n",
    "test_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/test_set\"\n",
    "print(hdfs.exists(train_ds), hdfs.exists(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the config\n",
    "For mixed precision the config stays exactly the same as usual. You can now start mp training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "config = TorchDistributedConfig(name='torch_mixed_precision', module=Classifier, train_set=train_ds, test_set=test_ds, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.lagom(train_fn, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
