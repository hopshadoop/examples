{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"Maggy Distributed Training with PyTorch and DeepSpeed ZeRO example\"\n",
    "date: 2021-05-03\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "optional-running",
   "metadata": {},
   "source": [
    "## MNIST training and DeepSpeed ZeRO\n",
    "Maggy enables you to train with Microsoft's DeepSpeed ZeRO optimizer. Since DeepSpeed does not follow the common PyTorch programming model, Maggy is unable to provide full distribution transparency to the user. This means that if you want to use DeepSpeed for your training, you will have to make small changes to your code. In this notebook, we will show you what exactly you have to change in order to make DeepSpeed run with Maggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorrect-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>189</td><td>application_1617699042861_0016</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://moritzgpu-master-upgrade.internal.cloudapp.net:8088/proxy/application_1617699042861_0016/\">Link</a></td><td><a target=\"_blank\" href=\"http://moritzgpu-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e78_1617699042861_0016_01_000001/PyTorch_spark_minimal__realamac\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-monitoring",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "First off, we have to define our model. Since DeepSpeed's ZeRO is meant to reduce the memory consumption of our model, we will use an unreasonably large CNN for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Conv2d(1,1000,3)\n",
    "        self.l2 = torch.nn.Conv2d(1000,3000,5)\n",
    "        self.l3 = torch.nn.Conv2d(3000,3000,5)\n",
    "        self.l4 = torch.nn.Linear(3000*18*18,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.softmax(self.l4(x.flatten(start_dim=1)), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-opening",
   "metadata": {},
   "source": [
    "### Adapting the training function\n",
    "There are a few minor changes that have to be done in order to train with DeepSpeed:\n",
    "- There is no need for an optimizer anymore. You can configure your optimizer later in the DeepSpeed config.\n",
    "- DeepSpeed's ZeRO _requires_ you to use FP16 training. Therefore, convert your data to half precision!\n",
    "- The backward call is not executed on the loss, but on the model (`model.backward(loss)` instead of `loss.backward()`).\n",
    "- The step call is not executed on the optimizer, but also on the model (`model.step()` instead of `optimizer.step()`).\n",
    "- As we have no optimizer anymore, there is also no need to call `optimizer.zero_grad()`.\n",
    "You do not have to worry about the implementation of these calls, Maggy configures your model at runtime to act as a DeepSpeed engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "planned-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import time\n",
    "    import torch\n",
    "        \n",
    "    from maggy.core.patching import MaggyPetastormDataLoader\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    batch_size = 4\n",
    "    lr_base = 0.1 * batch_size/256\n",
    "    \n",
    "    # Parameters as in https://arxiv.org/pdf/1706.02677.pdf\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = MaggyPetastormDataLoader(train_set, batch_size=batch_size)\n",
    "                            \n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        img, label = data[\"image\"].half(), data[\"label\"].half()\n",
    "        prediction = model(img)\n",
    "        loss = loss_criterion(prediction, label.long())\n",
    "        model.backward(loss)\n",
    "\n",
    "        m1 = torch.cuda.max_memory_allocated(0)\n",
    "        model.step()\n",
    "        m2 = torch.cuda.max_memory_allocated(0)\n",
    "        print(\"Optimizer pre: {}MB\\n Optimizer post: {}MB\".format(m1//1e6,m2//1e6))\n",
    "        print(f\"Finished batch {idx}\")\n",
    "    return float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True"
     ]
    }
   ],
   "source": [
    "train_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/train_set\"\n",
    "test_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/test_set\"\n",
    "print(hdfs.exists(train_ds), hdfs.exists(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-organization",
   "metadata": {},
   "source": [
    "### Configuring DeepSpeed\n",
    "In order to use DeepSpeed's ZeRO, the `deepspeed` backend has to be chosen. This backend also requires its own config. You can read a full specification of the possible settings [here](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entertaining-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "ds_config = {\"train_micro_batch_size_per_gpu\": 1,\n",
    " \"gradient_accumulation_steps\": 1,\n",
    " \"optimizer\": {\"type\": \"Adam\", \"params\": {\"lr\": 0.1}},\n",
    " \"fp16\": {\"enabled\": True},\n",
    " \"zero_optimization\": {\"stage\": 2},\n",
    "}\n",
    "\n",
    "config = TorchDistributedConfig(name='DS_ZeRO', module=CNN, train_set=train_ds, test_set=test_ds, backend=\"deepspeed\", deepspeed_config=ds_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-madagascar",
   "metadata": {},
   "source": [
    "### Starting the training\n",
    "You can now launch training with DS ZeRO. Note that the overhead of DeepSpeed is considerably larger than PyTorch's build in sharding, albeit also more efficient for a larger number of GPUs. DS will also jit compile components on the first run. If you want to compare memory efficiency with the default training, you can rewrite this notebook to work with standard PyTorch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "furnished-length",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3cdedb1b974eb5a0e31648cca4408a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=1.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Awaiting worker reservations.\n",
      "1: Awaiting worker reservations.\n",
      "1: All executors registered: True\n",
      "1: Reservations complete, configuring PyTorch.\n",
      "1: Torch config is {'MASTER_ADDR': '10.0.0.5', 'MASTER_PORT': '48985', 'WORLD_SIZE': '2', 'RANK': '1', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: All executors registered: True\n",
      "0: Reservations complete, configuring PyTorch.\n",
      "0: Torch config is {'MASTER_ADDR': '10.0.0.5', 'MASTER_PORT': '48985', 'WORLD_SIZE': '2', 'RANK': '0', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: Starting distributed training.\n",
      "1: Starting distributed training.\n",
      "0: Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "1: Using /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000004/.cache/torch_extensions as PyTorch extensions root...\n",
      "1: Creating extension directory /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000004/.cache/torch_extensions/utils...\n",
      "1: Emitting ninja build file /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000004/.cache/torch_extensions/utils/build.ninja...\n",
      "1: Building extension module utils...\n",
      "1: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "0: Using /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000002/.cache/torch_extensions as PyTorch extensions root...\n",
      "0: Creating extension directory /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000002/.cache/torch_extensions/utils...\n",
      "0: Emitting ninja build file /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000002/.cache/torch_extensions/utils/build.ninja...\n",
      "0: Building extension module utils...\n",
      "0: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "0: Loading extension module utils...\n",
      "0: Time to load utils op: 14.465313196182251 seconds\n",
      "0: Using /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000002/.cache/torch_extensions as PyTorch extensions root...\n",
      "0: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "0: Loading extension module utils...\n",
      "0: Time to load utils op: 0.0007085800170898438 seconds\n",
      "0: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "1: Loading extension module utils...\n",
      "1: Time to load utils op: 15.031829833984375 seconds\n",
      "1: Using /srv/hops/hopsdata/tmp/nm-local-dir/usercache/PyTorch_spark_minimal__realamac/appcache/application_1617699042861_0016/container_e78_1617699042861_0016_01_000004/.cache/torch_extensions as PyTorch extensions root...\n",
      "1: No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "1: Loading extension module utils...\n",
      "1: Time to load utils op: 0.0007691383361816406 seconds\n",
      "1: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 0\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 0\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 1\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 1\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 2\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 2\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 3\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 3\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 4\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 4\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 5\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 5\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 6\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 6\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 7\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 7\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 8\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 8\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 9\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 9\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 10\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 10\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 11\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 11\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 12\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 12\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 13\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 13\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 14\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 14\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 15\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 15\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 16\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 16\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 17\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 17\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 18\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 18\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 19\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 19\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 20\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 20\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 21\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 21\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 22\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 22\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 23\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 23\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 24\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 24\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 25\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 25\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 26\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 26\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 27\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 27\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 28\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 28\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 29\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 29\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 30\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 30\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 31\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 31\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 32\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 32\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 33\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 33\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 34\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 34\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 35\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 35\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 36\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 36\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 37\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 37\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 38\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 38\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 39\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 39\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 40\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 40\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 41\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 41\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 42\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 42\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 43\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 43\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 44\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 44\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 45\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 45\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 46\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 46\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 47\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 47\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 48\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 48\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 49\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 49\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 50\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 50\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 51\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 51\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 52\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 52\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 53\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 53\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 54\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 54\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 55\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 55\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 56\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 56\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 57\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 57\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 58\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 58\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 59\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 59\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 60\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 60\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 61\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 61\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 62\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 62\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 63\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 63\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 64\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 64\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 65\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 65\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 66\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 66\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 67\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 67\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 68\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 68\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 69\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 69\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 70\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 70\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 71\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 71\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 72\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 72\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 73\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 73\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 74\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 74\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 75\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 75\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 76\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 76\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 77\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 77\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 78\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 78\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 79\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 79\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 80\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 80\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 81\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 81\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 82\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 82\n",
      "0: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "0: Finished batch 83\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 83\n",
      "1: Optimizer pre: 4336.0MB\n",
      " Optimizer post: 4336.0MB\n",
      "1: Finished batch 84\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-1ca51e069556>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_cell_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'spark'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'result = experiment.lagom(train_fn, config)\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mrun_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2397\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuiltin_trap\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2398\u001B[0m                 \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmagic_arg_s\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcell\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2399\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2400\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2401\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<decorator-gen-116>\u001B[0m in \u001B[0;36mspark\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/IPython/core/magic.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f, *a, **k)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;31m# but it's overkill for just that one bit of state.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mmagic_deco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m         \u001B[0mcall\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    108\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_errors_are_fatal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     72\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mexceptions_to_handle\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_errors_are_fatal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/kernels/kernelmagics.py\u001B[0m in \u001B[0;36mspark\u001B[0;34m(self, line, cell, local_ns)\u001B[0m\n\u001B[1;32m    263\u001B[0m             \u001B[0mcoerce\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_coerce_value\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcoerce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute_spark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msamplemethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaxrows\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msamplefraction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoerce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001B[0m in \u001B[0;36mexecute_spark\u001B[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001B[0m\n\u001B[1;32m    132\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mDEBUG\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mipython_display\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwriteln\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Started heartbeating...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute_final\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_var\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplemethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxrows\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplefraction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msession_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoerce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m             \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001B[0m in \u001B[0;36mexecute_final\u001B[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mexecute_final\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcell\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_var\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplemethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxrows\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplefraction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msession_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoerce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 104\u001B[0;31m         \u001B[0;34m(\u001B[0m\u001B[0msuccess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmimetype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark_controller\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCommand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msession_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    105\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshutdown_session_on_spark_statement_errors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001B[0m in \u001B[0;36mrun_command\u001B[0;34m(self, command, client_name)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclient_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0msession_to_use\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_session_by_name_or_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclient_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcommand\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msession_to_use\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_sqlquery\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msqlquery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclient_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001B[0m in \u001B[0;36mexecute\u001B[0;34m(self, session)\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhttp_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_statement\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m             \u001B[0mstatement_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34mu'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_statement_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msession\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstatement_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001B[0m in \u001B[0;36m_get_statement_output\u001B[0;34m(self, session, statement_id)\u001B[0m\n\u001B[1;32m     74\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mstatus\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mFINAL_STATEMENT_STATUS\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m                 \u001B[0mprogress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstatement\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'progress'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m                 \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretries\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m                 \u001B[0mretries\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001B[0m in \u001B[0;36msleep\u001B[0;34m(self, retries)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    289\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretries\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 290\u001B[0;31m         \u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_policy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseconds_to_sleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretries\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    291\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    292\u001B[0m     \u001B[0;31m# This function will refresh the status and get the logs in a single call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(train_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-glory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}