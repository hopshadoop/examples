{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "title: \"Maggy Mixed precision training with PyTorch example\"\n",
    "date: 2021-05-03\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "guided-failure",
   "metadata": {},
   "source": [
    "## Mixed precision training with Maggy\n",
    "From the Pascal line on, NVIDIA GPUs are equipped with so called Tensor Cores. These cores accelerate computations with half precision and can be used to significantly speed up training of neural networks without loss of accuracy. This notebook shows a brief example on how to train an MNIST classifier with mixed precision on Maggy. For more information about mixed precision, see [here](https://pytorch.org/docs/stable/notes/amp_examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-trial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>197</td><td>application_1617699042861_0024</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://moritzgpu-master-upgrade.internal.cloudapp.net:8088/proxy/application_1617699042861_0024/\">Link</a></td><td><a target=\"_blank\" href=\"http://moritzgpu-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e78_1617699042861_0024_01_000001/PyTorch_spark_minimal__realamac\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-orbit",
   "metadata": {},
   "source": [
    "### Define a classifier CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cooperative-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Conv2d(1,10,5)\n",
    "        self.l2 = torch.nn.Conv2d(10,20,5)\n",
    "        self.l3 = torch.nn.Linear(20*20*20,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.softmax(self.l3(x.flatten(start_dim=1)), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-universe",
   "metadata": {},
   "source": [
    "### Define the training function\n",
    "As you can see from the example below, mixed precision in Maggy is distribution transparent and can be employed just as in your normal PyTorch code. Note however that you currently can not combine the `GradScaler` with ZeRO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "million-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import time    \n",
    "    import torch\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "    from maggy.core.patching import MaggyPetastormDataLoader\n",
    "\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    n_epochs = 11\n",
    "    batch_size = 64\n",
    "    lr_base =  0.01\n",
    "    \n",
    "    # Parameters as in https://arxiv.org/pdf/1706.02677.pdf\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr_base, momentum=0.5)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = MaggyPetastormDataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader = MaggyPetastormDataLoader(test_set, batch_size=batch_size)\n",
    "        \n",
    "    def eval_model(model, test_loader):\n",
    "        acc = 0\n",
    "        model.eval()\n",
    "        img_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                img, label = data[\"image\"].float(), data[\"label\"].float()\n",
    "                with autocast():\n",
    "                    prediction = model(img)\n",
    "                acc += torch.sum(torch.argmax(prediction, dim=1) == label).detach()\n",
    "                img_cnt += len(label.detach())\n",
    "        acc = acc/float(img_cnt)\n",
    "        print(\"Test accuracy: {:.3f}\\n\".format(acc) + 20*\"-\")\n",
    "        return acc\n",
    "\n",
    "    # Initialize a gradient scaler to keep precision of small gradients.\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    t_0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            img, label = data[\"image\"].float(), data[\"label\"].long()\n",
    "            with autocast():\n",
    "                prediction = model(img)\n",
    "                loss = loss_criterion(prediction, label)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        if epoch % 10 == 0:\n",
    "            acc = eval_model(model, test_loader)\n",
    "    t_1 = time.time()\n",
    "    minutes, seconds = divmod(t_1 - t_0, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    print(\"-\"*20 + \"\\nTotal training time: {:.0f}h {:.0f}m {:.0f}s.\".format(hours, minutes, seconds))\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "violent-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True"
     ]
    }
   ],
   "source": [
    "train_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/train_set\"\n",
    "test_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/test_set\"\n",
    "print(hdfs.exists(train_ds), hdfs.exists(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-metropolitan",
   "metadata": {},
   "source": [
    "### Defining the config\n",
    "For mixed precision the config stays exactly the same as usual. You can now start mp training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "labeled-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "config = TorchDistributedConfig(name='torch_mixed_precision', module=Classifier, train_set=train_ds, test_set=test_ds, backend=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bound-berry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e3e52bb55e4f169a62bd10009a8ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=1.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Awaiting worker reservations.\n",
      "0: Awaiting worker reservations.\n",
      "1: All executors registered: True\n",
      "1: Reservations complete, configuring PyTorch.\n",
      "1: Torch config is {'MASTER_ADDR': '10.0.0.5', 'MASTER_PORT': '40429', 'WORLD_SIZE': '2', 'RANK': '1', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: All executors registered: True\n",
      "0: Reservations complete, configuring PyTorch.\n",
      "0: Torch config is {'MASTER_ADDR': '10.0.0.5', 'MASTER_PORT': '40429', 'WORLD_SIZE': '2', 'RANK': '0', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "1: Starting distributed training.\n",
      "0: Starting distributed training.\n",
      "1: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "1: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/test_set\n",
      "0: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "0: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/test_set\n",
      "0: Test accuracy: 0.779\n",
      "--------------------\n",
      "1: Test accuracy: 0.770\n",
      "--------------------\n",
      "1: Test accuracy: 0.940\n",
      "--------------------\n",
      "0: Test accuracy: 0.941\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://10.0.0.7:8998/sessions/197/statements/21 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment.lagom(train_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}