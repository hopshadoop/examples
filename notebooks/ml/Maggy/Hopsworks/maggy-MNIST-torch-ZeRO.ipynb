{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reserved-visit",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Maggy Distributed Training with PyTorch's sharded optimize example\"\n",
    "date: 2021-05-03\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with PyTorch's sharded optimizer\n",
    "This notebook will show you how to train with PyTorch's ZeRO optimizer. There is only one change required in the config to make this work. Creating the model and the dataset is identical to previous notebooks and will not be commented further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "growing-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>194</td><td>application_1617699042861_0021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://moritzgpu-master-upgrade.internal.cloudapp.net:8088/proxy/application_1617699042861_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://moritzgpu-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e78_1617699042861_0021_01_000001/PyTorch_spark_minimal__realamac\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Conv2d(1,1000,3)\n",
    "        self.l2 = torch.nn.Conv2d(1000,3000,5)\n",
    "        self.l3 = torch.nn.Conv2d(3000,3000,5)\n",
    "        self.l4 = torch.nn.Linear(3000*18*18,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.softmax(self.l4(x.flatten(start_dim=1)), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-medicine",
   "metadata": {},
   "source": [
    "### Writing the training function\n",
    "Unlike ZeRO on DeepSpeed, ZeRO with PyTorch does not require any change in the code whatsoever. You can define your training function just like you are used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attended-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(module, hparams, train_set, test_set):\n",
    "    \n",
    "    import time\n",
    "    import torch\n",
    "        \n",
    "    from maggy.core.patching import MaggyPetastormDataLoader\n",
    "    \n",
    "    model = module(**hparams)\n",
    "    \n",
    "    batch_size = 4\n",
    "    lr_base = 0.1 * batch_size/256\n",
    "    \n",
    "    # Parameters as in https://arxiv.org/pdf/1706.02677.pdf\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_base)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_loader = MaggyPetastormDataLoader(train_set, batch_size=batch_size)\n",
    "                            \n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        img, label = data[\"image\"].float(), data[\"label\"].long()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(img)\n",
    "        loss = loss_criterion(prediction, label)\n",
    "        loss.backward()\n",
    "        m1 = torch.cuda.max_memory_allocated(0)\n",
    "        optimizer.step()\n",
    "        m2 = torch.cuda.max_memory_allocated(0)\n",
    "        print(\"Optimizer pre: {}MB\\n Optimizer post: {}MB\".format(m1//1e6,m2//1e6))\n",
    "        print(f\"Finished batch {idx}\")\n",
    "    return float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informative-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True"
     ]
    }
   ],
   "source": [
    "train_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/train_set\"\n",
    "test_ds = hdfs.project_path() + \"/DataSets/MNIST/PetastormMNIST/test_set\"\n",
    "print(hdfs.exists(train_ds), hdfs.exists(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-absolute",
   "metadata": {},
   "source": [
    "### Configuring the experiment\n",
    "With the `torch` backend, you don't have to create an extensive, separate config for ZeRO. Simply enter the ZeRO level you want to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nonprofit-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import TorchDistributedConfig\n",
    "\n",
    "config = TorchDistributedConfig(name='torch_ZeRO', module=CNN, train_set=train_ds, test_set=test_ds, backend=\"torch\", zero_lvl=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cubic-screening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87065dfefb940c091245bb1569a21c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=1.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Awaiting worker reservations.\n",
      "1: Awaiting worker reservations.\n",
      "1: All executors registered: True\n",
      "1: Reservations complete, configuring PyTorch.\n",
      "1: Torch config is {'MASTER_ADDR': '10.0.0.4', 'MASTER_PORT': '52577', 'WORLD_SIZE': '2', 'RANK': '1', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: All executors registered: True\n",
      "0: Reservations complete, configuring PyTorch.\n",
      "0: Torch config is {'MASTER_ADDR': '10.0.0.4', 'MASTER_PORT': '52577', 'WORLD_SIZE': '2', 'RANK': '0', 'LOCAL_RANK': '0', 'NCCL_BLOCKING_WAIT': '1', 'NCCL_DEBUG': 'INFO'}\n",
      "0: Starting distributed training.\n",
      "1: Starting distributed training.\n",
      "0: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "1: Petastorm dataset detected in folder hdfs://rpc.namenode.service.consul:8020/Projects/PyTorch_spark_minimal//DataSets/MNIST/PetastormMNIST/train_set\n",
      "0: Optimizer pre: 3870.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 0\n",
      "1: Optimizer pre: 3870.0MB\n",
      " Optimizer post: 5093.0MB\n",
      "1: Finished batch 0\n",
      "1: Optimizer pre: 5502.0MB\n",
      " Optimizer post: 5502.0MB\n",
      "1: Finished batch 1\n",
      "0: Optimizer pre: 7414.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 1\n",
      "0: Optimizer pre: 7414.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 2\n",
      "1: Optimizer pre: 5502.0MB\n",
      " Optimizer post: 5502.0MB\n",
      "1: Finished batch 2\n",
      "1: Optimizer pre: 5502.0MB\n",
      " Optimizer post: 5502.0MB\n",
      "1: Finished batch 3\n",
      "0: Optimizer pre: 7414.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 3\n",
      "0: Optimizer pre: 7414.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 4\n",
      "1: Optimizer pre: 5502.0MB\n",
      " Optimizer post: 5502.0MB\n",
      "1: Finished batch 4\n",
      "0: Optimizer pre: 7414.0MB\n",
      " Optimizer post: 7414.0MB\n",
      "0: Finished batch 5\n",
      "1: Optimizer pre: 5502.0MB\n",
      " Optimizer post: 5502.0MB\n",
      "1: Finished batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://10.0.0.7:8998/sessions/194/statements/6 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(train_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}