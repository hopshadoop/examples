{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal mnist example on Hopsworks\n",
    "---\n",
    "\n",
    "<font color='red'> <h3>Tested with TensorFlow 1.10</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hops Experiment paradigm <a class=\"anchor\" id='paradigm'></a>\n",
    "\n",
    "To be able to run your TensorFlow code on Hops, the code for the whole program needs to be provided and put inside a wrapper function. Everything, from importing libraries to reading data and defining the model and running the program needs to be put inside a wrapper function. If you wish to run gridsearch over a given set of hyperparameters, you can define arguments for this wrapper function that corresponds to the name of your hyperparameters.\n",
    "\n",
    "You can also submit one or more `.py`, `.zip` or `.egg` files that contain your code and import them in the wrapper function. To include files, navigate back to HopsWorks and restart restart Jupyter, you can then include files in the Jupyter configuration.\n",
    "\n",
    "## The `hops` python module\n",
    "\n",
    "Below you can see the aforementioned wrapper function, which is coincidently named `wrapper` but could potentially be named anything. You can see two imports from the `hops` module, a `tensorboard` and an `hdfs` module. These are the only two modules that you will need to use in your TensorFlow wrapper function. \n",
    "\n",
    "### Using the `tensorboard` module\n",
    "The `tensorboard` module allow us to get the log directory for summaries and checkpoints to be written to the TensorBoard we will see in a bit. The only function that we currently need to call is `tensorboard.logdir()`, which returns the path to the TensorBoard log directory. Furthermore, the content of this directory will be put in as a Dataset in your project in HopsFS after each hyperparameter configuration is finished. The `experiment.launch` function, that we will look at abit further down will return the exact path, which you can then navigate to using HopsWorks to inspect the files.\n",
    "\n",
    "The directory could in practice be used to store other data that should be accessible after each hyperparameter configuration is finished.\n",
    "```python\n",
    "# Use this module to get the TensorBoard logdir\n",
    "from hops import tensorboard\n",
    "tensorboard_logdir = tensorboard.logdir()\n",
    "```\n",
    "\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a single method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a mnist folder in your Resources Dataset, which is created automatically for each project, the path to the mnist data would be `hdfs.project_path() + 'Resources/mnist'`\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "![image11-Dataset-ProjectPath.png](../../images/datasets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_mnist():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('MNIST_data')\n",
    "\n",
    "    from hops import tensorboard\n",
    "\n",
    "    def input(dataset):\n",
    "        return dataset.images, dataset.labels.astype(np.int32)\n",
    "\n",
    "    # Specify feature\n",
    "    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\n",
    "\n",
    "    # Build 2 layer DNN classifier\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[256, 32],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "    n_classes=10,\n",
    "    dropout=0.1,\n",
    "    model_dir=tensorboard.logdir()\n",
    "    )\n",
    "\n",
    "    # Define the training inputs\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(mnist.train)[0]},\n",
    "    y=input(mnist.train)[1],\n",
    "    num_epochs=None,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "    classifier.train(input_fn=train_input_fn, steps=500)\n",
    "\n",
    "    # Define the test inputs\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input(mnist.test)[0]},\n",
    "    y=input(mnist.test)[1],\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "    print(\"\\nTest Accuracy: {0:f}%\\n\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "\n",
    "notebook = hdfs.project_path() + \"Jupyter/Experiment/TensorFlow/minimal_mnist_classifier_on_hops.ipynb\"\n",
    "experiment.launch(minimal_mnist,\n",
    "                  name='mnist estimator', \n",
    "                  description='A minimal mnist example with two hidden layers',\n",
    "                  versioned_resources=[notebook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring execution - TensorBoard <a class=\"anchor\" id='tensorboard'></a>\n",
    "To find the TensorBoard for the execution, please go back to HopsWorks and enter the Experiments service.\n",
    "Then copy & paste the experiment_id into the textbox and press enter to start a TensorBoard to see all experiments being run in parallel.\n",
    "\n",
    "![Image7-Monitor.png](../../images/experiments_service.png)\n",
    "![Image7-Monitor.png](../../images/tensorboard.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
